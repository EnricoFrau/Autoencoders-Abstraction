{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dde085c",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67b5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9eb50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders')\n",
    "from AE.models import AE_0\n",
    "from AE.train import train\n",
    "from AE.datasets import MNISTDigit2Dataset\n",
    "from AE.utils import calc_hfm_marginalized_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f451c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.004009525010224053)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_hfm_marginalized_prob(5, 0.9, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a4b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fed3cb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device(\"cpu\")  # Fallback to CPU if no GPU is available\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f780ae",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5435f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa86493",
   "metadata": {},
   "source": [
    "## MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5d5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059421c",
   "metadata": {},
   "source": [
    "## ExtendedMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0049eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53804cd0",
   "metadata": {},
   "source": [
    "## train over 2MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ab5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([64, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([64])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([64, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([64])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d5ddf",
   "metadata": {},
   "source": [
    "## train over FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecf056e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7a977",
   "metadata": {},
   "source": [
    "## OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train over pureHFM\n",
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM 32-1024\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61a402",
   "metadata": {},
   "source": [
    "# Autoencoders model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c88f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2559680",
   "metadata": {},
   "source": [
    "## EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6d768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec2ff3",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "106dc001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_1hl.pth', map_location=my_model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca2f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005581370054856471, Validation loss: 0.0005593168556175017\n",
      "Training completed. Final training loss: 0.0005581370054856471, Validation loss: 0.0005593168556175017\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_1hl')\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_1hl.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91655455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00047951453753436603, Validation loss: 0.00048274130927042125\n",
      "Training completed. Final training loss: 0.00047951453753436603, Validation loss: 0.00048274130927042125\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_1hl_1')\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=1).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_1hl_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ae564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e33516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1c42b3c",
   "metadata": {},
   "source": [
    "# Get empirical latent distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e257ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empirical_latent_distribution(model, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    latent_vectors = []\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for databatch, _ in dataloader:\n",
    "            dat_batch = databatch.to(device)\n",
    "            latent = model.encode(dat_batch.view(dat_batch.size(0), -1))\n",
    "            latent_vectors.append(latent.cpu().numpy())\n",
    "            total_samples += dat_batch.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b0c8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_latent_frequencies(model, dataloader):\n",
    "    \"\"\"\n",
    "    Extracts binary internal representations from the autoencoder and counts their frequencies.\n",
    "    \n",
    "    Args:\n",
    "        model: The autoencoder model with sigmoid activation before bottleneck\n",
    "        dataloader: DataLoader containing the dataset\n",
    "        device: Device to run computations on\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary where keys are binary state tuples and values are frequencies\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    state_counts = defaultdict(int)\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, _ in dataloader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            \n",
    "            # Encode to get latent representations (after sigmoid)\n",
    "            latent_vectors = model.encode(batch_data.view(batch_data.size(0), -1))\n",
    "            \n",
    "            # Convert to binary: < 0.5 → 0, >= 0.5 → 1\n",
    "            binary_states = (latent_vectors >= 0.5).int()\n",
    "            \n",
    "            # Convert each binary vector to tuple (hashable for dictionary keys)\n",
    "            for i in range(binary_states.size(0)):\n",
    "                state_tuple = tuple(binary_states[i].cpu().numpy())\n",
    "                state_counts[state_tuple] += 1\n",
    "                total_samples += 1\n",
    "\n",
    "    # Normalize frequencies by total_samples\n",
    "    frequency_dict = {k: v / total_samples for k, v in state_counts.items()}\n",
    "\n",
    "    \n",
    "    print(f\"Total samples processed: {total_samples}\")\n",
    "    print(f\"Number of unique binary states found: {len(frequency_dict)}\")\n",
    "    print(f\"Theoretical maximum states for {model.latent_dim}-dim latent: {2**model.latent_dim}\")\n",
    "    \n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1c83878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples processed: 18800\n",
      "Number of unique binary states found: 256\n",
      "Theoretical maximum states for 8-dim latent: 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0027659574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0075,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.008297872340425531,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.005478723404255319,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.009468085106382978,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0010106382978723404,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.005904255319148936,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0075,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0027659574468085106,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.009946808510638299,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008404255319148936,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0025,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003776595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008776595744680852,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.001968085106382979,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.011063829787234043,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.001648936170212766,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003776595744680851,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.009042553191489361,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0027659574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.008617021276595745,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0012234042553191488,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003776595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0041489361702127655,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003617021276595745,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008404255319148936,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005159574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0030851063829787236,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0047872340425531915,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.002127659574468085,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0063297872340425535,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.005478723404255319,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.004308510638297873,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.015957446808510637,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0025,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003457446808510638,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.004680851063829788,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0010638297872340426,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.007446808510638298,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.004946808510638298,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.022287234042553193,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.03829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008031914893617022,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008085106382978723,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0036702127659574467,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005638297872340425,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.007127659574468085,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0031914893617021275,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003776595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.013936170212765957,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.005638297872340425,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0063297872340425535,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.00824468085106383,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004627659574468085,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0028191489361702126,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.013829787234042552,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.012340425531914894,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0017553191489361702,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008297872340425531,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.002021276595744681,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0004787234042553191,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0014893617021276596,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004308510638297873,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003989361702127659,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0031382978723404256,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0050531914893617025,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0023936170212765957,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0031914893617021275,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003351063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0026595744680851063,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.008670212765957447,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005638297872340425,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0027659574468085106,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0014361702127659575,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.001968085106382979,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0013829787234042553,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.00324468085106383,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002553191489361702,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.001276595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004734042553191489,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.01148936170212766,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0028191489361702126,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003936170212765958,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.00851063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0013829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0013829787234042553,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.012712765957446808,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.006914893617021276,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003457446808510638,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0031382978723404256,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.009361702127659575,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.003989361702127659,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005159574468085106,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0009574468085106382,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.004734042553191489,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003617021276595745,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004893617021276595,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0012234042553191488,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0010638297872340426,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0038829787234042554,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.010372340425531914,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005212765957446809,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.002127659574468085,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.002180851063829787,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.00601063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.006117021276595745,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.005851063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0044148936170212765,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0022340425531914895,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004361702127659574,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0056914893617021275,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0018617021276595746,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.005106382978723404,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0031914893617021275,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.002340425531914894,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.007978723404255319,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0075,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0035638297872340424,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.006914893617021276,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.003351063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0027127659574468087,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.001968085106382979,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0012234042553191488,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.00526595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0017553191489361702,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.00675531914893617,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0015957446808510637,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0009574468085106382,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.006702127659574468,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.007712765957446808,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0027659574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0036702127659574467,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.002127659574468085,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0013297872340425532,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0020744680851063828,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.004946808510638298,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002553191489361702,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0010638297872340426,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.004468085106382979,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0005851063829787234,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.00648936170212766,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.001968085106382979,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0031382978723404256,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0005851063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003617021276595745,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.002925531914893617,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0022340425531914895,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.003829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0023936170212765957,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003936170212765958,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0020744680851063828,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003297872340425532,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0014893617021276596,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003351063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.007925531914893617,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0025,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0041489361702127655,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.010904255319148936,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0022872340425531914,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005212765957446809,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0009042553191489362,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0014361702127659575,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0015425531914893618,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0022340425531914895,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.005904255319148936,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.007127659574468085,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0027659574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.003723404255319149,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0035638297872340424,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.004202127659574468,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0024468085106382977,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003297872340425532,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.006117021276595745,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002553191489361702,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002872340425531915,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.000851063829787234,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0026595744680851063,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.005904255319148936,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0013297872340425532,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0012234042553191488,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0027659574468085106,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0029787234042553193,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0015425531914893618,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0020744680851063828,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004946808510638298,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0011170212765957447,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004095744680851064,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.004680851063829788,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.003776595744680851,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0013829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003404255319148936,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0013297872340425532,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.002925531914893617,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0010638297872340426,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.004308510638297873,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0017553191489361702,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.004042553191489362,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0022340425531914895,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0015957446808510637,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.001702127659574468,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002872340425531915,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.004202127659574468,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.002872340425531915,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0026595744680851063,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.002180851063829787,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.000425531914893617,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0024468085106382977,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0013829787234042553,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0020744680851063828,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0028191489361702126,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0005851063829787234,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0019148936170212765,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0018085106382978724,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.002872340425531915,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0044148936170212765,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.003297872340425532,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.001170212765957447,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0029787234042553193,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.002021276595744681,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.001702127659574468,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0031382978723404256,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0026595744680851063,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.002340425531914894,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0035638297872340424,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0014893617021276596,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0035638297872340424,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.000851063829787234,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.000851063829787234,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0010638297872340426,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0013297872340425532,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0007446808510638298,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.001968085106382979,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0018617021276595746,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.001170212765957447,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0007978723404255319,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0012234042553191488,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0014893617021276596,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0014361702127659575,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0009042553191489362,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.000425531914893617,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0013829787234042553,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.001170212765957447,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.001648936170212766,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0007446808510638298,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.001648936170212766,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.00026595744680851064,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0)): 0.0007978723404255319,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0006382978723404255,\n",
       " (np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0010638297872340426,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0)): 0.0010106382978723404,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0007446808510638298,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0010106382978723404,\n",
       " (np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.00010638297872340425,\n",
       " (np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1)): 0.0006914893617021277,\n",
       " (np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(0),\n",
       "  np.int32(1),\n",
       "  np.int32(0),\n",
       "  np.int32(1)): 0.0005851063829787234}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_binary_latent_frequencies(my_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "255e78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_binary_frequencies(frequency_dict, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze and display the most frequent binary states.\n",
    "    \n",
    "    Args:\n",
    "        frequency_dict: Dictionary from get_binary_latent_frequencies\n",
    "        top_k: Number of top states to display\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Sort by frequency (descending)\n",
    "    sorted_states = sorted(frequency_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop {top_k} most frequent binary states:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (state, count) in enumerate(sorted_states[:top_k]):\n",
    "        percentage = (count / sum(frequency_dict.values())) * 100\n",
    "        state_str = ''.join(map(str, state))\n",
    "       # print(f\"{i+1:2d}. {state_str} -> {count:5d} samples ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Plot frequency distribution\n",
    "    frequencies = [count for _, count in sorted_states]\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(frequencies)), frequencies)\n",
    "    plt.xlabel('Binary State (sorted by frequency)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Binary States')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(min(top_k, len(frequencies))), frequencies[:top_k])\n",
    "    plt.xlabel('Top Binary States')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Top {top_k} Most Frequent States')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45f8291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be936a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples processed: 18800\n",
      "Number of unique binary states found: 256\n",
      "Theoretical maximum states for 8-dim latent: 256\n",
      "\n",
      "Top 15 most frequent binary states:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.03829787234042553),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.022287234042553193),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.015957446808510637),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.013936170212765957),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.013829787234042552),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.012712765957446808),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.012340425531914894),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.01148936170212766),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.011063829787234043),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.010904255319148936),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.010372340425531914),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.009946808510638299),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.009468085106382978),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.009361702127659575),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.009042553191489361),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008776595744680852),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008670212765957447),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.008617021276595745),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.00851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008404255319148936),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008404255319148936),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.008297872340425531),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008297872340425531),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.00824468085106383),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008085106382978723),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.008031914893617022),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.007978723404255319),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.007925531914893617),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.007712765957446808),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0075),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0075),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0075),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.007446808510638298),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.007127659574468085),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.007127659574468085),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.006914893617021276),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.006914893617021276),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.00675531914893617),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.006702127659574468),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.00648936170212766),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0063297872340425535),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0063297872340425535),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.006117021276595745),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.006117021276595745),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.00601063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.005904255319148936),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.005904255319148936),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005904255319148936),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.005851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0056914893617021275),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005638297872340425),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.005638297872340425),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005638297872340425),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.005478723404255319),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.005478723404255319),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.00526595744680851),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005212765957446809),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005212765957446809),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005159574468085106),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.005159574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.005106382978723404),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0050531914893617025),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.004946808510638298),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.004946808510638298),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004946808510638298),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004893617021276595),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0047872340425531915),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004734042553191489),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.004734042553191489),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.004680851063829788),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004680851063829788),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004627659574468085),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.004468085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0044148936170212765),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0044148936170212765),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004361702127659574),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.004308510638297873),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004308510638297873),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.004308510638297873),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.004202127659574468),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.004202127659574468),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0041489361702127655),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0041489361702127655),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.004095744680851064),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.004042553191489362),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003989361702127659),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.003989361702127659),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003936170212765958),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003936170212765958),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0038829787234042554),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003829787234042553),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003829787234042553),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003776595744680851),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003776595744680851),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003776595744680851),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003776595744680851),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003776595744680851),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.003723404255319149),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0036702127659574467),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0036702127659574467),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003617021276595745),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003617021276595745),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.003617021276595745),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0035638297872340424),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0035638297872340424),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0035638297872340424),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0035638297872340424),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003457446808510638),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003457446808510638),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003404255319148936),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003351063829787234),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.003351063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003351063829787234),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003297872340425532),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.003297872340425532),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.003297872340425532),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.00324468085106383),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0031914893617021275),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0031914893617021275),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0031914893617021275),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0031382978723404256),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0031382978723404256),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0031382978723404256),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0031382978723404256),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0030851063829787236),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0029787234042553193),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0029787234042553193),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.002925531914893617),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.002925531914893617),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002872340425531915),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002872340425531915),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.002872340425531915),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.002872340425531915),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0028191489361702126),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0028191489361702126),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0028191489361702126),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0027659574468085106),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0027127659574468087),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0026595744680851063),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0026595744680851063),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0026595744680851063),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0026595744680851063),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002553191489361702),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002553191489361702),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002553191489361702),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0025),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0025),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0025),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0024468085106382977),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0024468085106382977),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0023936170212765957),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0023936170212765957),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.002340425531914894),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.002340425531914894),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0022872340425531914),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0022340425531914895),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0022340425531914895),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0022340425531914895),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0022340425531914895),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.002180851063829787),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.002180851063829787),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.002127659574468085),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.002127659574468085),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.002127659574468085),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0020744680851063828),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0020744680851063828),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0020744680851063828),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0020744680851063828),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.002021276595744681),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.002021276595744681),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.001968085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.001968085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.001968085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.001968085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.001968085106382979),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0019148936170212765),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0018617021276595746),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0018617021276595746),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0018085106382978724),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0017553191489361702),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0017553191489361702),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0017553191489361702),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.001702127659574468),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.001702127659574468),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.001648936170212766),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.001648936170212766),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.001648936170212766),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0015957446808510637),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0015957446808510637),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0015425531914893618),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0015425531914893618),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0014893617021276596),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0014893617021276596),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0014893617021276596),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0014893617021276596),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0014361702127659575),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0014361702127659575),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0014361702127659575),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0013829787234042553),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0013297872340425532),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0013297872340425532),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0013297872340425532),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0013297872340425532),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.001276595744680851),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0012234042553191488),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0012234042553191488),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0012234042553191488),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0012234042553191488),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0012234042553191488),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.001170212765957447),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.001170212765957447),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.001170212765957447),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0011170212765957447),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0010638297872340426),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0010106382978723404),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0010106382978723404),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0010106382978723404),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0009574468085106382),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0009574468085106382),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0009042553191489362),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0009042553191489362),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.000851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.000851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.000851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0007978723404255319),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0007978723404255319),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0007446808510638298),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0007446808510638298),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0007446808510638298),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0006914893617021277),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0006382978723404255),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.0005851063829787234),\n",
       " ((np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(0)),\n",
       "  0.0005851063829787234),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0005851063829787234),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.0005851063829787234),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0)),\n",
       "  0.0004787234042553191),\n",
       " ((np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.000425531914893617),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1)),\n",
       "  0.000425531914893617),\n",
       " ((np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.00026595744680851064),\n",
       " ((np.int32(1),\n",
       "   np.int32(0),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1),\n",
       "   np.int32(1)),\n",
       "  0.00010638297872340425)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_binary_frequencies(get_binary_latent_frequencies(my_model, val_loader), top_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b0d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0257dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2, 3), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92152569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1 = torch.randn((2, 3), requires_grad=True)\n",
    "w2 = torch.randn((2, 3), requires_grad=True)\n",
    "w3 = torch.randn((2, 3), requires_grad=True)\n",
    "w4 = torch.randn((2, 3), requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26bec468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1830, -0.0679,  0.5495],\n",
      "        [ 0.3463,  0.0700,  0.3287]])\n"
     ]
    }
   ],
   "source": [
    "w1.requires_grad = True\n",
    "\n",
    "b = (w1 * a).detach()\n",
    "c = w2 * a\n",
    "d = b*w3 + c*w4\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([a, w1, w2, w3, w4])\n",
    "optimizer.zero_grad()\n",
    "\n",
    "L = d.sum()\n",
    "L.backward()\n",
    "\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.requires_grad = False\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = b*w3 + c*w4\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([a, w1, w2, w3, w4])\n",
    "optimizer.zero_grad()\n",
    "\n",
    "L = d.sum()\n",
    "L.backward()\n",
    "\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cc196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6773a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b6abfa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3063,  0.7799, -0.6527],\n",
      "        [ 0.7012, -0.0188, -0.6859]])\n"
     ]
    }
   ],
   "source": [
    "w1.requires_grad = True\n",
    "\n",
    "b = (w1 * a).detach()\n",
    "c = w2 * a\n",
    "d = b*w3 + c*w4\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([a, w1, w2, w3, w4])\n",
    "optimizer.zero_grad()\n",
    "\n",
    "L = d.sum()\n",
    "L.backward()\n",
    "\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942e64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l1 = torch.nn.Linear(8,4)\n",
    "l2 = torch.nn.Linear(4,2)\n",
    "\n",
    "# net = torch.nn.Sequential(\n",
    "#     l1,\n",
    "#     l2\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb88442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1839, -0.2874,  0.1669, -0.1476, -0.1792, -0.0948, -0.1116, -0.1928],\n",
      "        [ 0.2935,  0.1523, -0.1539, -0.1568,  0.1730, -0.0159,  0.0629, -0.1093],\n",
      "        [ 0.2190, -0.1254,  0.1951, -0.1168,  0.2324, -0.2288, -0.0837,  0.0243],\n",
      "        [-0.2638,  0.0428, -0.0971, -0.3238,  0.1297,  0.2683, -0.0136,  0.1166]],\n",
      "       requires_grad=True)\n",
      "tensor(-0.2874, grad_fn=<SelectBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(l1.weight)\n",
    "print(l1.weight[0,1])\n",
    "print(l1.weight[0,1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d79f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.weight.is_leaf:  True\n",
      "l1.weight[0].is_leaf:  False\n",
      "l1.weight.data[0,1].is_leaf:  True\n",
      "l1.weight[0,1]:  tensor(-0.2874, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     l1.weight.requires_grad =\n",
    "print(\"l1.weight.is_leaf: \", l1.weight.is_leaf)\n",
    "print(\"l1.weight[0].is_leaf: \", l1.weight[0].is_leaf)\n",
    "print(\"l1.weight.data[0,1].is_leaf: \", l1.weight.data[0,1].is_leaf)\n",
    "print(\"l1.weight[0,1]: \", l1.weight[0,1])\n",
    "l1.weight.data[0,1].requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4e08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    l1.weight[0,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9133ff7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "l1.weight[0,1].is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9df7c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print(l1.weight[0,1])\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43ml1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "# print(l1.weight[0,1])\n",
    "l1.weight[0,1].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e4c90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.weight.requires_grad = False\n",
    "l1.weight.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "fc78efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1.weight[0,:].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "150cfac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.weight[1,1].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3c070",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2668322555.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m)\u001b[39m\n     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class ProgressiveAE(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            latent_dim,\n",
    "            device,\n",
    "            num_hidden_layers = 1,\n",
    "            decrease_rate = 0.5,\n",
    "            activation_fn = nn.ReLU,\n",
    "            bottleneck_in_fn = nn.Sigmoid\n",
    "        ):\n",
    "        \n",
    "        super.__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.device = device\n",
    "        self.activation_fn = activation_fn\n",
    "        self.bottleneck_in_fn = bottleneck_in_fn\n",
    "\n",
    "\n",
    "        # --------------Encoder----------------\n",
    "\n",
    "        encoder_layers_sizes = [input_dim]\n",
    "        for i in range(num_hidden_layers):\n",
    "            encoder_layers_sizes.append(int(encoder_layers_sizes[-1] * decrease_rate))\n",
    "\n",
    "        encoder_layers = []                             # creates the encoder without the bottleneck\n",
    "        for i in range(len(encoder_layers_sizes) - 1):\n",
    "            encoder_layers.append(\n",
    "                nn.Linear(encoder_layers_sizes[i], encoder_layers_sizes[i + 1])\n",
    "                )\n",
    "            encoder_layers.append(\n",
    "                activation_fn()\n",
    "                )\n",
    "        self.amputated_encoder = nn.Sequential(*encoder_layers).to(device)\n",
    "\n",
    "\n",
    "        # --------------Decoder----------------\n",
    "\n",
    "        decoder_layers_sizes = list(reversed(encoder_layers_sizes))\n",
    "\n",
    "        decoder_layers = []                             # creates the decoder without the bottleneck\n",
    "        for i in range(len(decoder_layers_sizes) - 1):\n",
    "            decoder_layers.append(\n",
    "                nn.Linear(decoder_layers_sizes[i], decoder_layers_sizes[i + 1])\n",
    "                )\n",
    "            decoder_layers.append(\n",
    "                activation_fn()\n",
    "                )\n",
    "        self.amputated_decoder = nn.Sequential(*decoder_layers).to(device)\n",
    "\n",
    "\n",
    "        # --------------Bottleneck----------------\n",
    "\n",
    "        self.bottleneck_in_dict = nn.ModuleDict(\n",
    "            {f\"n{i+1}\": nn.Linear(encoder_layers_sizes[-1], 1).to(device) for i in range(latent_dim)}\n",
    "        )\n",
    "\n",
    "        self.bottleneck_out_dict = nn.ModuleDict(\n",
    "            {f\"n{i+1}\": nn.Linear(1, decoder_layers_sizes[0]).to(device) for i in range(latent_dim)}\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        semi_encoded_x = self.amputated_encoder(x)\n",
    "        encoded_x = torch.cat(                                                                                  # .cat to collect the single neurons tensors into only one tensor.\n",
    "            [self.bottleneck_in_fn( linear(semi_encoded_x) ) for linear in self.bottleneck_in_dict.values()],      # Applies the bottleneck activation function to each linear layer's output.                              \n",
    "            dim=-1\n",
    "        )\n",
    "    \n",
    "\n",
    "    def decode(self, x)):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f96519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "for i in range(5):\n",
    "    my_dict[f\"var_{i+1}\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e878750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9810823",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vect = torch.rand((5, 6), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0b0166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4804, 0.8714, 0.7773, 0.0424, 0.2739, 0.6405],\n",
      "        [0.3801, 0.7623, 0.1396, 0.3274, 0.2377, 0.5924],\n",
      "        [0.0766, 0.5707, 0.4989, 0.4114, 0.0491, 0.2126],\n",
      "        [0.3109, 0.4813, 0.9972, 0.5222, 0.5321, 0.0432],\n",
      "        [0.1757, 0.6072, 0.0769, 0.3719, 0.5335, 0.4842]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(my_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3d091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.nn.Linear(6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19657c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2791, -0.4206, -0.2891],\n",
       "        [ 0.3040, -0.5833, -0.3038],\n",
       "        [ 0.2857, -0.2291, -0.4149],\n",
       "        [ 0.2163,  0.0475, -0.4027],\n",
       "        [ 0.2799, -0.5173, -0.3913]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(my_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a6b3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7905],\n",
      "        [-0.9058],\n",
      "        [-0.2607],\n",
      "        [-0.5465],\n",
      "        [ 2.1174],\n",
      "        [-1.7118],\n",
      "        [ 0.1651],\n",
      "        [ 1.5819],\n",
      "        [ 0.4485],\n",
      "        [ 0.0330]])\n",
      "tensor([[ 1.4503],\n",
      "        [-0.6936],\n",
      "        [ 0.9967],\n",
      "        [ 0.6131],\n",
      "        [ 0.7764],\n",
      "        [-0.3029],\n",
      "        [-1.2753],\n",
      "        [-0.4758],\n",
      "        [ 2.3839],\n",
      "        [ 0.9157]])\n",
      "\n",
      "------------------------\n",
      "tensor([[-0.7905,  1.4503],\n",
      "        [-0.9058, -0.6936],\n",
      "        [-0.2607,  0.9967],\n",
      "        [-0.5465,  0.6131],\n",
      "        [ 2.1174,  0.7764],\n",
      "        [-1.7118, -0.3029],\n",
      "        [ 0.1651, -1.2753],\n",
      "        [ 1.5819, -0.4758],\n",
      "        [ 0.4485,  2.3839],\n",
      "        [ 0.0330,  0.9157]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(10, 1)\n",
    "x2 = torch.randn(10, 1)\n",
    "x = []\n",
    "x.append(x1)\n",
    "x.append(x2)\n",
    "print(x1)\n",
    "print(x2)\n",
    "\n",
    "x3 = torch.cat(x, dim=-1)\n",
    "print('\\n------------------------')\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fad47e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6640, 0.0110, 0.1447])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randn(3)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.6640), tensor(0.0110), tensor(0.1447)]\n"
     ]
    }
   ],
   "source": [
    "input_data_unbind = list(torch.unbind(input_data, dim=0))  # Split along the first dimension\n",
    "print(input_data_unbind) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cc9e4ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmy_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[32m33\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "my_tuple[0] = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4358fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98de7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "[tensor(4., grad_fn=<UnbindBackward0>), tensor(1., grad_fn=<UnbindBackward0>)]\n",
      "c[1].requires_grad: True\n",
      "tensor(5., grad_fn=<SumBackward0>)\n",
      "None\n",
      "None\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor((a**2, a/2), requires_grad=True)\n",
    "print(b.requires_grad)\n",
    "c = list(torch.unbind(b, dim=0))\n",
    "print(c[0].is_leaf)\n",
    "print(c[0].requires_grad)\n",
    "print(c)\n",
    "c[1].detach()\n",
    "\n",
    "print(\"c[1].requires_grad:\", c[1].requires_grad)\n",
    "c = torch.tensor(c, requires_grad=True)\n",
    "loss = c.sum()\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41a2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "[tensor(4., grad_fn=<UnbindBackward0>), tensor(1., grad_fn=<UnbindBackward0>)]\n",
      "loss: tensor(5., grad_fn=<SumBackward0>)\n",
      "a.grad tensor(4.)\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/kqb36gjx6c51yhmhfvh767rr0000gn/T/ipykernel_6223/2839569559.py:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(b.grad)\n",
      "/var/folders/bb/kqb36gjx6c51yhmhfvh767rr0000gn/T/ipykernel_6223/2839569559.py:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(c.grad)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2., requires_grad=True)\n",
    "b = torch.stack([a**2, a/2])\n",
    "print(b.requires_grad)\n",
    "c = list(torch.unbind(b, dim=0))\n",
    "print(c[0].is_leaf)\n",
    "print(c[0].requires_grad)\n",
    "print(c)\n",
    "#c[0] = c[0].detach()\n",
    "c[1] = c[1].detach() \n",
    "\n",
    "c = torch.stack(c)  # Use stack, not tensor()\n",
    "loss = c.sum()\n",
    "print(\"loss:\", loss)\n",
    "\n",
    "loss.backward()\n",
    "print(\"a.grad\", a.grad)\n",
    "print(b.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f688dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad tensor([0.5000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor((2., 3.), requires_grad=True)\n",
    "b = torch.stack([a**2, a/2])\n",
    "c = list(torch.unbind(b, dim=0))\n",
    "\n",
    "c[0] = c[0].detach() \n",
    "\n",
    "\n",
    "c = torch.stack(c)\n",
    "loss = c.sum()\n",
    "\n",
    "loss.backward()\n",
    "print(\"a.grad\", a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1092abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad:  tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n",
      "b.grad:  tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "This prints grad tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "a.grad:  tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n",
      "b.grad:  tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "Before multiplying\n",
      "a.grad:  tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n",
      "b.grad:  tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "After multiplying\n",
      "a.grad:  tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n",
      "b.grad:  tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()   # Since b is non-leaf and it's grad will be destroyed otherwise.\n",
    "\n",
    "c = b.mean()\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"b.grad: \", b.grad)\n",
    "\n",
    "# Redo the experiment but with a hook that multiplies b's grad by 2. \n",
    "a = torch.ones(5)\n",
    "\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()\n",
    "\n",
    "b.register_hook(lambda grad: print(\"This prints grad\", grad))  \n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"b.grad: \", b.grad)\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5)\n",
    "\n",
    "a.requires_grad = True\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()\n",
    "\n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "print(\"Before multiplying\")\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"b.grad: \", b.grad)\n",
    "\n",
    "b.grad *= 2\n",
    "print(\"After multiplying\")\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"b.grad: \", b.grad)       # a's gradient needs to updated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ecbc44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "\n",
    "a.requires_grad = True\n",
    "b = 2*a\n",
    "\n",
    "b.retain_grad()\n",
    "\n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "b.grad *= 2\n",
    "\n",
    "print(a.grad, b.grad)       # a's gradient needs to updated manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbe9794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original a tensor([[[-0.4466,  1.4673,  1.4671],\n",
      "         [-0.1889, -0.6900,  0.2157],\n",
      "         [ 0.7927, -1.7409, -1.8248]],\n",
      "\n",
      "        [[-1.0475, -0.0026, -0.4129],\n",
      "         [ 0.9191,  1.8087, -0.2617],\n",
      "         [-0.6557,  1.0314, -1.2188]]])\n",
      "modif a tensor([[-0.4466,  1.4673,  1.4671, -0.1889, -0.6900,  0.2157,  0.7927, -1.7409,\n",
      "         -1.8248],\n",
      "        [-1.0475, -0.0026, -0.4129,  0.9191,  1.8087, -0.2617, -0.6557,  1.0314,\n",
      "         -1.2188]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,3)\n",
    "print(\"original a\", a)\n",
    "print(\"modif a\", a.view(-1, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937afa8",
   "metadata": {},
   "source": [
    "# Mask with Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d25b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveAE(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            latent_dim,\n",
    "            device,\n",
    "            num_hidden_layers = 1,\n",
    "            decrease_rate = 0.5,\n",
    "            activation_fn = nn.ReLU,\n",
    "            bottleneck_in_fn = nn.Sigmoid,\n",
    "            output_decoder_activ_func = None\n",
    "        ):\n",
    "        \n",
    "        super.__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.device = device\n",
    "        self.activation_fn = activation_fn\n",
    "        self.bottleneck_in_fn = bottleneck_in_fn\n",
    "\n",
    "\n",
    "        # --------------Encoder----------------\n",
    "\n",
    "        encoder_layers_sizes = [input_dim]\n",
    "        for i in range(num_hidden_layers):\n",
    "            encoder_layers_sizes.append(int(encoder_layers_sizes[-1] * decrease_rate))\n",
    "\n",
    "        encoder_layers = []                             # creates the encoder without the bottleneck\n",
    "        for i in range(len(encoder_layers_sizes) - 1):\n",
    "            encoder_layers.append(\n",
    "                nn.Linear(encoder_layers_sizes[i], encoder_layers_sizes[i + 1])\n",
    "                )\n",
    "            encoder_layers.append(\n",
    "                activation_fn()\n",
    "                )\n",
    "        self.amputated_encoder = nn.Sequential(*encoder_layers).to(device)\n",
    "\n",
    "\n",
    "        # --------------Decoder----------------\n",
    "\n",
    "        decoder_layers_sizes = list(reversed(encoder_layers_sizes))\n",
    "\n",
    "        decoder_layers = []                             # creates the decoder without the bottleneck\n",
    "        for i in range(len(decoder_layers_sizes) - 1):\n",
    "            decoder_layers.append(\n",
    "                nn.Linear(decoder_layers_sizes[i], decoder_layers_sizes[i + 1])\n",
    "                )\n",
    "            if i < len(decoder_layers_sizes) - 2:\n",
    "                decoder_layers.append(\n",
    "                    activation_fn()\n",
    "                    )\n",
    "            elif output_decoder_activ_func is not None:\n",
    "                decoder_layers.append(output_decoder_activ_func)\n",
    "        self.amputated_decoder = nn.Sequential(*decoder_layers).to(device)\n",
    "\n",
    "\n",
    "        # --------------Bottleneck----------------\n",
    "\n",
    "        self.bottleneck_in = nn.Sequential(nn.Linear(encoder_layers_sizes[-1], latent_dim), bottleneck_in_fn)\n",
    "        self.bottleneck_out = nn.Sequential(nn.Linear(latent_dim, decoder_layers_sizes[0]), activation_fn)\n",
    "\n",
    "\n",
    "    # ----------------Encode, decode and Forward-------------------\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.bottleneck_in(self.amputated_encoder(x))\n",
    "\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.amputated_decoder(self.bottleneck_out(x))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2acc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_bottleneck_in_weight(weight, neuron_idx):      # neurons 1-indexed\n",
    "    with torch.no_grad():\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        train_neuron = active_neurons[-1]\n",
    "        mask_mul = torch.zeros_like(weight)\n",
    "        mask_add = torch.zeros_like(weight)\n",
    "        mask_mul[active_neurons, :] = 1                 # weights have shape (latent_dim, encoder[-1])\n",
    "        mask_add[train_neuron, :] = torch.randn((1, weight.size(1)))\n",
    "        return weight * mask_mul + mask_add\n",
    "    \n",
    "def mask_bottleneck_in_bias(bias, neuron_idx):      # neurons 1-indexed\n",
    "    with torch.no_grad():\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        train_neuron = active_neurons[-1]\n",
    "        mask_mul = torch.zeros_like(bias)\n",
    "        mask_add = torch.zeros_like(bias)\n",
    "        mask_mul[active_neurons] = 1\n",
    "        mask_add[train_neuron] = torch.randn(1)\n",
    "        return bias * mask_mul + mask_add\n",
    "    \n",
    "def mask_bottleneck_out_weight(weight, neuron_idx):      # neurons 1-indexed\n",
    "    with torch.no_grad():\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        train_neuron = active_neurons[-1]\n",
    "        mask_mul = torch.zeros_like(weight)\n",
    "        mask_add = torch.zeros_like(weight)\n",
    "        mask_mul[:, active_neurons] = 1                 # weights have shape (latent_dim, encoder[-1])\n",
    "        mask_add[:, train_neuron] = torch.randn((weight.size(0),))\n",
    "        return weight * mask_mul + mask_add\n",
    "    \n",
    "def mask_bottleneck_out_bias(bias, neuron_idx):      # neurons 1-indexed\n",
    "    with torch.no_grad():\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        train_neuron = active_neurons[-1]\n",
    "        mask_mul = torch.zeros_like(bias)\n",
    "        mask_add = torch.zeros_like(bias)\n",
    "        mask_mul[active_neurons] = 1\n",
    "        mask_add[train_neuron] = torch.randn(1)\n",
    "        return bias * mask_mul + mask_add\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mask_bottleneck_in_weight_grad(grad, neuron_idx, freeze_previous_neurons_train=True):      # neurons 1-indexed\n",
    "    if freeze_previous_neurons_train:\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[neuron_idx-1, :] = 1                   # weights have shape (latent_dim, encoder[-1])\n",
    "    else:\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[active_neurons, :] = 1                 \n",
    "    return grad * mask\n",
    "\n",
    "\n",
    "def mask_bottleneck_in_bias_grad(grad, neuron_idx, freeze_previous_neurons_train=True):     #neurons 1-indexed\n",
    "    if freeze_previous_neurons_train:\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[neuron_idx-1] = 1\n",
    "    else:\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        mask[active_neurons] = 1\n",
    "    return grad * mask\n",
    "\n",
    "\n",
    "def mask_bottleneck_out_weight_grad(grad, neuron_idx, freeze_previous_neurons_train=True):      # neurons 1-indexed\n",
    "    if freeze_previous_neurons_train:\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[:, neuron_idx-1] = 1                   # weights have shape (latent_dim, encoder[-1])\n",
    "    else:\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[:, active_neurons] = 1\n",
    "    return grad * mask\n",
    "\n",
    "\n",
    "def mask_bottleneck_out_bias_grad(grad, neuron_idx, freeze_previous_neurons_train=True):     #neurons 1-indexed\n",
    "    if freeze_previous_neurons_train:\n",
    "        mask = torch.zeros_like(grad)\n",
    "        mask[neuron_idx-1] = 1\n",
    "    else:\n",
    "        active_neurons = list(range(neuron_idx))\n",
    "        mask[active_neurons] = 1\n",
    "    return grad * mask\n",
    "\n",
    "\n",
    "\n",
    "#handle = model.bottleneck_in[0].weight.register_hook(lambda grad: mask_bottleneck(grad, i_th_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b535859",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2269563368.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfreeze_previous_neurons_train = True\u001b[39m\n                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def train_single_neruon(model,\n",
    "                        epochs, \n",
    "                        neuron_to_train, # 1-indexed\n",
    "                        train_loader, \n",
    "                        val_loader,\n",
    "                        writer,\n",
    "                        lr = 1e-3,\n",
    "                        scheduler = None,\n",
    "                        freeze_previous_neurons_train = True,\n",
    "                        optimizer_func = torch.optim.Adam, \n",
    "                        save_tensorboard_parameters = False,\n",
    "                        save_tensorboard_bottleneck_parameters = False,\n",
    "                        ):\n",
    "    \n",
    "    print(f\"\\n ------------ Training of neuron{neuron_to_train}---------------\")\n",
    "\n",
    "    # ----------------------------- Optimizer initialization -----------------------------\n",
    "\n",
    "    optimizer = optimizer_func(model.parameters, lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    # ----------------------------- Mask parameters and register hook -----------------------------------\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.bottleneck_in[0].weight.copy_(\n",
    "            mask_bottleneck_in_weight(model.bottleneck_in[0].weight),\n",
    "            neuron_to_train,\n",
    "            )\n",
    "        model.bottleneck_in[0].bias.copy(\n",
    "            mask_bottleneck_in_bias(model.bottleneck_in[0],\n",
    "            neuron_to_train)\n",
    "        )\n",
    "        model.bottleneck_out[0].weight.copy_(\n",
    "            mask_bottleneck_out_weight(model.bottleneck_in[0].weight),\n",
    "            neuron_to_train,\n",
    "            )\n",
    "        model.bottleneck_out[0].bias.copy(\n",
    "            mask_bottleneck_out_bias(model.bottleneck_in[0],\n",
    "            neuron_to_train)\n",
    "        )\n",
    "\n",
    "\n",
    "    handle_weight_in = model.bottleneck_in[0].weight.register_hook(lambda grad: mask_bottleneck_in_weight_grad(grad, neuron_to_train, freeze_previous_neurons_train))\n",
    "    handle_bias_in = model.bottleneck_in[0].bias.register_hook(lambda grad: mask_bottleneck_in_bias_grad(grad, neuron_to_train, freeze_previous_neurons_train))\n",
    "    handle_weight_out = model.bottleneck_out[0].weight.register_hook(lambda grad: mask_bottleneck_out_weight_grad(grad, neuron_to_train, freeze_previous_neurons_train))\n",
    "    handle_bias_out = model.bottleneck_out[0].bias.register_hook(lambda grad: mask_bottleneck_out_bias_grad(grad, neuron_to_train, freeze_previous_neurons_train))\n",
    "\n",
    "\n",
    "    #-------------------------- Main loops --------------------------\n",
    "\n",
    "    global_batch_idx = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # --------------------------- Weight optimization loop --------------------\n",
    "\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(model.device)    \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.MSELoss()(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            global_batch_idx += 1\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        # -------------------------- Writer register scalars and parameters --------------------------\n",
    "\n",
    "        writer.add_scalar(\n",
    "            \"Loss/train\", train_loss / len(train_loader.dataset), global_step=(epoch + epochs * neuron_to_train) #to keep trace of the total number of epochs\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {}/{}, Average loss: {:.4f}\".format(\n",
    "                epoch, epochs, train_loss / len(train_loader.dataset)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(model.device)\n",
    "                output = model(data)\n",
    "                loss = nn.MSELoss()(output, data)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        writer.add_scalar(\n",
    "            \"Loss/val\", val_loss / len(val_loader.dataset), global_step=(epoch + epochs*neuron_to_train)\n",
    "        )\n",
    "\n",
    "        if save_tensorboard_bottleneck_parameters:\n",
    "            for name, param in model.bottleneck_in.named_parameters():\n",
    "                writer.add_histogram(name, param, global_step=(epoch + epochs*neuron_to_train))\n",
    "                writer.add_histogram(f\"{name}.grad\", param.grad, global_step=(epoch + epochs*neuron_to_train))\n",
    "            for name, param in model.bottleneck_out.named_parameters():\n",
    "                writer.add_histogram(name, param, global_step=(epoch + epochs*neuron_to_train))\n",
    "                writer.add_histogram(f\"{name}.grad\", param.grad, global_step=(epoch + epochs*neuron_to_train))\n",
    "\n",
    "\n",
    "        if save_tensorboard_parameters:\n",
    "            for name, param in model.named_parameters():\n",
    "                writer.add_histogram(name, param, global_step=epoch)\n",
    "                writer.add_histogram(f\"{name}.grad\", param.grad, global_step=epoch)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # ------------------------ Remove hook and zero_grads ----------------------------\n",
    "\n",
    "    handle_weight_in.remove()\n",
    "    handle_bias_in.remove()\n",
    "    handle_weight_out.remove()\n",
    "    handle_bias_out.remove()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Training of neuron {neuron_to_train} completed. Final training loss: {train_loss / len(train_loader.dataset)}, Validation loss: {val_loss / len(val_loader.dataset)}\"\n",
    "    )\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336e20d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784 * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82fb49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2406, -0.1101, -0.1442, -0.2865, -0.1795,  0.1862,  0.0311, -0.1614],\n",
       "        [-0.1142, -0.3058, -0.1900,  0.1145,  0.2635, -0.2777,  0.2904,  0.3423],\n",
       "        [-0.2553,  0.2700,  0.2232,  0.3136,  0.0847,  0.0129,  0.3514, -0.2732],\n",
       "        [-0.1779, -0.3012, -0.1169, -0.2992, -0.2825, -0.0326,  0.2929, -0.0684]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(8,4).weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e6fb8",
   "metadata": {},
   "source": [
    "## Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f765c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING N.1----------------------\n",
      "-----------------------Before training------------------------\n",
      "linear weights: Parameter containing:\n",
      "tensor([[ 0.1583, -0.4698,  0.2429,  0.0485],\n",
      "        [-0.4801,  0.4939,  0.4996, -0.0552],\n",
      "        [-0.1223, -0.1967,  0.1190, -0.3731],\n",
      "        [ 0.2674, -0.0664, -0.4295,  0.4457],\n",
      "        [ 0.3615, -0.1372, -0.2546, -0.0474],\n",
      "        [ 0.3163,  0.1145, -0.2043,  0.2018],\n",
      "        [-0.3837,  0.0837, -0.2856, -0.4784],\n",
      "        [-0.1346, -0.1053,  0.0158, -0.0046]], requires_grad=True)\n",
      "linear_bias Parameter containing:\n",
      "tensor([ 0.2929,  0.3968,  0.1428, -0.2952, -0.2075, -0.0737, -0.1421,  0.3178],\n",
      "       requires_grad=True)\n",
      "\n",
      " ---------------------After masking-----------------------\n",
      "Parameter containing:\n",
      "tensor([[-0.2341,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.0264,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2044,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.5746,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0752,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3256,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2233,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9607,  0.0000,  0.0000,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9723, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = torch.nn.Linear(4,8)\n",
    "\n",
    "print(\"------------------TRAINING N.1----------------------\")\n",
    "print(\"-----------------------Before training------------------------\")\n",
    "print(\"linear weights:\", linear.weight)\n",
    "print(\"linear_bias\", linear.bias)\n",
    "\n",
    "print(\"\\n ---------------------After masking-----------------------\")\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(mask_bottleneck_out_weight(linear.weight, 1))\n",
    "    linear.bias.copy_(mask_bottleneck_in_bias(linear.bias, 1))\n",
    "\n",
    "\n",
    "print(linear.weight)\n",
    "print(linear.bias)\n",
    "\n",
    "handle = linear.weight.register_hook(lambda grad: mask_bottleneck_out_weight_grad(grad, 1))\n",
    "handle_bias = linear.bias.register_hook(lambda grad: mask_bottleneck_out_bias_grad(grad, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f2a3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9249,  0.7262,  0.1996, -0.5617])\n"
     ]
    }
   ],
   "source": [
    "fake_data = torch.randn(4)\n",
    "print(fake_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a67b7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT GRAD = None\n",
      "\n",
      " ---------------------After training-----------------------\n",
      "Parameter containing:\n",
      "tensor([[-1.2341,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0264,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.7956,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.4254,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9248,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.6744,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7767,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0393,  0.0000,  0.0000,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0277,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer_func = torch.optim.Adam\n",
    "optimizer = optimizer_func(linear.parameters(), lr=1)\n",
    "\n",
    "b = linear(fake_data) ** 2\n",
    "loss = b.sum()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "print(\"WEIGHT GRAD =\", linear.weight.grad)\n",
    "handle.remove()\n",
    "handle_bias.remove()\n",
    "print(\"\\n ---------------------After training-----------------------\")\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad333f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------TRAINING N.2---------------------\n",
      "\n",
      " ---------------------After masking-----------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--------------------TRAINING N.2---------------------\")\n",
    "\n",
    "print(\"\\n ---------------------After masking-----------------------\")\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(mask_bottleneck_out_weight(linear.weight, 3))\n",
    "    linear.bias.copy_(mask_bottleneck_out_bias(linear.bias, 3))\n",
    "\n",
    "optimizer = torch.optim.Adam(linear.parameters(), lr=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "422349a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.2341,  0.0000, -0.3466,  0.0000],\n",
      "        [-0.0264,  0.0000,  1.9828,  0.0000],\n",
      "        [-0.7956,  0.0000, -1.1989,  0.0000],\n",
      "        [-0.4254,  0.0000,  0.5044,  0.0000],\n",
      "        [-0.9248,  0.0000,  0.5415,  0.0000],\n",
      "        [-0.6744,  0.0000, -0.5585,  0.0000],\n",
      "        [ 0.7767,  0.0000,  0.5276,  0.0000],\n",
      "        [-0.0393,  0.0000,  1.1779,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0277,  0.0000,  0.2479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(linear.weight)\n",
    "print(linear.bias)\n",
    "\n",
    "handle = linear.weight.register_hook(lambda grad: mask_bottleneck_out_weight_grad(grad, 3))\n",
    "handle_bias = linear.bias.register_hook(lambda grad: mask_bottleneck_out_bias_grad(grad, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a30a33b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------------------After training-----------------------\n",
      "WEIGHT GRAD tensor([[ 0.0000,  0.0000,  0.1221, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0154,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0237,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0354, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0562,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0590,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0333, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0058, -0.0000]])\n",
      "Parameter containing:\n",
      "tensor([[-1.2341,  0.0000,  7.0656,  0.0000],\n",
      "        [-0.0264,  0.0000, -0.2416,  0.0000],\n",
      "        [-0.7956,  0.0000, -0.5078,  0.0000],\n",
      "        [-0.4254,  0.0000,  2.3711,  0.0000],\n",
      "        [-0.9248,  0.0000,  3.4160,  0.0000],\n",
      "        [-0.6744,  0.0000,  2.2361,  0.0000],\n",
      "        [ 0.7767,  0.0000, -2.9272,  0.0000],\n",
      "        [-0.0393,  0.0000,  0.3942,  0.0000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0277,  0.0000,  0.9389,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n ---------------------After training-----------------------\")\n",
    "optimizer.zero_grad()\n",
    "b = linear(fake_data) ** 2\n",
    "loss = b.sum()\n",
    "loss.backward()\n",
    "print(\"WEIGHT GRAD\",linear.weight.grad)\n",
    "optimizer.step()\n",
    "\n",
    "print(linear.weight)\n",
    "print(linear.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336288d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "handle.remove()\n",
    "handle_bias.remove()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bottleneck[0].weight.copy_(mask_bottleneck_weight(bottleneck[0].weight, 1))\n",
    "print(bottleneck[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfa8c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.2865, 0.0000, 0.0000, 0.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bottleneck[0].bias.copy_(mask_bottleneck_in_bias(bottleneck[0].bias, 1))\n",
    "print(bottleneck[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24ed60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.2865, 0.5435, 0.0000, 0.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bottleneck[0].bias.copy_(mask_bottleneck_in_bias(bottleneck[0].bias, 2))\n",
    "print(bottleneck[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bbca36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "1\n",
      "Parameter containing:\n",
      "tensor([[ 0.3302, -0.0120,  0.0296,  0.0739,  0.0459,  0.1319, -0.0863,  0.0363],\n",
      "        [ 0.5851,  0.6393, -1.0494, -1.4463, -1.0969,  2.1376,  0.1661,  0.0495],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bottleneck[0].weight.copy_(mask_bottleneck_weight(bottleneck[0].weight, 2))\n",
    "\n",
    "print(bottleneck[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55588e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27bb2f94",
   "metadata": {},
   "source": [
    "# Emp_states_dict not binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcd4a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 8\n",
    "decrease_rate = 0.6\n",
    "decrease_rate_str = \"06\"\n",
    "dataset = \"MNIST\"\n",
    "num_hidden_layers = 3\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "my_model = AE_0(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    decrease_rate=decrease_rate,\n",
    "    device=device,\n",
    "    hidden_layers=num_hidden_layers\n",
    ").to(device)\n",
    "model_path = '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/prova/MNIST/ld8_dr06_lr1e3_30ep_3hl.pth'\n",
    "# model_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/simultaneous train/{dataset}/ld{latent_dim}_dr{decrease_rate_str}_lr1e3_lwpretrain_{num_hidden_layers}hl.pth\"\n",
    "my_model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df468c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.0547, 0.8851, 0.7088, 0.2870, 0.2315, 0.6479, 0.5466, 0.4587]]), tensor([[0.7664, 0.3190, 0.5705, 0.3122, 0.7257, 0.7429, 0.8473, 0.3340]]), tensor([[0.3805, 0.5761, 0.3209, 0.4664, 0.0909, 0.7202, 0.9166, 0.7392]]), tensor([[0.1248, 0.8996, 0.2979, 0.1314, 0.8051, 0.3309, 0.6412, 0.6099]]), tensor([[0.1567, 0.8868, 0.7168, 0.4512, 0.3011, 0.5634, 0.5527, 0.6897]]), tensor([[0.3776, 0.5318, 0.7566, 0.1240, 0.4581, 0.4045, 0.5027, 0.9324]]), tensor([[0.4063, 0.3022, 0.7355, 0.9099, 0.6408, 0.8170, 0.2192, 0.1803]]), tensor([[0.1859, 0.6834, 0.5455, 0.2101, 0.6520, 0.2765, 0.4924, 0.6479]]), tensor([[0.2185, 0.8017, 0.1934, 0.8058, 0.1726, 0.8729, 0.1680, 0.5847]]), tensor([[0.3785, 0.6490, 0.6850, 0.1682, 0.4863, 0.3459, 0.5186, 0.9153]])]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "\n",
    "data_loader = val_loader_MNIST\n",
    "\n",
    "images, labels = next(iter(data_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "# Select random indices to visualize\n",
    "indices = torch.randint(0, images.size(0), (num_samples,))\n",
    "\n",
    "latent = []\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images[idx].unsqueeze(0)\n",
    "    # Flatten and encode\n",
    "    with torch.no_grad():\n",
    "        latent.append(my_model.encode(img.view(1, -1)))\n",
    "\n",
    "\n",
    "print(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b766b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 8\n",
    "decrease_rate = 0.6\n",
    "dataset = \"MNIST\"\n",
    "num_hidden_layers = 5\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "my_model = AE_0(\n",
    "    input_dim=input_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    decrease_rate=decrease_rate,\n",
    "    device=device,\n",
    "    hidden_layers=num_hidden_layers,\n",
    "    output_activation_encoder=nn.ReLU\n",
    ").to(device)\n",
    "model_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/simultaneous train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl.pth\"\n",
    "my_model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aac2db70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[5.0971, 1.5529, 0.0000, 2.5322, 0.4985, 1.6270, 3.3599, 0.0000]]), tensor([[4.1834, 4.5390, 0.0000, 3.8908, 0.7799, 7.7804, 1.4963, 0.0000]]), tensor([[ 4.8467,  6.1197,  0.0000, 13.2106,  1.2700,  6.5020,  6.4476,  0.0000]]), tensor([[5.3718, 4.1672, 0.0000, 4.2626, 0.9603, 7.1562, 7.6085, 0.0000]]), tensor([[ 8.3916,  6.0611,  0.0000,  3.9146,  1.3395, 10.7969,  9.9270,  0.0000]]), tensor([[5.7260, 3.5400, 0.0000, 1.8647, 0.5600, 3.5516, 0.8872, 0.0000]]), tensor([[7.2303, 5.9940, 0.0000, 4.0863, 1.1255, 8.9473, 6.8789, 0.0000]]), tensor([[5.0937, 0.8461, 0.0000, 6.1747, 0.7487, 4.0879, 5.2206, 0.0000]]), tensor([[5.7772, 5.8247, 0.0000, 5.4352, 1.1588, 9.1460, 8.3670, 0.0000]]), tensor([[0.8379, 7.9885, 0.0000, 9.7196, 1.0375, 5.5952, 4.7456, 0.0000]])]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "\n",
    "data_loader = val_loader_MNIST\n",
    "\n",
    "images, labels = next(iter(data_loader))\n",
    "\n",
    "images = images.to(device)\n",
    "\n",
    "# Select random indices to visualize\n",
    "indices = torch.randint(0, images.size(0), (num_samples,))\n",
    "\n",
    "latent = []\n",
    "for i, idx in enumerate(indices):\n",
    "    img = images[idx].unsqueeze(0)\n",
    "    # Flatten and encode\n",
    "    with torch.no_grad():\n",
    "        latent.append(my_model.encode(img.view(1, -1)))\n",
    "\n",
    "\n",
    "print(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5c20904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders')\n",
    "from AE.utils import compute_emp_states_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b9e6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AE.depth_utils import calc_optimal_g, compute_emp_states_dict_gauged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5c85d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_states_dict_gauged = compute_emp_states_dict_gauged(my_model,data_loader,binarize_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "134a9a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHGCAYAAABaXqDXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXfJJREFUeJzt3Qd4U1UbB/C36d6FDtrSUkrZeyMgQxmyFBARAVniAAEZMqxsVIqAiCKCgAIqyFBRPhBZMpQhe8teZRRaRvfO/Z73lISkTUrTptyb5P/zuebm5iQ5GfS+Oec959hJkiQRAAAAgAKp5K4AAAAAgDEIVAAAAECxEKgAAACAYiFQAQAAAMVCoAIAAACKhUAFAAAAFAuBCgAAACgWAhUAAABQLAQqAAAAoFgIVAAAAECxEKgAAACAYiFQAQAAAMVCoAIAWsnJyXJXAQBADwIVsAlTpkwhOzs7On/+PL3++uvk7e1N/v7+NHHiROIFxKOjo6lz587k5eVFgYGB9Nlnn5n8HIcOHaIXXniB/Pz8yNXVlcLDw+mNN9544v1+//136tixIwUHB5OzszNFRETQRx99RNnZ2doyQ4cOJQ8PD0pJSclz/549e4o665bftGkTNWvWjNzd3cnT01M8/unTp/Xu179/f/GYly5dog4dOohyvXv3Frf9/fff1L17dypTpoyoU2hoKI0cOZJSU1PzPP/atWupatWq5OLiQtWrV6d169aJxy5btqxeObVaTXPnzqVq1aqJsqVKlaJ33nmHHjx4kO/7M3v2bPHZXbt2Lc9tkZGR5OTkpH2MCxcuULdu3cT7wc8REhJCr732GsXHx+f7HC1bthR1P3HiBLVo0YLc3NyofPny9PPPP4vbd+3aRY0aNRKfa6VKlWjbtm1kqlWrVlG9evXE+8zfsxo1atAXX3yR732uXr0qXju/B/Pnz6dy5cqJurVt21Z8Z/m7y98Vfp1cN/4O379/P8/jFOT7wK+dPzd+Dn7v+D3k7++9e/cM/lu6ePGiKO/j4yP+PQ0YMMDg9xOgyCQAGzB58mSJv+61a9eWevbsKX399ddSx44dxbE5c+ZIlSpVkgYPHiyON23aVBzftWtXgR//zp07UokSJaSKFStKs2bNkhYvXiyNHz9eqlKlyhPv26VLF+nVV18V91uwYIHUvXt38fyjR4/Wltm9e7c4tmbNGr37JicnS+7u7tKQIUO0x77//nvJzs5OateunTRv3jzp008/lcqWLSv5+PhIV65c0Zbr16+f5OzsLEVERIj9hQsXivuyYcOGSR06dJCmT58uffPNN9LAgQMle3t76ZVXXtF7/g0bNojnqlmzpngfJ06cKN6H6tWrS2FhYXpl33zzTcnBwUF66623xHONGzdO1L1BgwZSRkaG0ffn2rVr4jlmzpyZ57Zy5cqJz5Glp6dL4eHhUnBwsPTxxx9LS5YskaZOnSoe/+rVq/l+Bi1atBD3Cw0NlcaMGSPet6pVq4rXvGrVKikwMFCaMmWKNHfuXKl06dKSt7e3lJCQIBXUli1bxOfXqlUraf78+WIbOnSo+Kzzw5+X5nvL9eH3eMKECZKTk5P0zDPPSB9++KHUpEkT6csvv5Tee+898T4NGDBA7zEK+n2YPXu21KxZM2natGnSokWLpOHDh0uurq5Sw4YNJbVaneffUp06daSXX35Z/Jvhz5aPjR07tsDvCUBBIVABm6D54/r2229rj2VlZUkhISHij/iMGTO0xx88eCD+QPPJu6DWrVsnHv/gwYMm1y0lJSXPsXfeeUdyc3OT0tLSxHU+UfAJslu3bnrlOHDh5+VAhiUmJooTEAcDumJiYsTJVfc4vz6+7wcffFCgOkVFRYn3igMHjRo1aoj3kJ9XY+fOneJxdQOVv//+WxxbsWKF3mP++eefBo/n1rhxY6levXp6xw4cOCDuqwmujh49Kq6vXbtWMhUHKnzflStXao+dPXtWHFOpVNL+/fu1xzdv3iyOL126tMCPzyd9Ly8v8Z0zhSZQ8ff3lx4+fKg9HhkZKY7XqlVLyszM1B7nIJyDGM33xpTvg6HP/KefftL7fun+W3rjjTf0ynbt2lXy9fU16fUBFAS6fsCmvPnmm9p9e3t7ql+/vmg+HzhwoPY4N2Vz8/7ly5cL/Lh8H7ZhwwbKzMw0qU7cZK+RmJhIcXFxopmem9HPnj0rjnNTO3fF/PHHH5SUlKQtv3r1aipdujQ9++yz4vrWrVvp4cOHojuIH0ez8WvlrosdO3bkef7BgwfnWyfOW+HHaNKkiXivjh49Ko7funWLTp48SX379hVdSBrcdcLdGrm7h7h7oE2bNnr14q4Qvq+heunq0aMHHT58WHRT6b527pbi7g7Gj882b95cqC4Irgd3E2nwd4A/1ypVqoj3TkOzb+r3g99H/nwKgz97zevTrQN3Yzo4OOgdz8jIoJs3b5r8fdD9zNPS0kS5Z555Rlw/cuRInjoNGjRI7zp/Z7mbKCEhoVCvEcAYBCpgUzjnQhf/8ef+eM4ryX38SbkTuvjkzLkRU6dOFY/FJ8+lS5dSenr6E+/LuQJdu3YVz8m5C5w7wycgpptbwSdrzhFZv369uM4BCwcufBLjQEaTo8Gef/558Ti625YtW+ju3bt6z80nOc5vyO369esi/6BkyZLiBM7359eoWydNzgjncuSW+xjXi+8XEBCQp178OnLXKzd+jSqVSgQnjAMmDn7at28v3jPGOUGjRo2iJUuWiM+A84U4r+NJ+Ska/D5o3kcN/kw4Pyf3MWbK9+Pdd9+lihUrivry83Dux59//lmk7y17Ut1M+T5wbsvw4cNF7hAHLVyG31Nm6D3MXacSJUroPTeAuTwOxQFsAP+SLMgxzcmwoPgEx4mX+/fvp//973/iVz2fjDgpl4/ptjjo4l+7HADwyXbatGkikZYDJ/4FO27cOJGAqsG/bjlBdc2aNdSrVy/xPBy4cACjoSn/ww8/iGTI3HR/fTNukeAAQBcn5XLLB5+4uA6VK1cWSZj8K52DF906FRTfh4OUFStWGLydT4r54URj/sXOr/3DDz8U7ykHU59++qleOX6/uY6coMwn4vfee4+ioqJEeUMBWUG+B+b4fvBrP3bsmPhecGIrbxzIcmvU8uXLn3j/wtbNlO/Dq6++Snv37qUxY8ZQ7dq1xXeW79+uXTuDn7k53heAgkCgAmBGHEzw9sknn9DKlSvFKBoe7aHb5aRr586dorn8119/pebNm2uPX7lyxWB5PpnwSBFuXufWBQ5cNM3zjAMdzYmxdevWhXoN3J3Do6P4BMonUo3c3RZhYWHikkd/5Jb7GNeLR8o0bdpUr4vBFByQccvEuXPnxGvn0S8vvvhinnLc7cTbhAkTxImXn3PhwoX08ccfk5x4dBLXlzc+8fNr+eabb8TIM0OtUuZQ0O8Dt4Js375dtAhOmjRJe1zTIgMgJ3T9AJgB/6HP/UuSf5Wy/Lp/NL9Kde/LOQZff/210ZM1Px4HEdx1wIGLLu7u4NaZ6dOnG8yViY2NfeJrMVQn3s89lJZbOXhI7/fff6+XN8NDeTnY0cX15JYaHkqbW1ZWlmhZehLuWuO6/fTTT6Lbp1OnTqKlR4ODN34sXRywcItRQbrgilPuIb5cp5o1a4r94qxbQb8Phj5zxsPJAeSGFhUAM+DAgYMLzjXhX7GcFLt48WJxkuA5SozhBFXu2+/Xr5/opuAuJG6mN9Z8XrduXfHre/z48eIEp9vtw/j5FixYQH369BFlOTmUu1W4m2Tjxo2ideGrr77K97VwVw+/htGjR4vuHn7MX375xWDuAZ8AOR+HH5fn0eAy/PgcwOgGL9y9xXOmcDcMd4HwPCCOjo7iFzsHHRwEvfLKK/nWi1sFnnvuOZozZ454f3O/9r/++kvMN8P5LJwPwkELv5d8EuYgR07cosZdaZwrwl1QnN8zb948Ecxysm5xKej3gctxi97MmTNFQMMJ2tx1ZqxlD+BpQqACYAZ8Ij5w4IDo5rlz545IamzYsKHIydAkJBri6+srRgq9//77oquCgxZOpG3VqpX4NWwIn6C5a4kDFj755Mb5K9zaMWPGDJo1a5YIaPjEwzkeHEw8CQcQnP+iye/gnBkOwDgIqFWrll5Z7sbgFg6eBOyDDz6gChUq0LJly0TglntCMe5+4VE+3N3BeSacH8FdV/x6+YRZEPzauQuJJy3LHQBy3fg947pzgMVdQ3yM80F0u8fkwK9x0aJFIpjl1iPOF+HXwu9b7hwhcyvo94G7KocNGyYSkDlQ5mCS3zu+L4Cc7HiMsqw1AACrwy0F/Mu9sMNxAQA0kKMCAIXG3QS580I4Qfj48eNiWnoAgKJCiwrAE3DCoe46OoZGc/B8I7aI16Lh0STctcFdBDxBHXfxcNfXqVOnRNeWNePvxZMSlHmYr7Hh6QDwZAhUAJ6A8ygMLYinm5/CrQi2iCcCe/vtt2nPnj3ihM2jcDi/hvMhNENjrT1Qyy8HiU2ePFnkogBA4SBQAXgCPgkbWjVYgxNgOUkUbA9PNf/PP//kW4ZXI+YNAAoHgQoAAAAolkUPT+bZHXlhNB6qmHuNDgAAAFAmbiPh+ZA4t+1JQ/QtOlDhICX3olwAAABgGaKjo5+4DpdFByrckqJ5oZoVVAEAzCE5I5mCP8uZ7OzW+7fI3enxdP0AUDS85AU3NGjO41YbqGi6ezhIQaACAOZkn2FP5JKzz39fEKgAmF9B0jYw4RsAAAAoFgIVAAAAUCyL7voBACguDioH6lern3YfAOSBf30AAAY4OzjTsi7LSEnT9fPaSgCWgFdht7e3N8tjIVABAFD4fBMxMTH08OFDuasCYBIfHx8KDAws8jxnCFQAAIwECCmZKWLfzdFNtkklNUFKQEAAubnJVw8Ak/7tpKTQ3bt3xfWgoCAqCgQqAAAGcJDiEZWz6nFSZJIsw5O5u0cTpFj7StRgXVxdXcUlByv8/S1KNxBG/QAAKJQmJ4VbUgAsjeZ7W9TcKgQqAAAKh+4esOXvLQIVAAAAUCxZAxXuf504cSKFh4eL/qyIiAj66KOPRCIOAAAAgKzJtJ9++iktWLCAli9fTtWqVaNDhw7RgAEDyNvbm9577z05qwYAAAC2Hqjs3buXOnfuTB07dhTXy5YtSz/99BMdOHBAzmpRSkYW3U/OMNrPpntN9yY7nVv0j+dSiPvo1qEgz5/7AYw9trHHze+xjXU75j5ekNdm7L3lw+iXBwAAWQOVJk2a0KJFi+j8+fNUsWJFOn78OP3zzz80Z84cg+XT09PFprtMdHHY9t9deu+no8Xy2GAalR1vdqRS2T3eFxs9Opb3OAc4KhWR/aNjHO/wpb2K93XLP34MLuvoYEcOKhU52tuRo72KHOwf7at4P+eY7m1O9nbi0kFlR04OfJuKXB3tycXRnlyd7MnFQSUu9Y7xpaO9qAsom73Knl6p+op2H6Ag6Qw5f3+Q/mk1gcoHH3wggo3KlSuLMdb8IX/yySfUu3dvg+WjoqJo6tSpxV4vPmm5OOZ80XKny0hGrkg6V3TvkzvbRjf/Rvc2pOUYppZ4E/8ja+JkrxLfMU8XR/J0cSAvF0fycnXQuy4uXR3J29WRfN2dyM/TmfzcnUU5tDYVPxcHF1rbfS0pDf8NSc3MluW5Ocgu6HevZcuWVKNGDfG3nbv3nZyc6OOPP6ZevXrR0KFD6eeff6ZSpUrRvHnzqH379k98vAcPHoj7bdmyhZKSkigkJIQ+/PBDkS5gyJ9//ime79SpU6IOjRs3pi+++ELkQmp+KDdr1kykIGjExsZScHAwbd++nZo3by5+GI8fP1609PN8NtWrVxfl+bWxZcuW0YgRI+j7778X5zP+0X3x4kXxOFy3o0ePiqG5tWvXps8//5zq1q2rfa6zZ8/Sm2++KVIeypUrR19++SW1adOG1q1bR126dBFloqOj6f333xevmYMfri+/Bu59sCWyBipr1qyhFStW0MqVK0WOyrFjx8SHzl+Ufv1yFgPTFRkZSaNGjdJe5yAnNDTU7PXqWDNIbEqhF9wYCYKMBUB572P4sagQ9zH2/HlvM3yD7uPmLsfBifToUq1+dMnHJKJsdc4+xy78vNk6ZXJimpxjfJsIdNSa65rHyTmm3efyaokys9WUmS1Rlrh8tK/Oucy5rqasbIkyHl3ybRlZmjK8r6a0TLU4iaRlZudcZjy6fHRcgx+Dt4S0LDIVt+r4ujuTn6eTuCzl5UylfdyodAlXKu3jSiElXCnQ20W08ID14e9R1UmbZXnuM9NeIDengp82OEAZO3as6M5fvXo1DR48WJyIu3btKk7kfPLu06cPXb9+/YlzxfDAizNnztCmTZvIz89PBASpqalGyycnJ4vzRc2aNUVgM2nSJPG8fJ7hkz7/IJ45cybNmDFDG3xxHfn8wwEB48CIn3PVqlXiONe9Xbt2dPLkSapQoYIowzOwcvCyZMkSMSkfT252+fJlcQ7jIIz/Dn322WfUoUMHunDhAnl6eoof5RyMlClThv79919KTEwUAYkuDnBeeOEFEWD9/fff5ODgIAIvfv4TJ06IwM9W2EkyDrHhIIOj0CFDhmiP8Qfx448/imjzSThQ4cTb+Ph48vLyKubaAhQN/1NLz1JTakY2pWVlU0pGNiWlZVFCWiYl8mVqpt6+uEzLpIcpmXQvOYPiEtMpMb1ggQ33LAV6uVBZP3cqH+BBFQI8KCLAQ+z7ezijRcZCpKWl0ZUrV8TISBcXF20OnSUEKtzqwCdkPsky3ue/1y+//LJogdAsD8DTq+/bt4+eeeaZfB/vpZdeEgHKd999V6i6x8XFkb+/vwgyuGVE03ry119/aQMTbmXhlhQOXjh44pYOvuRyGq1bt6aGDRvS9OnTRYsKt+hw8FOrVi2jz61Wq8W6N/yjvFOnTqK158UXXxQtJrwWDtu2bZteiwqfB/l8+N9//2n/vWZkZIjH+e2336ht27Zkid/fwpy/ZW1R4Ug0d18eN9HxhwpgbfiPDeeo8FZY3FLDid5xSel0LymDYpPS6U58Gt14kEo3Hz7euHXnVnya2PZeuqf3GD5ujlSjtDfVDOHNh2qF+IhWGQQv+pIzkmWfQt9Y9wsHDHI9tym4NUP3bzu3OHB3kAZ3/TDNmjD54daYbt260ZEjR8RJmk/mHFgYw60X3IrCLRYcpGjOKxx4cKDCQQs/Drfqc6DCJ1QOmL755htRjgMaDq44f1IXdwfpLmfALRu6r5PduXOHJkyYQDt37hSvjR+Hz3f83OzcuXPih7omSGEc/OjinE1uNfL09Mxz8r906RLZElkDFY4oOSeFm7+464f78ziR9o033pCzWgCKxUFOsI+r2Izhbq245HSKvp9Kl2OT6CJvd3Iuo++niBaavy/EiU0jyNuFGpfzpcYROVtICUzZrlQcUJrS/SInR0fHPHXXPaYJjgvy45TzWK5du0Z//PEHbd26lVq1aiVa42fPnm30/BIWFkaLFy8WLSL8HBygcKuEBnf/8FQY3EXDrR0cRGkCKe4u4uDq8OHDedap8fDICWAZzwGWO8jnbp979+6JfBKug7Ozs+jC0X3uJ+Hnr1evngikcuMgy5bI+m3nLwf3O7777rsi6uQv0zvvvCOiYAAoHB7JFODpIrZ6YSXytMhcuJNEJ24+pBPR8XTiZjydv5NIt+PT6NejN8XGyvq6UesqpeiF6oFUt0wJjFICReATNAcBvHEryJgxYwwGKhwkcKsFBymabh0eUZobT4/x9ttvi64YDlT69u2rva1OnTqiJYTPTZrHKKg9e/bQ119/LfJSGHfxcKuORqVKlcQxbnnRtCodPHhQ7zE48ZZzZgICAmw+tUHWQIWbtObOnSs2AHg6LTI1QrzF1rtRzjHOmTl6/YHoItp7KY6O34inq/dSaMk/V8Tm5+FEL1QLpO71Q6lWiDe6iEAW/AOWWxi49Z27XzZs2EBVqlQxWLZEiRKie4anv+AcGO5y4XzI3Nzd3UUXEv9g5lyQnj17am/jLh9uceHghZNhOXDhvBYeEcRdPZr5vwzhRNsffviB6tevL3IxOKDSrCbMOBeFRx9xwMUJvZxMy11FTPPvi5971qxZIpiaNm2aGOXELUq//vqrSFDm67YCwwIAbBzP79KkvB+NfqES/fpuUzo2qQ0tfL0uda1TmrxcHCguKYNW/HuduszfQ+3m/k1L/r5M8alFWw0VwFScC8IjPzlI4IRX7o7h0TiGcO4j38bdNtzdM3LkSHHSN4QDAs4H4VYTTkPQtXTpUhGo8IgcbgXhoIZbPnKXy+3bb78Vw6m5VYRHNXH3EreMaHDdOSGWu3caNGgghinzMGimSTrlUVC7d+8Wz8UJyByUDRw4UOSo2FoLi6yjfooKo34AihcPu9536R79euQGbToVI0YtMXcne+rVqAy98Ww4BXkbz5exZEpIps1v1ARYF+4uevbZZ0UCrWauF0uXZg2jfgBA2XguluYV/cU2NTWT1h+/RT/su0rn7yTR4r+v0NI9V0XA8l6rCuTn4Sx3dQEsBg9D5qRc7ibi4GT48OHUtGlTqwlSzAldPwBQIDxDbp9nwmjziOa0tH8DeqZcScpSS/T9vmvUYuYOmrf9gkjWtRY8bX6HCh3Ehin0i9+gQYPEidvQxrdZG85L4VFLPDN7//79RRfQ77//Lne1FAldPwBQaJx8G/XHWTp5M15cL+fvTp92q0kNypaUu2pWwZa6fnh0jbH12/jvu26OB1gGdP0AgOyaRPjR70Oa0v9O3KKPN/5Hl2OTqfvCfTSgaVn6oH1lcnZASwQUDAciCEbAEHT9AECR523pXLs0bRvZgl6tnzNkknNXXl24j248SJG7egBg4RCoAIBZeLs50sxXatF3/euLfBaej6Xjl/+IUUOWOurHfbq72HgfAOSBQAUAzOr5yqVo43vPisnheL6Vft8doA0nbpElSslMERsAyAeBCgCYHa8VtPqdxtSuWiBlZKtp2E9HadWBnAXZAABMgUAFAIptuv75veuKIc08tjBy3Ulad/SG3NUCAAuDQAUAig0vZjitczVtsPL+muO05XSM3NUCAAuCQAUAihUvsjb1pWpiRJBaIhqx+hj9d9vwfBkAALkhUAGApzKEeXrXGvRseT9KycimN5cforikdLmrBQAWAIEKADwVDvYqmt+rLoX7udPNh6k0eu1xUvLE2Co7FbUIayE23lec5GTjW1pawcumphasrIlatmxJw4YNoxEjRlCJEiWoVKlStHjxYkpOTqYBAwaQp6cnlS9fnjZt2iTK82rDvJKxv78/ubq6ijVwePXiJ7l69apotVuzZo1YAZnvy9PRnz9/Xqx0XL9+fTENf/v27Sk2NlbvvkuWLBGrEvOsqTyV/ddff613+7hx46hixYpiJeNy5crRxIkTKTPz8crhU6ZModq1a9MPP/xAZcuWFTOtvvbaa2J6fDAjyYLFx8fzXzlxCQCW4VxMglRx/B9S2LgN0rI9V+SujqKlpqZKZ86cEZd58J9vY1uHDvpl3dyMl23RQr+sn5/hciZq0aKF5OnpKX300UfS+fPnxaW9vb3Uvn17adGiReLY4MGDJV9fXyk5OVkaMmSIVLt2bengwYPSlStXpK1bt0rr169/4vNwWT4PVK5cWfrzzz/F+/XMM89I9erVk1q2bCn9888/0pEjR6Ty5ctLgwYN0t7vxx9/lIKCgqRffvlFunz5srgsWbKktGzZMm0ZrvOePXvEc3BdSpUqJX366afa2ydPnix5eHhIL7/8snTy5Elp9+7dUmBgoPThhx+a/H7Z2vc33oTzNwIVAHjqOEDhQKXC+D+kC3cS5a6OYll6oPLss89qr2dlZUnu7u5Snz59tMdu374t/obv27dPevHFF6UBAwaY/DyaQGXJkiXaYz/99JM4tn37du2xqKgoqVKlStrrERER0sqVK/UeiwOTxo0bG32uWbNmiQBIN1Bxc3OTEhIStMfGjBkjNWrUyOTXYY1SzRSoYK0fAHjq+jYOo7/O3qVd52Npwm8n6ae3nhHN92CCpCTjt9nnWmPp7l3jZVW5urWuXiVzqVmzpk6V7MnX15dq1KihPcbdQTnVu0uDBw+mbt260ZEjR6ht27bUpUsXatKkSaGeS/O4uZ+Ln4dx99OlS5do4MCB9NZbb2nLZGVlie4bjdWrV9OXX34pyiYlJYnbcy+gx10+3I2lERQUpH0eMA8FdrwCgLXjoOTjLtXJxVFF+y/fp9+O3SSl4Wnz/Wf5i02RU+i7uxvfcq+0nF9ZV9eClS0ER0fHPJ+77jFNcKpWq0UOybVr12jkyJF069YtatWqFY0ePbpQz6V53NzH+HkYBx2Mc2aOHTum3U6dOkX79+8Xt+3bt0/kzHTo0IE2bNhAR48epfHjx1NGRsYTX6PmecA8EKgAgCxCS7rRsOcriP1PNp6llIwsUpq4lDixwdPBibT9+vWjH3/8kebOnUuLFi0qlufh1pXg4GC6fPmySOjV3cLDw0WZvXv3UlhYmAhOOCGXk3s5kIKnD10/ACCbt5qVo9UHo+n6/RSx4vKQ58rLXSWQyaRJk6hevXpUrVo1Sk9PF60YPCKnuEydOpXee+890dXTrl078ZyHDh0So49GjRolApPr16/TqlWrxCiijRs30rp164qtPmAcWlQAQDZODip6v21Fsb9w1yV6mKLfrA62w8nJiSIjI0WuSfPmzUVOCwcJxeXNN98Uw5N5CDTnsrRo0YKWLVumbVF56aWXRDfU0KFDxRBkbmHh4cnw9NlxRi1ZqISEBBENx8fH50lwAgDLoFZL1OHLv+lsTCK92zKCxrarTErAeSkeUR5iPykyidydCpenURRpaWl05coVcfLkuT4ALEl+319Tzt9oUQEA2WetHdkmp1Xlh/3XKCldebkqACAfBCoAILs2VUpROT93SkzLEjkrABrTp08XM8sa2nikEFg/JNMCgCJaVd5sVo4+XHeSvvvnCvVrHCam3Je1TnYqqh9cX7sP8hg0aBC9+uqrBm/j6fLB+iFQAQBFeLluaZq95ZxYB2jHuVhqUzVn0i65uDq60sG3DspaByAqWbKk2MB24WcCACiCi6M9datbWuyvPnhd7uoAgEIgUAEAxejRoIy45On1Y+JzrQAMADYJgQoAKEb5AA9qULYEqSWinw/Lm1SbkplCZeeWFRvvA4A8EKgAgKJ0rx8qLtcfvyVrPXiKqWvx18RmwdNNAVg8WQMVXnWSF3DKvQ0ZMkTOagGAjF6oGkiO9nZ0/k4Snb+TKHd1AMCWA5WDBw/S7du3tdvWrVvF8e7du8tZLQCQkbebIzWr4C/2N564LXd1QIH4Ry4vWmgtzwMKDlR4pczAwEDtxotQRUREiDUXAMB2dawRJC43nryNbhcbxmvv+Pj4GPyR+/bbb5OtTkvfv39/sT6Rg4MDdenSpUD347WLypQpI6ayDwoKoj59+tCtW4+7V3fu3EmdO3cWt7m7u4v1jVasWKH3GL/++qtYSZo/E02ZH374gWwmRyUjI0Ms7f3GG2+I7h9DeHVLXh9AdwMA69OmWilyslfRxbtJdCk2Se7qgMLwj1w3NzeyRdnZ2WKiO175uXXr1gW+33PPPUdr1qyhc+fO0S+//EKXLl2iV155RXs7L7rIC0LybSdOnKABAwZQ3759RQOCBs9nM378eNq3b5+2DG+bN2+mYiUpxOrVqyV7e3vp5s2bRstMnjyZf1rl2eLj459qXQGg+L2+ZL8UNm6DtHj3JVmePyk9SaIpJDbel0Nqaqp05swZcWmofsa21MzUApdNyUgpUFlTpaWlScOGDZP8/f0lZ2dnqWnTptKBAwe0t+/YsUP8/d6wYYNUo0YNUaZRo0bSyZMn9W7X3fgcwMLCwqTPP/9c+1h828KFC6WOHTtKrq6uUuXKlaW9e/dKFy5ckFq0aCG5ublJjRs3li5evKi9D++/9NJLUkBAgOTu7i7Vr19f2rp1q95ryP08uWVmZorX6O3tLZUsWVIaO3as1LdvX6lz587S09CvX79CP9fvv/8u2dnZSRkZGUbLdOjQQRowYEC+j1OnTh1pwoQJJn9/+bxd0PO3YlpUvv32W7FuQ3BwsNEyvAQ4r7So2aKjsSYIgLVqUTEnT2XnuVhZnp9bdqv6VxWbsVZeOfHKzsa2bmu66ZUNmB1gtGz7Ffrr5ZT9oqzBcqYaO3as+HW+fPlyOnLkCJUvX55eeOEFun//vl65MWPG0GeffSa6c7il5MUXX6TMzExq0qSJyA/hlXU1eYyjR482+nwfffSRaAE4duwYVa5cmXr16kXvvPOOOG8cOnRIdCEOHTpUWz4pKYk6dOhA27dvp6NHj1K7du3Ec1+/XvDJBj/99FPRPbJ06VLas2ePaOX/7bff8r0PP76xtYs0G69vVJzu378v6s3vsaOjo9FyfJ41Niswv5/83nELTfPmza1/Cv1r167Rtm3bRP9XfpydncUGANavZaUA+njjf3Tgyn1KTs8id+en++fKzdGNTr97+qk+p7VITk6mBQsWiBwTzcKBixcvFgMm+EcpBycakydPpjZt2oh9DmpCQkJo3bp1Yn0fb29vESRyDuOTcBeEZk2gcePGUePGjWnixIkiOGLDhw8XZTRq1aolNt1Ah593/fr1egFNfubNmycCoa5du4rrX331Ff3xxx/53od/jHMwlZ/iWjJg3Lhxoo4pKSn0zDPP6HXr5MbdRBw8fvPNN3mCl9KlS4tUDHt7e/r666+1n59VByocjQYEBFDHjh3lrgoAKESEvzuFlnSl6PuptPfSPdnX/lGapEjjuTv2Knu963dH3zVaNveCi1eHXy1y3Tj/gVtFmjZtqj3Gv9wbNmxI//33n15ZDih0T9CVKlXKU6YgOL9Co1SpnO8KJ5zqHuNEVG714FYablGZMmUKbdy4UbTWZGVlUWpqaoFbVPiEfefOHfGaNPjEXa9ePVKr1Ubvxwmw3LokhzFjxtDAgQNF48DUqVO1OSi5Wwx37NghgjoOLqtVq6Z3m6enpwi0+P3jFpVRo0ZRuXLlqGXLltYbqPAHyoFKv379xAcIAMD4j2fLigH0w/5rtOv8XQQqubg7ucteVkl0uzA0J15DxzRBBHcjcQvP7NmzReDACaqcXMoDO4oTB0JVq1bNt8yHH34oNnPz8/MTW8WKFalKlSoUGhpK+/fv1wsWd+3aJbrAPv/8cxHI5KZSqbSBFo/64aAyKirKugMV7vLhD45H+wAA6Hq2gp8IVPZf1s9reBp42vwGixuIfV5FmbuCoGB4mgknJyeRtxEWFiaOcQsLdyWMGDFCryyfKHnYLHvw4AGdP39enEQZPwaPcikOXDce5qvptuEWgqtXC96axN1S3ErDr0mTo8F15XwcPoErsetHlyZg4y4c3SHKnTp1Erk3BR3+zY+j+xhWGai0bdsW8yQAgEENyub8weZhynFJ6eTn8fRy1Pjv0pnYM9p9KDieY2Pw4MGiq4FPuhyIzJw5U+RGcNeDrmnTppGvr6846fPQV/7Fr5kbhCdc03QxcD4JD0k217DkChUqiLxIbj3g1hbOZ8mvy8aQYcOGidYEbmHgBF7OWeFgK7/ka3N0/Zw5c0a0/HBSbGJiojbw0QRIBw4cEK0h/L5xPsm///4rAqpnn32WSpQoIbrm+PVyQKlpTeHuHg5SOJenW7duFBMTow0WNYETv1aeR4Xvx8EJ5+PwPCqcj2TVgQoAgDEl3Z2oUilPOncnkQ5euU/tH00EB8o3Y8YMceLnicX4ZMonOJ5vg0+UucvxyfHChQviRPu///1PnBwZj0oZNGgQ9ejRg+7duycSbzmvxBzmzJkjWvL5OTg44kRTU+fm4vvwCZ2DAs5P4VYITt7l/eLUoUMHkWeiUadOHb2AmgNCHo3DrViMgzsOyvj940RnntSNRzlNmDBBO0CFE5n5fhyM8KbBE7BySwvj+7777rt048YN0VXGwRnPf8afT3Gy4zHKZKH4S8XNb5zUxMlRAGB9Jv1+ir7fd436NylLU17ST+wrTskZydphuZy4KkfuBid/XrlyhcLDw8WMotaET348CRm3QBiafdYScWDG3VY8+ohHEdm6tHy+v6acvxUzjwoAgCGNwn3F5f7L9+SuCoAebtXgkTGcV3Py5EnR3cUnZp7DBcwHgQoAKFqD8JyuAu7+SUzLacoGUAIeAcNzxTRo0EAMxeZghQeIaJKBwTyQowIAihbg6UKlfVzp5sNUOnkznppE+MldJTADHs5qwZkHAg/v5dFDULzQogIAilc7NCeH4Xh0/FN7Th65EeYdJjYlTqEPYCvQogIAilcr1Js2nrxNx6MfPrXn5HlTro4o+iyt5mDqsFkAa/reIlABAMWrFfKoReXG0wtUlICH6XIexK1bt8SCfXwdrTugdNylx/O8xMbGiu+vZrh5YSFQAQDFqxHiTSo7otvxaXQnIY1KeVnXUF1j+I88D+3ktWg4WAGwJDx/C0/2x9/jokCgAgCK5+bkQBVLedLZmETR/dO22pNX0y2q1MxUar4sZ2r03f13k6ujK8mBf43yH3teNK+4ppMHMDee9I5n4TVHCyACFQCwCNWCvUWg8t/txKcSqKglNR26dUi7Lyf+Y88L7OkusgdgKzDqBwAsQpUgT3H5323TpjkHAMuGQAUALEKVoJxpts/GIFABsCUIVADAIlQOzGlRuXY/hZLTs+SuDgA8JQhUAMAi+Ho4U4CnM/FkppyrAgC2AYEKAFgMdP8A2B4EKgBgMSo/5YRaPzc/sQGAfDA8GQAsLk/l/J2kYn8udyd3ih0TW+zPAwD5Q4sKAFiM8v45gcrl2OIPVABAGRCoAIDFKOfvLi7jkjLoYUqG3NUBgKcAgQoAWAx3ZwcK8s5Z5+dSbHKxT6HfcllLsfE+AMgDOSoAYFHKB3iIxQkvxSZRvbASxfY8PG3+rmu7tPsAIA+0qACARYnw9xCXl+4iTwXAFiBQAQCLEvEoT4VbVADA+iFQAQCLEhHg8VRyVABAGRCoAIBFKf+o6+favWTKyELuCIC1Q6ACABbF39OZ3JzsSS0R3XiQInd1AKCYIVABAItiZ2dHZUq6if3r94s3UHFzdBMbAMgHw5MBwOJwoMIrKBdnoMJT6Cd/iDwYALmhRQUALI6mReXaPXT9AFg7BCoAYHHCfJ9O1w8AyE/2QOXmzZv0+uuvk6+vL7m6ulKNGjXo0KFDclcLABQsVJOjUowtKmlZadRxZUex8T4A2GCOyoMHD6hp06b03HPP0aZNm8jf358uXLhAJUoU37TYAGD5wnzdtS0qkiSJBFtzy1Zn0x8X/tDuA4ANBiqffvophYaG0tKlS7XHwsPD5awSAFiA0j6upLLjhQOzKTYpnQI8cxYqBADrI2vXz/r166l+/frUvXt3CggIoDp16tDixYuNlk9PT6eEhAS9DQBsj5ODioK8XYu9+wcAbDxQuXz5Mi1YsIAqVKhAmzdvpsGDB9N7771Hy5cvN1g+KiqKvL29tRu3xgCAbSfUYuQPgHWTNVBRq9VUt25dmj59umhNefvtt+mtt96ihQsXGiwfGRlJ8fHx2i06Ovqp1xkAFBaoYOQPgFWTNVAJCgqiqlWr6h2rUqUKXb9+3WB5Z2dn8vLy0tsAwDaFlMgJVG49TJW7KgBgrYEKj/g5d+6c3rHz589TWFiYbHUCAMsQ7JOTQItABcC6yTrqZ+TIkdSkSRPR9fPqq6/SgQMHaNGiRWIDAMhP8KNk2uIKVHgKfWmyVCyPDQAW0qLSoEEDWrduHf30009UvXp1+uijj2ju3LnUu3dvOasFABYg2OdRoBKfJuZSAQDrJPuihJ06dRIbAIApAr1diOd5y8hS073kDPLzcJa7SgBgjVPoAwAUhqO9igI8nYut+4enze++trvYMIU+gHwQqACA5Xf/FEOgwtPm/3zmZ7FhCn0A+SBQAQCLD1RuPkSLB4C1QqACABa95g/DEGUA64VABQAsVrA35lIBsHYIVADAYhVnjgoAKAMCFQCwWMhRAbB+CFQAwOIDlbikdErPwsgcAGsk+4RvAACFVcLNkZwcVGLSt7sJ6RRaMmehQnNwc3SjpMgk7T4AyAMtKgBgsezs7KiUV86kb3cT08z+2LzeD2+8DwDyQKACABatlGfOyJ87CelyVwUAigECFQCwaKW8NIGKeVtU0rPSqf9v/cXG+wAgDwQqAGDRAh51/cSYOVDJUmfR8uPLxcb7ACAPBCoAYNECH7WocDItAFgfBCoAYNGKq+sHAJQBgQoAWEXXDwIVAOuEQAUArKJFBV0/ANYJgQoAWEWgkpieRcnpSHoFsDYIVADAonk4O5C7k73Yv5uIVhUAa4Mp9AHAKlpVLsclU0x8GoX7uZvlMXna/Luj72r3AUAeaFEBAKtJqDXnNPo8bb6/u7/YMIU+gHwQqACA1cylgpE/ANYHgQoAWNFcKubLUeFp84dsHCI2TKEPIB8EKgBg8QIeBSrmnEafp83/+tDXYsMU+gDyQaACABbP3zMnRyUWo34ArA4CFQCweH4eTuIyLgmBCoC1KVSgcunSJZowYQL17NmT7t7NGb63adMmOn36tLnrBwDwRAGPWlTi0KICYHVMDlR27dpFNWrUoH///Zd+/fVXSkpKEsePHz9OkydPLo46AgDky88jJ1BJSMui9KxsuasDAHIGKh988AF9/PHHtHXrVnJyymluZc8//zzt37/fnHUDACgQb1dHcrTPmevkXlKG3NUBADkDlZMnT1LXrl3zHA8ICKC4uDhz1QsAoMB4QjZfdyTUAlgjk6fQ9/Hxodu3b1N4eLje8aNHj1Lp0qXNWTcAgALz83QSw5PNlVDr6uhKV4Zf0e4DgIW0qLz22ms0btw4iomJEb9i1Go17dmzh0aPHk19+/Y16bGmTJkiHkN3q1y5sqlVAgAg/0d5KuYKVFR2KirrU1ZsvA8AFtKiMn36dBoyZAiFhoZSdnY2Va1aVVz26tVLjAQyVbVq1Wjbtm2PK+SAdRIBoPAJtXHIUQGwKiZHBZxAu3jxYpo4cSKdOnVKjPqpU6cOVahQoXAVcHCgwMDAQt0XAEDDz8yTvmVkZ9D47ePF/ietPiEn+8eDBwDg6Sl080WZMmXEVlQXLlyg4OBgcnFxocaNG1NUVJTRx01PTxebRkJCQpGfHwCsrUXFPIFKZnYmzd43W+xPaTkFgQqApQQqb7zxRr63f/fddwV+rEaNGtGyZcuoUqVKIkF36tSp1KxZM9FS4+npmac8BzFcBgDA2Oy0GPUDYOOByoMHD/SuZ2ZmisDi4cOHYi4VU7Rv3167X7NmTRG4hIWF0Zo1a2jgwIF5ykdGRtKoUaP0WlQ4VwYAQLPeD6bRB7DxQGXdunV5jvHIn8GDB1NERESRKsNDnytWrEgXL140eLuzs7PYAACMj/pBMi2ANTHLmDuVSiVaOj7//PMiPQ4n5vI6QkFBQeaoFgDYYI5KfGomZWSp5a4OAJiJ2SYH4AAjKyvLpPvw3Cu8dtDVq1dp7969YsZbe3t7sdghAICp0+g7qB5No5+M7h8Am+360c0RYZIkiUTYjRs3Ur9+/Ux6rBs3boig5N69e+Tv70/PPvusWC+I9wEATKFS2ZGvhxPdSUinuMQMCvLGbLIANhmo8FT5ubt9OLD47LPPnjgiKLdVq1aZ+vQAAPkm1HKgEpuUxm0sRXosnjb/1OBT2n0AsJBAZceOHcVTEwAAc82lklj0hFqeNr9aQDUz1AoAigILWACA1QUqsRiiDGBbLSo8RT4vGFgQR44cKWqdAAAKhXNU2D0zDFHmKfSn/z1d7H/Y7EPMTAug5EClS5cuxV8TAIAi8nXPCSbum2HUD0+hP3VXzkzYY5qMQaACoORAZfLkycVfEwCAIirpntP1cy8Zk74BWAvkqACAFbaoIFABsNlRP9nZ2WIGWl6P5/r165SRof8H4f79++asHwBAgZVEoAJgdUxuUeHVi+fMmUM9evSg+Ph4MQHcyy+/LOZTmTJlSvHUEgDAhECFu354MkoAsMFAZcWKFbR48WJ6//33ycHBQcwsu2TJEpo0aZKYVRYAQO5RP7zWT3JGttzVAQA5ApWYmBiqUaOG2Pfw8BCtKqxTp05iGn0AALm4OTmQi2POn7X7WEUZwDYDlZCQELG2D4uIiKAtW7aI/YMHD5Kzc07GPQCAXHy1I3+KNkTZxcGFDrx5QGy8DwAWEqjwCsfbt28X+8OGDaOJEydShQoVqG/fviav9QMAoNSEWnuVPTUo3UBsvA8ACh/189VXX9Hrr79OM2bM0B7jhNoyZcrQvn37RLDy4osvFlc9AQBMTqgFABtqURk/fjwFBwdT79696a+//tIeb9y4sRj5gyAFAJQ0l0pRp9HnKfRn7ZklNt4HAIUHKpxEu3DhQrp16xa1adOGwsPD6aOPPqLo6OjirSEAQCFG/hR1Gn2eQn/strFi430AUHig4urqKvJQduzYQRcuXKA+ffrQt99+KwKWdu3a0dq1aykzE/+YAUBemEYfwLoUagr9cuXK0bRp0+jKlSu0adMm8vX1pf79+1Pp0qXNX0MAABNgGn0A61KktX7s7OzEpG98ybNAokUFAOSGafQBrEuhAhXOS+EWFW5Z4XwVzlvh2Wo186sAAMilpId5kmkBwMKGJ/Pig7/++it99913YtRPUFAQ9evXT8ydwgELAIASoOsHwEYDlcDAQEpJSRFT5f/vf/+jF154QSxECACgxK6f1MxsSs3IJlcnTNYGYBOByoQJE8RIH39//+KtEQBAEXg4O5CTvYoystViGv0QJ7dCPQ5Pm7+j3w7tPgAoPFDhSd0AAJSOk/u5VSUmIU10/4SUKFygwtPmtyzb0uz1AwDToO8GAKwOptEHsMEWFQAAi5udtggjf3g22kWHF4n9t+u9TY72jmarHwAUHAIVALDiFpXCT6PP6/sM3TRU7Pev3R+BCoBM0PUDAFYHXT8ANtaiYkoi7Zw5c4pSHwAA882lgknfAGwjUDl69Kje9SNHjlBWVhZVqlRJXD9//jzZ29tTvXr1iqeWAACFWJgQk74B2Eigwism67aYeHp60vLly6lEiRLi2IMHD2jAgAHUrFmz4qspAEABoesHwIZzVD777DOKiorSBimM9z/++GNxGwCAYkb9IFABsL1AJSEhgWJjY/Mc52OJiYmFrsiMGTPERE0jRowo9GMAADCs9wNgw8OTu3btKrp5uPWkYcOG4ti///5LY8aMoZdffrlQlTh48CB98803VLNmzULdHwBAl++jHJWk9CxKy8wmF0fT1/txdnCmDT03aPcBwEIClYULF9Lo0aOpV69elJmZmfMgDg40cOBAmjVrlskVSEpKot69e9PixYtF9xEAQFF5uTqQg8qOstQSPUjJoCBvV5Mfw0HlQB0rdiyW+gFAMXb9uLm50ddff0337t0To4F4u3//vjjm7u5u6sPRkCFDqGPHjtS6desnlk1PTxddT7obAEBu3I1cQpNQiyHKALY54dvt27fFVqFCBRGgSJJk8mOsWrVKDHXm5NyC4HLe3t7aLTQ0tBA1BwBbUNQ8FZ5Cf9mxZWLjfQCwkECFW1JatWpFFStWpA4dOohghXHXz/vvv1/gx4mOjqbhw4fTihUryMWlYEuoR0ZGUnx8vHbjxwAAyG+IcmEDFZ5Cf8DvA8TG+wBgIYHKyJEjydHRka5fvy66gTR69OhBf/75Z4Ef5/Dhw3T37l2qW7euyHHhbdeuXfTll1+K/ezs7Dz3cXZ2Ji8vL70NAMAQzKUCYKPJtFu2bKHNmzdTSEiI3nHuArp27VqBH4dbZU6ePKl3jEcTVa5cmcaNGydmugUAKHrXT+EXJgQACwxUkpOT9VpSNDihlls8Copnt61evbreMc518fX1zXMcAMBUmEYfwEa7fnia/O+//14vu16tVtPMmTPpueeeM3f9AAAKpaS7o7jEqB8AG2tR4YCEu20OHTpEGRkZNHbsWDp9+rRoUdmzZ0+RKrNz584i3R8AQAMtKgA22qLC3TK8WvKzzz5LnTt3Fl1BPCMtz6cSERFRPLUEAHjKo34AwEJbVHi0D89fMn78eIO3lSlTxlx1AwAo8sKEhR31w9Pmr3lljXYfACwkUAkPDxdzpwQEBOSZX4VvMzSsGABArhaV+NRMysxWk6O9yuQp9LtX615MtQOAYuv64RloOYHW0Jo9BZ24DQCguJVwcyLNnype7wcArLxFZdSoUeKSg5SJEyfqDVHmVhReQbl27drFU0sAABPZq+zIx9WRHqRkijyVAE/TfkhlqbNo3X/rxH7XKl1FCwsAPH0F/pfHybKaFhWeqM3JKadZlfF+rVq1xKrKAABK6v4RgUohhiinZ6XTqz+/KvaTIpPIwQmBCoAcCvwvb8eOHdrZY7/44gtMXw8Aiufr7kyXYpMxjT6ALeWozJ07l7KysvIc53lUEhISzFUvAIAiwxBlABsMVF577TVatWpVnuNr1qwRtwEAKEXJIg5RBgALDFQ4adbQVPktW7YUtwEAKAUWJgSwwUAlPT3dYNdPZmYmpaammqteAABFhq4fABsMVBo2bEiLFi3Kc3zhwoVUr149c9ULAMBsgQoWJgSwXCaPt/v444+pdevWdPz4cbE4Idu+fTsdPHiQtmzZUhx1BAAo9KifwraoONk70dLOS7X7AGAhgUrTpk1p3759YhVlTqB1dXWlmjVr0rfffksVKlQonloCADzlrh9He0fqX7t/MdQKAExRqBmMeAbalStXFuauAABPfWFCnkJfrZZIpcq7/AcAWFmOCrt06RJNmDCBevXqRXfv3hXHNm3aRKdPnzZ3/QAAirTeD1NLOYsTmjqF/sbzG8XG+wBgIYHKrl27qEaNGmIo8i+//CIWI2ScszJ58uTiqCMAQKE4OajI08WhUHOp8BT6nX7qJDbeBwALCVQ++OADkVC7detWvfV+nn/+edq/f7+56wcAYKa5VDDyB8AmAhVekLBr1655jgcEBFBcXJy56gUAYOaEWrSKANhEoOLj40O3b982uLpy6dKlzVUvAACzKPloiDKm0QewobV+xo0bRzExMWRnZ0dqtZr27NlDo0ePpr59+xZPLQEAitr1g0nfAGwjUJk+fTpVrlyZQkNDRSJt1apVqXnz5tSkSRMxEggAQEmwMCGAjc2jwgm0ixcvpokTJ9KpU6dEsFKnTh1M9gYAioRkWgAbnPCNlSlTRrSqMO4CAgCwptlpedr8r9p/pd0HAAua8I2ny69evTq5uLiIjfeXLFli/toBAJhrYUITAxWeQn9IwyFi430AsJAWlUmTJtGcOXNo2LBh1LhxY3GM1/4ZOXIkXb9+naZNm1Yc9QQAKOLChBieDGATgcqCBQtEjkrPnj21x1566SWxMCEHLwhUAECJybTc9SNJUoG7qrPV2fT39b/FfrMyzcheZV+s9QQAMwUqmZmZVL9+/TzH69WrR1lZWA8DAJSZTJuZLVFiehZ5uRSsGyctK42eW/6c2E+KTCJ3J/dirScAmClHpU+fPqJVJbdFixZR7969TX04AIBi5eJoT25OOa0hmEsFwEZG/XAy7ZYtW+iZZ54R13mBQs5P4QnfRo0apS3HuSwAAEpIqE3JSBUJtWX90DICYNWBCs+dUrduXbF/6dIlcenn5yc2vk0DQ5YBQEndPzcepGIuFQBbCFR27NhhtifnLiTerl69Kq5Xq1ZNjCpq37692Z4DAAALEwLYUI5KbGxsvisrmyIkJIRmzJhBhw8fpkOHDtHzzz9PnTt3ptOnT5taLQAAo7AwIYANBSo1atSgjRs35jk+e/ZsatiwoUmP9eKLL1KHDh3E9PsVK1akTz75hDw8PGj//v2mVgsAwChfzRBlJNMCWH/XDyfLduvWjQYMGCCSZe/fvy+SaLk1ZeXKlYWuSHZ2Nq1du5aSk5O1E8nllp6eLjaNhISEQj8fANiOwkyjz7PRzmw9U7sPABYSqIwdO5batGkjhinzJG8cqDRq1IhOnDhBgYGBJleAAxwOTNLS0kRryrp168SKzIZERUXR1KlTTX4OALBthZlGn9f3GdN0TDHWCgCKba2f8uXLi/V9OAmWWzV69OhRqCCFVapUiY4dOyaGOA8ePJj69etHZ86cMVg2MjKS4uPjtVt0dHShnhMAbAtWUAawoUBlz549oiXlwoULohWFR+3w1PkcrDx48MDkCjg5OYnAh2e25RaTWrVq0RdffGGwrLOzM3l5eeltAADF0fXDU+gfvHlQbLwPABYSqPDIHA5KOOG1SpUq9Oabb9LRo0fFhG+caFtUarVaLw8FAMBcCxPeM2F4Mk+h33BJQ7HxPgBYSI4Kz0jbokULvWMRERGipYVH7ZiCu3J4zpQyZcpQYmKiSMbduXMnbd682dRqAQA8cWHCtEw1pWRkkZtToSblBgAZmPyvNXeQoqFSqWjixIkmPdbdu3fFiKHbt2+Tt7e36FLiIIWTdQEAzMXdyZ6cHFSUkaWme0kZ5FYSgQqA1XX98HwnnMCqwRO1PXz4UHv93r17Rkfr5LdmECfkclcPBy3btm1DkAIAZsdLeiChFsDKAxVu6dDNHZk+fboYmqyRlZVF586dM38NAQDMOOmbKXkqAGBBgYokSfleBwBQMn+PnITauES0qABY/TwqAACWxt8zJ1CJTUKLCoAlcTClj5e33McAACwqUEksWKDC0+ZPbjFZuw8ACg9UuKunf//+YtI1xlPeDxo0iNzd3cV1zH0CAJbQ9VPQQIWn0J/Sckox1woAzBao8NT2ul5//fU8ZXioMQCAEvl7upgUqACAhQUqS5cuLd6aAAAoKEdFLanpv9j/xH4V/yqkskNKH4AcMOsRANgEU3NUUjNTqfqC6mI/KTKJ3J1yurkB4OnCTwQAsKlAJSk9S0yjDwCWAYEKANjMNPqujvZiH3OpAFgOBCoAYBN4OoXHeSpYDRnAUiBQAQCbYWqeCgDID4EKANgMv0fr/SBQAbAcCFQAwGagRQXA8mB4MgDYDH8PlwLPpcLT5o9uPFq7DwDyQKACADbYopJRoCn0Z7Wd9RRqBQD5QdcPANgMrKAMYHnQogIANheoxBUgR4Wn0L8ef13sl/Eugyn0AWSCQAUAbDKZlleE57lV8ptCP/yLcLGPKfQB5IOfCABgc8OTM7LVlJCKafQBLAECFQCwGc4O9uTtmjOCB7PTAlgGBCoAYFMCHnX/xMQjoRbAEiBQAQCbEuidM5dKTAJaVAAsAQIVALApgV45gcodBCoAFgGBCgDYZIvK7fhUuasCAAWA4ckAYJtdP0/IUXFQOdC79d/V7gOAPPCvDwBssusnJiH/FhVnB2ea33H+U6oVABiDrh8AsCkFbVEBAGVAoAIANtmiEpeUThlZaqPleOba2ORYsfE+AMgDgQoA2JSS7k7kZJ/zp+9uovGRPymZKRQwO0BsvA8A8kCgAgA2hdf3KeWtmfQNQ5QBlE7WQCUqKooaNGhAnp6eFBAQQF26dKFz587JWSUAsKmEWgQqAEona6Cya9cuGjJkCO3fv5+2bt1KmZmZ1LZtW0pOTpazWgBg5QK9XcUlWlQAlE/W4cl//vmn3vVly5aJlpXDhw9T8+bNZasXAFi3QC90/QBYCkXNoxIfHy8uS5YsafD29PR0sWkkJCQ8tboBgPW1qNxG1w+A4ikmmVatVtOIESOoadOmVL16daM5Ld7e3totNDT0qdcTAKxovR+0qAAonmJaVDhX5dSpU/TPP/8YLRMZGUmjRo3Sa1FBsAIAhV/vx3igwtPm96vVT7sPAPJQxL++oUOH0oYNG2j37t0UEhJitJyzs7PYAACKIqTEo2TahDTKylaTw6N5VXJPob+syzIZagcAiun64dkeOUhZt24d/fXXXxQeHi5ndQDARvh7OItJ37LVUr6tKgBg44EKd/f8+OOPtHLlSjGXSkxMjNhSU7H8OgAUH5XKjko/alW58SDV6A+p5IxksWEKfQAbDVQWLFggRvq0bNmSgoKCtNvq1avlrBYA2FD3z40HhqfH52nzPaI8xIYp9AFsNEcFv1IAQP5ABS24AEqmmOHJAABPU0gJN3GJQAVA2RCoAIBNelLXDwAoAwIVALBJ6PoBsAwIVADAprt+NHOpAIAyIVABAJuEuVQALIMiZqYFAJBrLpUrccmi+ye0ZE4Li4a9yp5eqfqKdh8A5IFABQBsOk+FA5Xo+ynUOMJX7zYXBxda232tbHUDgBzo+gEAmxXmm9OKcvVestxVAQAjEKgAgM0K9/MQl9yqAgDKhEAFAGxWuJ+b0UCF1/ixm2onNt4HAHkgUAEAm1XW111cXruXQmo1lvQAUCIEKgBgs3ikj73KjlIzs+lOIoYoAygRAhUAsFmO9ioKfTRDLfJUAJQJgQoA2LRwv5zuHwQqAMqEQAUAbFrZR4HKVQQqAIqEQAUAbFo5tKgAKBpmpgUAm6ZpUbmcK1DhafM7VOig3QcAeSBQAQCbVj7AQztEOT0rm5wd7LVT6G/stVHm2gEAun4AwKYFermQp4uDWEX5ciy6fwCUBoEKANg0Ozs7qhzoKfbP30mUuzoAkAsCFQCweRVL5QQqZ2MeByo8bb77dHexYQp9APkgRwUAbF4lTYuKTqDCUjJTZKoRAGigRQUAbF6lRy0q59D1A6A4CFQAwOZpWlRuPEilpPQsuasDADoQqACAzfNxc6JSXs5iHwm1AMqCQAUAQCeh9r/bCXJXBQB0IFABACCi6qW9xeWpm/FyVwUAdGDUDwAAEdUKyQlUjkfnBCoqOxW1CGuh3QcAeSBQAQAgopohPtqRP2mZ2eTq6Eo7+++Uu1oANg8/EwAAiCjI24X8PJzEVPpnkKcCoBgIVAAAHk2lr2lVORH9UO7qAIASApXdu3fTiy++SMHBweKPxG+//SZndQDAxtV4lFB74ka8mDbff5a/2DCFPoCNBirJyclUq1Ytmj9/vpzVAAAQapfJaVE5cv2BuIxLiRMbANhoMm379u3FBgCgBPXCSpCdHdHVeyl0JzFN7uoAgNyBiqnS09PFppGQgIQ3ADAfLxdHqhrkRadvJdDhqzmtKgAgL4tKpo2KiiJvb2/tFhoaKneVAMDKNAwvKS4PXb0vd1UAwNIClcjISIqPj9du0dHRclcJAKxMI02gcg0tKgBKYFFdP87OzmIDACguDcrmBCoX7iYRucpdGwCwqEAFAKC4+Xo4U+VATzoTk07lfWqRj5sjptAHsNVAJSkpiS5evKi9fuXKFTp27BiVLFmSypQpI2fVAMCGtajkT2djEqlT4DL6vEdtuasDYNNk/Zlw6NAhqlOnjtjYqFGjxP6kSZPkrBYA2LiWFQPE5e7zsaRWS3JXB8Cmydqi0rJlS5Ik/BEAAGWpX7YEeTg70L3kDDp1K147tT4APH3oeAUAyMXRXkWNynnQDec3qOWP1SglM0XuKgHYLOtIpk1OJrK3z3ucj7m46JczRqUicnUtXNmUFCJjLUM8zaWbW+HKpqYSqdXG6+HuXriyaWlE2dnmKcv15XoznowvK8s8Zfn95feZZWQQZWaapyx/HzTfFVPKcjkubwyPRnNwML0svwc6kxjm4eRE5Ohoeln+zPizM4bLcXlTy/J3jL9r5ijL74FmFB//m+B/G+Yoa8q/+3zKtivnQd9dvUsP0onUSYlELnb4G1GYsvgbkQN/I/KWLSjJgsXHx/O/Zik+55913q1DB/07uLkZLsdbixb6Zf38jJetX1+/bFiY8bJVq+qX5evGyvLj6OLnMVaW66eL62+sLL9uXfy+GCub+yvxyiv5l01Kely2X7/8y969+7jsu+/mX/bKlcdlR4/Ov+ypU4/LTp6cf9kDBx6XnTkz/7I7djwu+9VX+ZfdsOFx2aVL8y+7Zs3jsryfX1l+LA1+jvzKch01uO75leXXrsHvSX5l+T3V4Pc6v7L8WWnwZ5hfWf4OaPB3I7+y/N3S4O9cfmX5O6srv7L5/I1IciSJpuRsvI+/ETqbLvyNyIG/ESb9jdCev+PjpSdB1w8AAAAolh1HK2SheK0fnko//tYt8vLyylsAXT+Gy6JZ1/SyaNa1ua6f5Ixk8viylNh/3v4X2jaqHdnp/vvE34iClcXfiBz4G6FXVnv+jo83fP62ukClAC8UAMAUIlCJ8hD7oak/0+q3WlLjCF+5qwVgFUw5f6PrBwCgAFYeuC53FQBsEgIVAAAD7OzsqKp/VYrwqSyu/3nqNsUl5dOsDgDFAoEKAIABbo5udPrd03Rx+H9UJ6QUZWZLtAqtKgBPHQIVAIAnGNA0XFx+t+cqpWTkk+AJAGaHQAUA4Ak61QyiMiXd6H5yBv10IFru6gDYFAQqAAAG8LT51b6uJrYMdRoNahEhjn+z6xJaVQCeIgQqAAAG8MwNZ2LPiI33u9UrTaV9XOluYjot3n1F7uoB2AwEKgAABeDsYE8ftM8ZAbRw1yWKic9nAiwAMBsEKgAAJuSq1AsrQamZ2TR5/SnR0gIAxQuBCgCACXOrfNS5Ojmo7Gjz6Tu0/vgtuasEYPUQqAAAmKBqsBe916qC2J/0+2mKvp/PukMAUGQIVAAATDS4ZQTVCvGm+NRMGvTjYUrLzGdRPgAoEgQqAABGunnCvMPExvu6HO1V9PXr9aikuxOdvpVAY34+QWo18lUAigMCFQAAI1PoXx1xVWy8nxsPVf6qZx2Rr/K/47do8vrTSK4FKAYIVAAACqlJeT/67NVaxA0uP+y/RlP/dwYtKwBmhkAFAKAIOtcuTZ90qSH2l+29SsN+OoqcFQAzQqACAGBAamYqNVjcQGy8n59ejcrQlz3rkKO9HW08eZu6LdhLV+OSn1pdAawZAhUAAAPUkpoO3TokNt5/kpdqBdPyNxpSCTdHkWDbad4/tOZgNPJWAIoIgQoAgJk0ifCjP4Y3owZlS1BSehaN/eUE9Vi0n87fSZS7agAWC4EKAIAZBXm70k9vPUOR7SuTq6M9Hbhyn9rN3U2jVh+ja/fQHQRgKgQqAABm5mCvondaRNDWUc2pXbVA4oFAvx69Sc9/toveXXGY/r18D11CAAXkUNCCAABgmpASbrSwTz06eSOePtt6jnaei6U/TsaIrUKAB3WuHUydagZTWT93uasKoFh2kgWH9QkJCeTt7U3x8fHk5eUld3UAwIokZySTR5SH2E+KTCJ3p6IHE2djEuj7fddo3ZGbYgVmjeqlvei5SgHUrII/1SnjI2a+BbBmCSacvxGoAAAYCVTKflFW7F8dftUsgYoGrxG0+XSMmNF276V7lK0zSZyHswPVCytBtUN9qHYZH6od4kMl3J3M9twASoBABQDAQsQlpYsuob8v8BZH95Mz8pQJKeFKFUt5UoVSHlQhwJMqlvIQ3UVeLo6y1BnA5gKV+fPn06xZsygmJoZq1apF8+bNo4YNGz7xfghUAMCa8PT7Z24n0JHrD+jY9Yd0NPohXcln4jgvFweRB8OBDF8G+7iQv6dzzubhTH4ezuTj5phnUUUAuVlUoLJ69Wrq27cvLVy4kBo1akRz586ltWvX0rlz5yggICDf+yJQAQBr9zAlg87GJNKFu0l04U6imJPl4t0kikvK2/JiCM+W6+vuLFZ69nJ1EK0wXq6O5O3q+Gg/55iniwO5OTmQq5OKXB350p7cnOzJxTHnEnkzYLOBCgcnDRo0oK+++kpcV6vVFBoaSsOGDaMPPvgg3/siUAGA4sLT5rdf0V7sb+q9iVwdXUlJUjKy6OaDVLohthRxefNhquhKik1MF4EM58KYC68SzcELzw3j7KgSgYsTbw45+xwQaY456hx3csg5zhs/hkplR/Z2jy85/uF9cZu4nrNp9zVluVyuY3yd24q4wUhsfM2O9I/zfzmHxf8010WZR8dzGpx0r+fcX1OGcj9W7sfNJW8DVt5Shhq5DD+WXQHLGXq8gj3vk8pwAMuBrjmZcv6WdXhyRkYGHT58mCIjI7XHVCoVtW7dmvbt25enfHp6uth0XygAQHHgafN3Xdul3VcaPnlUEHkrnkbLpGdl072kDBG4PEjJoIS0LEpIzaSEtExKSM0SgUzOfiYlpmVRaka2GI2UkpEtFlbkYEiT55ullkQZ3sC2vFQrWKxlJRdZA5W4uDjKzs6mUqVK6R3n62fPns1TPioqiqZOnfoUawgAYLmcHewp2MdVbIXBDe4Z2Wq9AIb307PUlJn9eMvIkh5d6hzLzjmW+ehYeraasrMlypYkkYvDl9nqnLwc/WMSqR9ditvzHHu8z0GUiKMkSVxy/wDv8aW47VGHge7xnHI65Q3cV9PPwLep89xX3KK9bug907tusMyT72fsvmTovgWoh6FyButh4NHk7vazqAnfuOVl1KhRei0q3E0EAADmx90OHOzw5iN3ZcBmyRqo+Pn5kb29Pd25c0fvOF8PDAzMU97Z2VlsAAAAYBtkbc9xcnKievXq0fbt27XHOJmWrzdu3FjOqgEAAIACyN71w105/fr1o/r164u5U3h4cnJyMg0YMEDuqgEAAICtByo9evSg2NhYmjRpkpjwrXbt2vTnn3/mSbAFAHja3Bzd5K4CgM2TfR6VosA8KgAAANZ9/sZUgwAAAKBYCFQAAABAsRCoAAAYkJaVRh1XdhQb7wOAjSbTAgAoUbY6m/648Id2HwDkgRYVAAAAUCwEKgAAAKBYCFQAAABAsRCoAAAAgGIhUAEAAADFsuhRP5pJdXmGOwAAc0rOSCZ6NCqZ/8ZkO2HkD4C5aM7bBZkc36Kn0L9x4waFhobKXQ0AAAAohOjoaAoJCbHeQEWtVtOtW7fI09OT7OzsyBYiUA7M+IO1tbWN8Npt77Xb6uu25dduq6/bFl+7JEmUmJhIwcHBpFKprLfrh1/ckyIxa8RfYlv4IhuC1257r91WX7ctv3Zbfd229tq9vb0LVA7JtAAAAKBYCFQAAABAsRCoWBBnZ2eaPHmyuLQ1eO2299pt9XXb8mu31ddt66/9SSw6mRYAAACsG1pUAAAAQLEQqAAAAIBiIVABAAAAxUKgAgAAAIqFQEXBdu7cKWbcNbQdPHjQ6P1atmyZp/ygQYPI0pQtWzbP65gxY0a+90lLS6MhQ4aQr68veXh4ULdu3ejOnTtkKa5evUoDBw6k8PBwcnV1pYiICDESICMjI9/7WepnPn/+fPE5u7i4UKNGjejAgQP5ll+7di1VrlxZlK9Rowb98ccfZGmioqKoQYMGYkbtgIAA6tKlC507dy7f+yxbtizP58vvgaWZMmVKntfBn6e1f+aG/pbxxn+rrPnzNheLnpnW2jVp0oRu376td2zixIm0fft2ql+/fr73feutt2jatGna625ubmSJ+DXwa9HgP+75GTlyJG3cuFH8ceNZD4cOHUovv/wy7dmzhyzB2bNnxdIQ33zzDZUvX55OnTolXn9ycjLNnj3bqj7z1atX06hRo2jhwoUiSJk7dy698MIL4qTNJ/Dc9u7dSz179hQn+k6dOtHKlSvFSf7IkSNUvXp1shS7du0SJygOVrKysujDDz+ktm3b0pkzZ8jd3d3o/Xi2Ut2AxlKXDalWrRpt27ZNe93BwfhpyFo+c/5hmZ39eFFL/nfdpk0b6t69u9V/3mbBw5PBMmRkZEj+/v7StGnT8i3XokULafjw4ZKlCwsLkz7//PMCl3/48KHk6OgorV27Vnvsv//+4+H30r59+yRLNXPmTCk8PNzqPvOGDRtKQ4YM0V7Pzs6WgoODpaioKIPlX331Valjx456xxo1aiS98847kiW7e/eu+I7u2rXLaJmlS5dK3t7ekqWbPHmyVKtWrQKXt9bPnP+tRkRESGq12qo/b3NB148FWb9+Pd27d48GDBjwxLIrVqwgPz8/8asjMjKSUlJSyBJxVw9349SpU4dmzZolfoEac/jwYcrMzKTWrVtrj3GTcZkyZWjfvn1kqeLj46lkyZJW9ZlzVxZ/XrqfFa/dxdeNfVZ8XLc84xYYS/5sNZ8ve9JnnJSURGFhYWLhus6dO9Pp06fJEl24cEEsRFeuXDnq3bs3Xb9+3WhZa/zM+bv/448/0htvvJFvK4m1fN7mgK4fC/Ltt9+Kf6RPWoixV69e4gvOfwxOnDhB48aNE02Iv/76K1mS9957j+rWrSv+gHMTMJ98uStszpw5BsvHxMSQk5MT+fj46B0vVaqUuM0SXbx4kebNm/fEbh9L+8zj4uJEUzh/Nrr4Ond/GcKfoaHylvrZMu7mGzFiBDVt2jTfroxKlSrRd999RzVr1hSBDX8fuGuYT16WtDArd/Fx/gW/Hv63PHXqVGrWrJnoCjHUrWuNn/lvv/1GDx8+pP79+1v95202ZmubgQIbN26caOrNb+MuC13R0dGSSqWSfv75Z5Ofb/v27eIxL168KFnia9f49ttvJQcHByktLc3g7StWrJCcnJzyHG/QoIE0duxYydJe940bN0Tz8MCBAy36Mzfk5s2bon579+7VOz5mzBjRJWQId+utXLlS79j8+fOlgIAAyVINGjRIdHHyv29Tu4H5uzFhwgTJkj148EDy8vKSlixZYjOfedu2baVOnTrZ5OddWGhRkcH777+fbzTNuFlU19KlS0UXyEsvvVSoXzGaX+c8isTSXrvu6+CuHx4Zw784cgsMDBTNqvxrRbdVhUf98G2W9Lpv3bpFzz33nPgVtWjRIov+zA3hLip7e/s8I7Ly+6z4uCnllY4TvTds2EC7d+82+Veyo6Oj6A7lz9eS8b/TihUrGn0d1vaZX7t2TSQSm9rS6Wgln3dhIVCRgb+/v9gKipdj4kClb9++4gtrqmPHjonLoKAgsrTXnvt1cB6DoREhrF69euL94VFRPCyZcfcH94E3btyYLOV137x5UwQp/Hr4c+fXbMmfuSHcRcevjz8rHsWh6Qbh63wCN4Q/Q76du0o0tm7dKvtnayr+9zxs2DBat26dmIKAh6KbirvNTp48SR06dCBLxnkYly5doj59+lj1Z67B/57571fHjh1t8vMutEK3xcBTs23bNqNdItw9UKlSJenff/8V17mpn0cFHTp0SLpy5Yr0+++/S+XKlZOaN28uWRLuEuARP8eOHZMuXbok/fjjj2LEU9++fY2+dk1TepkyZaS//vpLvAeNGzcWm6Xg11S+fHmpVatWYv/27dvazdo+81WrVknOzs7SsmXLpDNnzkhvv/225OPjI8XExIjb+/TpI33wwQfa8nv27BFdf7Nnzxb/FngECXcNnDx5UrIkgwcPFiM6du7cqff5pqSkaMvkfu1Tp06VNm/eLP4tHD58WHrttdckFxcX6fTp05Ilef/998Xr5u8pf56tW7eW/Pz8xMgna/7MNaPa+G8TdwPnZq2ft7kgULEAPXv2lJo0aWLwNv4Hz0HMjh07xPXr16+LE1TJkiXFSYBPetzvHx8fL1kS/sfJwxD5Dzr/A61SpYo0ffp0vfyU3K+dpaamSu+++65UokQJyc3NTerataveSV7peFiisRwWa/zM582bJ/54c24R56bs379fb8h1v3799MqvWbNGqlixoihfrVo1aePGjZKlMfb58mdv7LWPGDFC+z6VKlVK6tChg3TkyBHJ0vTo0UMKCgoSr6N06dLium4elbV+5owDD/6cz507l+c2a/28zcWO/1f49hgAAACA4oN5VAAAAECxEKgAAACAYiFQAQAAAMVCoAIAAACKhUAFAAAAFAuBCgAAACgWAhUAAABQLAQqAAAAoFgIVAAAAECxEKgAAACAYiFQAQAAAMVCoAIAipKYmEi9e/cmd3d3CgoKos8//5xatmxJI0aMkLtqACADBCoAoCijRo2iPXv20Pr162nr1q30999/05EjR+SuFgDIxEGuJwYAMNSasnz5clq5ciW1atVKHFu6dCkFBwfLXTUAkAlaVABAMS5fvkyZmZnUsGFD7TFvb2+qVKmSrPUCAPkgUAEAAADFQqACAIpRrlw5cnR0pIMHD2qPxcfH0/nz52WtFwDIBzkqAKAYnp6e1K9fPxozZgyVLFmSAgICaPLkyaRSqcjOzk7u6gGADNCiAgCKMmfOHGrcuDF16tSJWrduTU2bNqUqVaqQi4uL3FUDABnYSZIkyfHEAAAFkZycTKVLl6bPPvuMBg4cKHd1AOApQ9cPACjK0aNH6ezZs2LkD+enTJs2TRzv3Lmz3FUDABkgUAEAxZk9ezadO3eOnJycqF69emLSNz8/P7mrBQAyQNcPAAAAKBaSaQEAAECxEKgAAACAYiFQAQAAAMVCoAIAAACKhUAFAAAAFAuBCgAAACgWAhUAAABQLAQqAAAAQEr1f8Jt5qVWnpiyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.3234411470490155)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_optimal_g(emp_states_dict_gauged, plot_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1c547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba0aa30",
   "metadata": {},
   "source": [
    "# Set bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c35f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(16, 8), nn.Linear(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6095c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Sequential' object has no attribute 'bias'"
     ]
    }
   ],
   "source": [
    "print(net.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a45cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range((len(net))):\n",
    "        if isinstance(net[i], nn.Linear):\n",
    "            net[i].bias.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8278753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net[1].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fea42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a4ef783",
   "metadata": {},
   "source": [
    "# Optimize threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_dataset_klds_gs_dict_with_optimal_threshold_(dataset, data_loader, model_kwargs, model_path_kwargs, binarize_threshold_range, num_hidden_layers_range, dataset_klds_dict = None, dataset_gs_dict = None, verbose=True):\n",
    "\n",
    "    \n",
    "    if dataset_klds_dict is None:\n",
    "        dataset_klds_dict = {\n",
    "            '2MNIST': [],\n",
    "            'MNIST': [],\n",
    "            'EMNIST': []}\n",
    "    if dataset_gs_dict is None:\n",
    "        dataset_gs_dict = {\n",
    "            '2MNIST': [],\n",
    "            'MNIST': [],\n",
    "            'EMNIST': []}\n",
    "\n",
    "\n",
    "    dataset_klds_dict[dataset] = []\n",
    "    dataset_gs_dict[dataset] = []\n",
    "\n",
    "\n",
    "    memoize_klds_dict = {}\n",
    "    memoize_gs_dict = {}\n",
    "\n",
    "\n",
    "    klds_sign_changes_lst, gs_distances_lst = compute_energy_addends_lst(\n",
    "        binarize_threshold_range=binarize_threshold_range, \n",
    "        dataset=dataset, \n",
    "        data_loader=data_loader, \n",
    "        model_kwargs=model_kwargs, \n",
    "        model_path_kwargs=model_path_kwargs, \n",
    "        num_hidden_layers_range=num_hidden_layers_range, \n",
    "        memoize_klds_dict=memoize_klds_dict, \n",
    "        memoize_gs_dict=memoize_gs_dict\n",
    "    )\n",
    "\n",
    "    \n",
    "    best_binarize_thresold, best_energy = optimize_binarize_threshold(\n",
    "        klds_sign_changes_lst, \n",
    "        gs_distances_lst, \n",
    "        binarize_threshold_range\n",
    "    )\n",
    "\n",
    "    dataset_klds_dict[dataset] = memoize_klds_dict[str(best_binarize_thresold)]\n",
    "    dataset_gs_dict[dataset] = memoize_gs_dict[str(best_binarize_thresold)]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"best_binarize_thresold: \", best_binarize_thresold)\n",
    "        print(f\"best_energy: \", best_energy)\n",
    "\n",
    "    return dataset_klds_dict, dataset_gs_dict\n",
    "\n",
    "\n",
    "def optimize_binarize_threshold(klds_sign_changes_lst, gs_distances_lst, binarize_threshold_range):\n",
    "    best_energy = float('inf')\n",
    "    best_binarize_thresold = None\n",
    "\n",
    "    for i, binarize_threshold in enumerate(binarize_threshold_range):\n",
    "        energy = (\n",
    "            10 * klds_sign_changes_lst[i] \n",
    "            + 1 * gs_distances_lst[i]\n",
    "        )\n",
    "        if energy < best_energy:\n",
    "            best_energy = energy\n",
    "            best_binarize_thresold = binarize_threshold\n",
    "\n",
    "    return best_binarize_thresold, best_energy\n",
    "\n",
    "\n",
    "\n",
    "def compute_energy_addends_lst(binarize_threshold_range, dataset, data_loader, model_kwargs, model_path_kwargs, num_hidden_layers_range, memoize_klds_dict, memoize_gs_dict):\n",
    "\n",
    "    klds_sign_changes_lst = []\n",
    "    gs_distances_lst = []\n",
    "\n",
    "    for binarize_threshold in binarize_threshold_range:\n",
    "\n",
    "        klds_lst, gs_lst = compute_klds_gs_lst_with_fixed_threshold(dataset, data_loader, model_kwargs, model_path_kwargs, binarize_threshold, num_hidden_layers_range, memoize_klds_dict, memoize_gs_dict)\n",
    "\n",
    "        klds_sign_changes_lst.append( count_sign_changes(klds_lst) )\n",
    "        gs_distances_lst.append( sum(abs(a - np.log(2)) for a in gs_lst) )\n",
    "\n",
    "    klds_sign_changes_lst = np.array(klds_sign_changes_lst) / max(klds_sign_changes_lst)\n",
    "    gs_distances_lst = np.array(gs_distances_lst) / max(gs_distances_lst)\n",
    "\n",
    "    return klds_sign_changes_lst, gs_distances_lst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_klds_gs_lst_with_fixed_threshold(dataset, data_loader, model_kwargs, model_path_kwargs, binarize_threshold, num_hidden_layers_range, memoize_klds_dict, memoize_gs_dict):\n",
    "\n",
    "    from AE.depth_utils import calc_hfm_kld_with_optimal_g\n",
    "\n",
    "    klds_lst = []\n",
    "    gs_lst = []\n",
    "\n",
    "    for num_hidden_layers in num_hidden_layers_range:\n",
    "\n",
    "        my_model = AE_0(\n",
    "            **model_kwargs,\n",
    "            hidden_layers=num_hidden_layers\n",
    "        ).to(device)\n",
    "        model_path = f\"../models/{model_path_kwargs['output_activation_encoder']}/{model_path_kwargs['initialization']}/{model_path_kwargs['train_type']}/{dataset}/ld{model_kwargs['latent_dim']}_lr{model_path_kwargs['learning_rate']}_dr{model_kwargs['decrease_rate']}_bias{model_path_kwargs['bias']}_{num_hidden_layers}hl_{model_path_kwargs['train_num']}.pth\"\n",
    "        my_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "        current_kld, current_g = calc_hfm_kld_with_optimal_g(my_model, data_loader, return_g=True, binarize_threshold=binarize_threshold)\n",
    "        \n",
    "        gs_lst.append(current_g)\n",
    "        klds_lst.append(current_kld)\n",
    "\n",
    "    if memoize_klds_dict is not None:\n",
    "        memoize_klds_dict[str(binarize_threshold)] = klds_lst\n",
    "    if memoize_gs_dict is not None:\n",
    "        memoize_gs_dict[str(binarize_threshold)] = gs_lst\n",
    "\n",
    "    return klds_lst, gs_lst\n",
    "\n",
    "\n",
    "\n",
    "def count_sign_changes(values):\n",
    "    \"\"\"\n",
    "    Counts the number of sign changes in the first differences of a sequence.\n",
    "    Args:\n",
    "        values (list or np.ndarray): Input sequence of numbers.\n",
    "    Returns:\n",
    "        int: The computed 'sign_changes' (number of sign changes).\n",
    "    \"\"\"\n",
    "    diff_0 = 0\n",
    "    sign_changes = 0\n",
    "    for a, b in zip(values[:-1], values[1:]):\n",
    "        diff_1 = b - a\n",
    "        if diff_0 == 0:\n",
    "            diff_0 = diff_1\n",
    "            continue\n",
    "        if np.sign(diff_1) != np.sign(diff_0):\n",
    "            sign_changes += 1*abs(diff_1)\n",
    "        diff_0 = diff_1\n",
    "    return sign_changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'input_dim': 28*28,\n",
    "    'latent_dim': 6,\n",
    "    'decrease_rate': 0.6,\n",
    "    'device': device,\n",
    "    'output_activation_encoder': nn.ReLU\n",
    "}\n",
    "models_paths_kwargs = {\n",
    "    'output_activation_encoder': 'relu_output',\n",
    "    'initialization': 'he init',\n",
    "    'train_type': 'simultaneous',\n",
    "    'learning_rate': 5e-4,\n",
    "    'bias': 0.0,\n",
    "    'train_num': 1\n",
    "}\n",
    "\n",
    "binarize_threshold_range = np.linspace(0.5, 9, 20)\n",
    "num_hidden_layers_range = range(1,7)\n",
    "\n",
    "dataset_klds_dict = {\n",
    "    '2MNIST': [],\n",
    "    'MNIST': [],\n",
    "    'EMNIST': []}\n",
    "dataset_gs_dict = {\n",
    "    '2MNIST': [],\n",
    "    'MNIST': [],\n",
    "    'EMNIST': []}\n",
    "\n",
    "compute_dataset_klds_gs_dict_with_optimal_threshold_(\n",
    "    dataset='MNIST', \n",
    "    data_loader=val_loader_MNIST, \n",
    "    model_kwargs=model_kwargs, \n",
    "    model_path_kwargs=models_paths_kwargs, \n",
    "    binarize_threshold_range=binarize_threshold_range, \n",
    "    num_hidden_layers_range=num_hidden_layers_range, \n",
    "    dataset_klds_dict=dataset_klds_dict, \n",
    "    dataset_gs_dict=dataset_gs_dict, \n",
    "    verbose=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c21bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_random_samples_from_loader(val_loader, num_samples=5, class_names=None, EMNIST=False):\n",
    "    \"\"\"\n",
    "    Plots `num_samples` random images from a given DataLoader (MNIST/EMNIST style).\n",
    "    Args:\n",
    "        val_loader: PyTorch DataLoader\n",
    "        num_samples: Number of random samples to plot\n",
    "        class_names: Optional list of class names for labels\n",
    "    \"\"\"\n",
    "    # Get a batch\n",
    "    images, labels = next(iter(val_loader))\n",
    "    indices = torch.randperm(images.size(0))[:num_samples]\n",
    "    plt.figure(figsize=(num_samples * 2, 2))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img = images[idx].squeeze().cpu().numpy()\n",
    "        if EMNIST == True:\n",
    "            img = np.rot90(img, k=1)\n",
    "            img = np.flipud(img)  # Flip upside down (mirror vertically)\n",
    "        label = labels[idx].item()\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5942533a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_random_samples_from_loader(val_loader_EMNIST, EMNIST=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5eb4547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_random_samples_from_loader(val_loader_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50e4f555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_random_samples_from_loader(val_loader_2MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f5fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
