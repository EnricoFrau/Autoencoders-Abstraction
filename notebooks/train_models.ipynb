{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7771820b",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e526374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107f4bcb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders')\n",
    "\n",
    "\n",
    "from AE.models import AE_0, ProgressiveAE\n",
    "from AE.train import train, layer_wise_pretrain_load_dict, train_ProgressiveAE\n",
    "from AE.datasets import MNISTDigit2Dataset, MNIST_digit2_translated_dataset, MNISTDigit2OnlyDataset\n",
    "from AE.plotter_functions import plot_original_vs_decoded\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "#–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device(\"cpu\")  # Fallback to CPU if no GPU is available\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa9424",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb36d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([64, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([64])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([64, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([64])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"MNIST\", \"EMNIST\", \"2MNIST\"]\n",
    "train_loaders = {\n",
    "    \"MNIST\": train_loader_MNIST,\n",
    "    \"EMNIST\": train_loader_EMNIST,\n",
    "    \"2MNIST\": train_loader_2MNIST\n",
    "}\n",
    "val_loaders = {\n",
    "    \"MNIST\": val_loader_MNIST,\n",
    "    \"EMNIST\": val_loader_EMNIST,\n",
    "    \"2MNIST\": val_loader_2MNIST\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929f0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 samples of digit '2'\n",
      "Found 1032 samples of digit '2'\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_2MNISTonly_train = MNISTDigit2OnlyDataset(train=True, download=True)\n",
    "train_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_2MNISTonly_val = MNISTDigit2OnlyDataset(train=False, download=True)\n",
    "val_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"MNIST\", \"EMNIST\", \"2MNISTonly\"]\n",
    "train_loaders = {\n",
    "    \"MNIST\": train_loader_MNIST,\n",
    "    \"EMNIST\": train_loader_EMNIST,\n",
    "    \"2MNISTonly\": train_loader_2MNISTonly\n",
    "}\n",
    "val_loaders = {\n",
    "    \"MNIST\": val_loader_MNIST,\n",
    "    \"EMNIST\": val_loader_EMNIST,\n",
    "    \"2MNISTonly\": val_loader_2MNISTonly\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137f72b",
   "metadata": {},
   "source": [
    "\n",
    "## FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e545536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ab4c4",
   "metadata": {},
   "source": [
    "\n",
    "## OTHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c223ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AE.datasets import Dataset_HFM, Dataset_pureHFM\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## train over pureHFM\n",
    "\n",
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM 32-1024\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc5dc6",
   "metadata": {},
   "source": [
    "# Simultaneous train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c49669",
   "metadata": {},
   "source": [
    "## 20 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c977213d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=5, EMNIST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d2b3778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=5, EMNIST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "814e6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "TRAINING ON EMNIST\n",
      "--------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m writer = SummaryWriter(log_dir=\u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m torch.save(new_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m ex_model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=device))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Tesi/Autoencoders/AE/train.py:24\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, epochs, train_loader, val_loader, optimizer, writer, scheduler, save_tensorboard_parameters)\u001b[39m\n\u001b[32m     21\u001b[39m model.train()\n\u001b[32m     22\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:143\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    139\u001b[39m img, target = \u001b[38;5;28mself\u001b[39m.data[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.targets[index])\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m     img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/PIL/Image.py:3326\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3323\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstrides\u001b[39m\u001b[33m'\u001b[39m\u001b[33m requires either tobytes() or tostring()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3324\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m3326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/PIL/Image.py:3208\u001b[39m, in \u001b[36mfrombuffer\u001b[39m\u001b[34m(mode, size, data, decoder_name, *args)\u001b[39m\n\u001b[32m   3206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _MAPMODES:\n\u001b[32m   3207\u001b[39m     im = new(mode, (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m3208\u001b[39m     im = im._new(\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   3209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3210\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImagePalette\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--------------------------------\")\n",
    "print(\"TRAINING ON EMNIST\")\n",
    "print(\"--------------------------------\\n\")\n",
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth', map_location=device))\n",
    "\n",
    "# 8 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld20_dr07_lr1e3_lwpretrain_8hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth', map_location=device))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------------\")\n",
    "print(\"TRAINING ON MNIST\")\n",
    "print(\"--------------------------------\\n\")\n",
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth', map_location=device))\n",
    "\n",
    "# 8 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld20_dr07_lr1e3_lwpretrain_8hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth', map_location=device))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--------------------------------\")\n",
    "print(\"TRAINING ON 2MNIST\")\n",
    "print(\"--------------------------------\\n\")\n",
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_7hl.pth', map_location=device))\n",
    "\n",
    "# 8 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=8, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld20_dr07_lr1e3_lwpretrain_8hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld20_dr07_lr1e3_lwpretrain_8hl.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf168b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784\n",
    "\n",
    "\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=20, decrease_rate=0.7, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld20_dr07_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=5, EMNIST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "731bd364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=5, EMNIST=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcebec4",
   "metadata": {},
   "source": [
    "\n",
    "## EMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab78264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715b8c4",
   "metadata": {},
   "source": [
    "### 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3a87c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006671493084018006, Validation loss: 0.0006706287850566367\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005703243172516161, Validation loss: 0.000574058924821463\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005192512599586225, Validation loss: 0.0005240801423589917\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005185240246718815, Validation loss: 0.0005222989016390861\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.000504827496186163, Validation loss: 0.000509214156624326\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld6_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434c476",
   "metadata": {},
   "source": [
    "\n",
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbeb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005579704764556695, Validation loss: 0.0005586765916582118\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004768919632836172, Validation loss: 0.00048095194969643306\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00045787642444054604, Validation loss: 0.00046297180169123283\n",
      "Epoch: 0/15, Average loss: 0.0011\n",
      "Epoch: 1/15, Average loss: 0.0009\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006312550977202701, Validation loss: 0.0006319056930852697\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_1hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_1hl.pth')\n",
    "\n",
    "# 2 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_2hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_2hl.pth')\n",
    "\n",
    "# 3 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_3hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_3hl.pth')\n",
    "\n",
    "# 4 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_4hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_ep15_dr05_lr1e3_opeSigm_4hl.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f2088",
   "metadata": {},
   "source": [
    "#### lw pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.000555559009105522, Validation loss: 0.0005578385744283491\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00046272703999627036, Validation loss: 0.000464878491661016\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00042365494312995925, Validation loss: 0.0004289189242619149\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004345204001325612, Validation loss: 0.0004371700959002718\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004821541091816909, Validation loss: 0.0004965193389340284\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005919927645950242, Validation loss: 0.0005861428737680012\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0009\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007159039917474626, Validation loss: 0.000713422570100173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld8_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1240705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "plot_original_vs_decoded(new_model, train_loader, device, num_samples=10, EMNIST=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3f4b1",
   "metadata": {},
   "source": [
    "### 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33b7861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004879725108365693, Validation loss: 0.0004913068522124531\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003956046629142254, Validation loss: 0.0003991217708135856\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003674436644287714, Validation loss: 0.0003687902345777826\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00037998655379967806, Validation loss: 0.00038173216870649064\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00047021761908136466, Validation loss: 0.00047327337439786245\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005338921935416441, Validation loss: 0.0005392646971852221\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006593303702414987, Validation loss: 0.0006568064168095589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/EMNIST/ld10_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/EMNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643b8da",
   "metadata": {},
   "source": [
    "### 20 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ac936",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecf1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac5bc2",
   "metadata": {},
   "source": [
    "### 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "261572ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004458057051834961, Validation loss: 0.00044437530282884837\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003826654177469512, Validation loss: 0.00037966157253831623\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00036050114215662084, Validation loss: 0.00036128411181271075\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003742689094506204, Validation loss: 0.00037566260807216166\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004360294774795572, Validation loss: 0.00043535569086670874\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004605742823332548, Validation loss: 0.000460664582811296\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004336616972771784, Validation loss: 0.00043706499096006154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld6_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b52b9",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077d281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/30, Average loss: 0.0008\n",
      "Epoch: 1/30, Average loss: 0.0006\n",
      "Epoch: 2/30, Average loss: 0.0005\n",
      "Epoch: 3/30, Average loss: 0.0005\n",
      "Epoch: 4/30, Average loss: 0.0004\n",
      "Epoch: 5/30, Average loss: 0.0004\n",
      "Epoch: 6/30, Average loss: 0.0004\n",
      "Epoch: 7/30, Average loss: 0.0004\n",
      "Epoch: 8/30, Average loss: 0.0004\n",
      "Epoch: 9/30, Average loss: 0.0004\n",
      "Epoch: 10/30, Average loss: 0.0004\n",
      "Epoch: 11/30, Average loss: 0.0004\n",
      "Epoch: 12/30, Average loss: 0.0004\n",
      "Epoch: 13/30, Average loss: 0.0004\n",
      "Epoch: 14/30, Average loss: 0.0004\n",
      "Epoch: 15/30, Average loss: 0.0004\n",
      "Epoch: 16/30, Average loss: 0.0004\n",
      "Epoch: 17/30, Average loss: 0.0003\n",
      "Epoch: 18/30, Average loss: 0.0003\n",
      "Epoch: 19/30, Average loss: 0.0003\n",
      "Epoch: 20/30, Average loss: 0.0003\n",
      "Epoch: 21/30, Average loss: 0.0003\n",
      "Epoch: 22/30, Average loss: 0.0003\n",
      "Epoch: 23/30, Average loss: 0.0003\n",
      "Epoch: 24/30, Average loss: 0.0003\n",
      "Epoch: 25/30, Average loss: 0.0003\n",
      "Epoch: 26/30, Average loss: 0.0003\n",
      "Epoch: 27/30, Average loss: 0.0003\n",
      "Epoch: 28/30, Average loss: 0.0003\n",
      "Epoch: 29/30, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00032756967861205337, Validation loss: 0.0003269697917625308\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/prova/MNIST/ld8_dr06_lr1e3_30ep_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=30)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/prova/MNIST/ld8_dr06_lr1e3_30ep_3hl.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d28ab",
   "metadata": {},
   "source": [
    "#### layer-wise pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00038860385194420817, Validation loss: 0.00038361831456422803\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003214299828434984, Validation loss: 0.0003194232942536473\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00030545842091863354, Validation loss: 0.0003037545906379819\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003437552148476243, Validation loss: 0.00034089266527444125\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035896782154838246, Validation loss: 0.0003574332971125841\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d96afbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00038005521170174085, Validation loss: 0.0003919882113113999\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00043801957465087376, Validation loss: 0.0004395412316545844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e2548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_original_vs_decoded(new_model, train_loader, device, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780836c",
   "metadata": {},
   "source": [
    "### 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14816e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003496197787734369, Validation loss: 0.0003449242809787393\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0003\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00028631444416629773, Validation loss: 0.00028309164820238946\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00026422937518606586, Validation loss: 0.0002626147655770183\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002833214360754937, Validation loss: 0.0002795711975544691\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003233633367344737, Validation loss: 0.0003292232652194798\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003849304151410858, Validation loss: 0.0003832368116825819\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004310954162850976, Validation loss: 0.00043273404873907565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/MNIST/ld10_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/MNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba0267",
   "metadata": {},
   "source": [
    "## 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "914dad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d779f",
   "metadata": {},
   "source": [
    "### 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea2dfcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006649616351972024, Validation loss: 0.0006466862354427576\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005671333520362775, Validation loss: 0.0005474678963422776\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005346114627706507, Validation loss: 0.0005170126728713513\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005592295238748192, Validation loss: 0.0005406487181782723\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005925827169790864, Validation loss: 0.0005777685467153788\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007368630743275086, Validation loss: 0.0007244323406368494\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0009\n",
      "Epoch: 2/15, Average loss: 0.0009\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0008\n",
      "Epoch: 11/15, Average loss: 0.0008\n",
      "Epoch: 12/15, Average loss: 0.0008\n",
      "Epoch: 13/15, Average loss: 0.0008\n",
      "Epoch: 14/15, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0007617240328341722, Validation loss: 0.0007485475596040488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1cb7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006561037523671985, Validation loss: 0.0006368833906948567\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005567022061596314, Validation loss: 0.0005379140861332416\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005331244395735364, Validation loss: 0.0005145324783399701\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006074252864345909, Validation loss: 0.0005900269061326981\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006252268606175979, Validation loss: 0.0006170182537287473\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006602966479336222, Validation loss: 0.0006464471396058798\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.000680868663949271, Validation loss: 0.0006651634700596332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_1.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_1.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_1.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_1.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_1.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_1.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_1.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d45fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006576664683098594, Validation loss: 0.0006402463529258966\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005569881114487847, Validation loss: 0.0005346119947731495\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005268071550875902, Validation loss: 0.0005075074099004269\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005506252358357112, Validation loss: 0.0005288476228713989\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006163592938954631, Validation loss: 0.0006006545588374138\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006715158810839057, Validation loss: 0.0006617596566677094\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007372868461534381, Validation loss: 0.0007235011868178845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_1hl_2.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_2hl_2.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_2.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_4hl_2.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_5hl_2.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_6hl_2.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_2')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_2.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_7hl_2.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a59f861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex_model = AE_0(input_dim=input_dim, latent_dim=6, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld6_dr06_lr1e3_lwpretrain_3hl_1.pth', map_location=device))\n",
    "\n",
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=10, EMNIST=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc6194",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_1hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_1hl.pth')\n",
    "\n",
    "# 2 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_2hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_2hl.pth')\n",
    "\n",
    "# 3 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_3hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_3hl.pth')\n",
    "\n",
    "# 4 hidden layers\n",
    "my_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.5, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_4hl')\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_ep15_dr05_lr1e3_opeSigm_4hl.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af3478",
   "metadata": {},
   "source": [
    "#### lw pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37b199b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005826798321679234, Validation loss: 0.0005628092616796494\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004772975029113392, Validation loss: 0.00046158193983137605\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044556478665520746, Validation loss: 0.0004335757439956069\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00047973332591354846, Validation loss: 0.0004637591078877449\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006045042552053928, Validation loss: 0.000587162808328867\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006223678158596158, Validation loss: 0.0006070775493979454\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0008\n",
      "Epoch: 11/15, Average loss: 0.0008\n",
      "Epoch: 12/15, Average loss: 0.0008\n",
      "Epoch: 13/15, Average loss: 0.0008\n",
      "Epoch: 14/15, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0007547314415375392, Validation loss: 0.0007641417760401964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=8, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld8_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld8_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df0cbf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_original_vs_decoded(ex_model, train_loader, device, num_samples=10, EMNIST=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfec7b",
   "metadata": {},
   "source": [
    "### 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "556d7c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005163868137945732, Validation loss: 0.0004988918887451291\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004150463686014215, Validation loss: 0.0003992401422932744\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003849295432679355, Validation loss: 0.00037595084719359875\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004318966383735339, Validation loss: 0.0004231390498578548\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005259496935022374, Validation loss: 0.0005039803432300687\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.000674611867715915, Validation loss: 0.0006577189192175865\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006525560999910036, Validation loss: 0.0006374304786324501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8698d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005152687742995719, Validation loss: 0.0004991521753370762\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004180147496672968, Validation loss: 0.00040302676931023597\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003808076212803523, Validation loss: 0.0003705016890540719\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004831920280431708, Validation loss: 0.0004709503239020705\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005684091319019596, Validation loss: 0.0005485634457319975\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006697737351680795, Validation loss: 0.0006507408153265715\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007404556368167201, Validation loss: 0.0007291659861803054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# 1 hidden layer\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=1, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "ex_model.load_state_dict(new_model.state_dict())\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_1hl_1.pth', map_location=device))\n",
    "\n",
    "# 2 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=2, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_2hl_1.pth', map_location=device))\n",
    "\n",
    "# 3 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=3, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_3hl_1.pth', map_location=device))\n",
    "\n",
    "# 4 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=4, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_4hl_1.pth', map_location=device))\n",
    "\n",
    "# 5 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=5, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_5hl_1.pth', map_location=device))\n",
    "\n",
    "# 6 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=6, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_6hl_1.pth', map_location=device))\n",
    "\n",
    "# 7 hidden layers\n",
    "new_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "ex_model = AE_0(input_dim=input_dim, latent_dim=10, decrease_rate=0.6, device=device, hidden_layers=7, output_activation_encoder=nn.Sigmoid, output_activation_decoder=None).to(device)\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl_1')\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "torch.save(new_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl_1.pth')\n",
    "ex_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/2MNIST/ld10_dr06_lr1e3_lwpretrain_7hl_1.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf56a9d",
   "metadata": {},
   "source": [
    "# 2_M_E_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae1e0b",
   "metadata": {},
   "source": [
    "## simultaneous train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e723e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004428318206531306, Validation loss: 0.00044095743726938966\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006665260378099609, Validation loss: 0.0006681937692647285\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003780051603913307, Validation loss: 0.000376651724986732\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005559340215466441, Validation loss: 0.000561785903620593\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035128639473890267, Validation loss: 0.00034963927548378705\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005153045336161047, Validation loss: 0.0005169869392634706\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035037008539463084, Validation loss: 0.0003546344969421625\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005136331942110778, Validation loss: 0.0005283514647051058\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003741685180614392, Validation loss: 0.0003809337055310607\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005142427292976079, Validation loss: 0.0005175580873966534\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004526857624451319, Validation loss: 0.0004516970505937934\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.000657988865199294, Validation loss: 0.0006636039439113216\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005026713530843456, Validation loss: 0.0005043669285252691\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.000725676582842167, Validation loss: 0.0007265734175180501\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003879630285004775, Validation loss: 0.00038440030496567486\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005599914458481239, Validation loss: 0.0005625995151460805\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003172559652167062, Validation loss: 0.00031433851569890973\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004602701553797468, Validation loss: 0.00046510988727529\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0003\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002971326175145805, Validation loss: 0.0002963588478974998\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00042659884011571076, Validation loss: 0.0004297780084047229\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002868361996021122, Validation loss: 0.00028717312309890986\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004163947349566147, Validation loss: 0.0004238389277870351\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003460515552821259, Validation loss: 0.0003475943502038717\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00047095767377565304, Validation loss: 0.0004694141965716126\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000382333462468038, Validation loss: 0.00038090774975717067\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005294064724329093, Validation loss: 0.0005315125878266198\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005401921780779958, Validation loss: 0.0005291044371202588\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0009\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0008\n",
      "Epoch: 4/15, Average loss: 0.0008\n",
      "Epoch: 5/15, Average loss: 0.0008\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0008\n",
      "Epoch: 11/15, Average loss: 0.0008\n",
      "Epoch: 12/15, Average loss: 0.0008\n",
      "Epoch: 13/15, Average loss: 0.0008\n",
      "Epoch: 14/15, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0007594319810786992, Validation loss: 0.000754297424424836\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003383276074193418, Validation loss: 0.0003339469493366778\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00047748861472776276, Validation loss: 0.0004782219627436171\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0003\n",
      "Epoch: 1/15, Average loss: 0.0003\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002740414647695919, Validation loss: 0.0002718835523352027\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00038268855049651356, Validation loss: 0.000382389464475056\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0003\n",
      "Epoch: 1/15, Average loss: 0.0003\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002588593672029674, Validation loss: 0.00025644850740209223\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035920536895249543, Validation loss: 0.0003614151459979884\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0003\n",
      "Epoch: 2/15, Average loss: 0.0003\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002744783790161212, Validation loss: 0.0002707174599170685\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003747661139300846, Validation loss: 0.00037657863926142453\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0004\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003158732164030274, Validation loss: 0.0003147346937097609\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044945134901844546, Validation loss: 0.0004553577008637342\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00041954548321664336, Validation loss: 0.0004175068153068423\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006018036282588299, Validation loss: 0.0006065356574873341\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000428797900583595, Validation loss: 0.00042958058398216963\n",
      "------------EMNIST------------\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006253826819320625, Validation loss: 0.0006254306944840132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for latent_dim in (6, 8, 10):\n",
    "    print(f\"-----------------------Training models with {latent_dim} latent_dim----------------------\\n\")\n",
    "    for num_hidden_layers in range(1,8):\n",
    "        print(f\"----------------- {num_hidden_layers} num_hidden_layers --------------\")\n",
    "        for dataset in (\"MNIST\", \"EMNIST\"):\n",
    "            print(f\"------------{dataset}------------\")\n",
    "            \n",
    "            input_dim = 784\n",
    "            learning_rate = 1e-3\n",
    "            weight_decay = 1e-5\n",
    "            decrease_rate = 0.6\n",
    "            decrease_rate_str = \"06\"\n",
    "            train_loader = train_loaders[dataset]\n",
    "            val_loader = val_loaders[dataset]\n",
    "            input_dim = 28 * 28\n",
    "\n",
    "            my_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers=num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            model_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/simultaneous train/2_M_E_MNIST/ld{latent_dim}_dr{decrease_rate_str}_lr1e3_lwpretrain_{num_hidden_layers}hl.pth\"\n",
    "            my_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            writer_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/simultaneous train/2_M_E_MINST/ld{latent_dim}_dr{decrease_rate_str}_lr1e3_lwpretrain_{num_hidden_layers}hl.pth\"\n",
    "            writer = SummaryWriter(log_dir = writer_path)\n",
    "            optimizer = optim.Adam(my_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "            torch.save(my_model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce9ed8",
   "metadata": {},
   "source": [
    "## progressive train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4de134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 1 completed. Final training loss: 0.0006927045747016867, Validation loss: 0.0007599017009139061\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 2 completed. Final training loss: 0.0008131742510323724, Validation loss: 0.000806530949100852\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 3 completed. Final training loss: 0.0006828688252717256, Validation loss: 0.0006799816485494375\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 4 completed. Final training loss: 0.0006012092467397452, Validation loss: 0.0005991118101403118\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0005340180429940423, Validation loss: 0.0005295493200421333\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.0004742164734130104, Validation loss: 0.00046736855152994396\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011906148756173909, Validation loss: 0.0011979010855739422\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010557263431713935, Validation loss: 0.0010499973293948681\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0009564110307115401, Validation loss: 0.0009516866201970805\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008618380532910427, Validation loss: 0.0008570579776262983\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.000764880458269506, Validation loss: 0.0007628008239763848\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0006879823909847871, Validation loss: 0.0006834658090659278\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 1 completed. Final training loss: 0.0005268019605117539, Validation loss: 0.0005907089810818434\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0007242313789824644, Validation loss: 0.0007228335853666067\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005908727482582132, Validation loss: 0.0005867694167420269\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0005133603177033364, Validation loss: 0.0005068997014313937\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004646363638341427, Validation loss: 0.00045713354106992484\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004121637710680564, Validation loss: 0.0003993100717663765\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011331493853994296, Validation loss: 0.0011261472383395155\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009913838263733168, Validation loss: 0.000983384772540724\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.000883913665798539, Validation loss: 0.0008779233993288684\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007869533447429855, Validation loss: 0.0007791829658077752\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0006882967832927586, Validation loss: 0.000683078611309224\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0006066028189561362, Validation loss: 0.0006026617138388943\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.000439000096047918, Validation loss: 0.0004780499082058668\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006918085407465696, Validation loss: 0.0006654777217656374\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005626603291369975, Validation loss: 0.0005540784917771816\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004883364547664921, Validation loss: 0.0004861886283382773\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004633343919180334, Validation loss: 0.0004611680544912815\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.00043889536922797563, Validation loss: 0.00042049930933862927\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011248600949262473, Validation loss: 0.001122594944577902\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009907257138480638, Validation loss: 0.0009642261011090051\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008662118214506207, Validation loss: 0.0008531099855107196\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007671196207219193, Validation loss: 0.0007566796041073951\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0007011983575339013, Validation loss: 0.0006911968913404866\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0006626914971646793, Validation loss: 0.000661646908030231\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.00043732685564706723, Validation loss: 0.00044837653562426565\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006776149315138658, Validation loss: 0.0006615192402154207\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005493492738654216, Validation loss: 0.0005437070731073618\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004803782521436612, Validation loss: 0.00047034557964652777\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004605147774331272, Validation loss: 0.0004529846977442503\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00045034975750992697, Validation loss: 0.0004464749922975898\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010950630191541837, Validation loss: 0.001088639498549573\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009640552542429972, Validation loss: 0.0009678702644924535\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008811046350218936, Validation loss: 0.0008637242704788421\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.000781801733657295, Validation loss: 0.0007698318195786882\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007521270833090476, Validation loss: 0.0007440005878898057\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0007356116778699431, Validation loss: 0.0007270912950898105\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0004489837648657461, Validation loss: 0.00045485249366611244\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006812544104953607, Validation loss: 0.0006853860575705766\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005353919779260953, Validation loss: 0.0005302185470238328\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.00047756761697431406, Validation loss: 0.00047015577387064697\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004555855798535049, Validation loss: 0.00045195235088467595\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004443435338015358, Validation loss: 0.00046584336012601854\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.001135370027049319, Validation loss: 0.0011357989871597037\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0010012711134098522, Validation loss: 0.0010236076376539595\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0009708450660968186, Validation loss: 0.0009777139997149402\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 4 completed. Final training loss: 0.0009649558777504779, Validation loss: 0.0009300639323810948\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 5 completed. Final training loss: 0.0008851507395928633, Validation loss: 0.0008855720555924989\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 6 completed. Final training loss: 0.0008782358616194193, Validation loss: 0.0008569081630954082\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 1 completed. Final training loss: 0.0004977880665101111, Validation loss: 0.0004912766128778457\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 2 completed. Final training loss: 0.0006446036618823806, Validation loss: 0.0006596688367426396\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005583394639194012, Validation loss: 0.0005452965442091227\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004773197522697349, Validation loss: 0.0004669518809765577\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004698488690890372, Validation loss: 0.0004601802924647927\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004460210152591268, Validation loss: 0.0004779092511162162\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011455077512837048, Validation loss: 0.0011279561699546398\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009901808339653287, Validation loss: 0.0009740452452542934\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0009496462252981802, Validation loss: 0.0009505876242876687\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.000946633494426067, Validation loss: 0.0009301574282506679\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 5 completed. Final training loss: 0.0008772764234070449, Validation loss: 0.0008709966177318959\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 6 completed. Final training loss: 0.0008541599005883467, Validation loss: 0.00088212004664572\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010522316474467516, Validation loss: 0.001060774326696992\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010521374105165401, Validation loss: 0.0010610620815306902\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 3 completed. Final training loss: 0.0010521913774311543, Validation loss: 0.0010600983072072267\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 4 completed. Final training loss: 0.0010521491452430685, Validation loss: 0.001061267391219735\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 5 completed. Final training loss: 0.0010521018836026391, Validation loss: 0.0010596711602061987\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 6 completed. Final training loss: 0.001052039536461234, Validation loss: 0.0010596682354807854\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.0013222707806696706, Validation loss: 0.0013235624047352912\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.001322169362085191, Validation loss: 0.0013234228533791734\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.0013221355201346232, Validation loss: 0.0013234171092985792\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 4 completed. Final training loss: 0.001322035214513646, Validation loss: 0.0013235971264224102\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 5 completed. Final training loss: 0.0013220985636696325, Validation loss: 0.0013231675425901058\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 6 completed. Final training loss: 0.0013221039465820114, Validation loss: 0.0013231667903985116\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 1 completed. Final training loss: 0.0006970697753131389, Validation loss: 0.0008236701764166355\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 2 completed. Final training loss: 0.0008068074750403563, Validation loss: 0.0008147586755454541\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 3 completed. Final training loss: 0.0006903573550283909, Validation loss: 0.0006785782847553492\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 4 completed. Final training loss: 0.0006094853981708487, Validation loss: 0.0005995642196387052\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0005360760197974741, Validation loss: 0.0005246367825195193\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00047432957186053197, Validation loss: 0.0004645771041512489\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.0004294468635072311, Validation loss: 0.0004205947998911142\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00039253108855336906, Validation loss: 0.0003847051365301013\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0012074759317205308, Validation loss: 0.0012037688362947169\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010739231097069404, Validation loss: 0.0010711919135869817\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0009559603393117481, Validation loss: 0.0009526354793776223\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008641122731690605, Validation loss: 0.0008607120342314878\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007799407190491333, Validation loss: 0.0007747295996213847\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0006999279160369583, Validation loss: 0.0006970081607157245\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0006324638112230187, Validation loss: 0.0006289437902655373\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005723521888507068, Validation loss: 0.0005684657053070816\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 1 completed. Final training loss: 0.0005174268611706793, Validation loss: 0.0006243443053215742\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0007024981207524736, Validation loss: 0.0007118074357509613\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005964679457247257, Validation loss: 0.0005892706453800202\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0005185149403909842, Validation loss: 0.0005117224231362343\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00047753668492659925, Validation loss: 0.0004650733195245266\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.00043749901686484616, Validation loss: 0.00042421076074242593\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.0003934833775895337, Validation loss: 0.00038593850303441285\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0003590130646713078, Validation loss: 0.00035119304060935976\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011472460458464657, Validation loss: 0.0011374577808570354\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009878753353874628, Validation loss: 0.000984804989730424\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008854239279443913, Validation loss: 0.0008843531945046592\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007897701761388082, Validation loss: 0.0007841885801246191\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.000697255715404155, Validation loss: 0.000691377326965015\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0006318882782778419, Validation loss: 0.0006208025465937371\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0005530696938389001, Validation loss: 0.0005473036540949598\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.0004931690809250881, Validation loss: 0.0004861066641999369\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0004003981835519274, Validation loss: 0.00045987285897135737\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006906611067553361, Validation loss: 0.0006661454353481531\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005517336678380767, Validation loss: 0.0005427151136100292\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.00048126170750086505, Validation loss: 0.00047356631867587564\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00045718791236480075, Validation loss: 0.0004460660059005022\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004160346661073466, Validation loss: 0.0004125420812517405\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00040167291856681305, Validation loss: 0.00039821673519909383\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0003689194458226363, Validation loss: 0.0003648882931098342\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011383626069080956, Validation loss: 0.0011352264021146806\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009896479775561085, Validation loss: 0.0009820942128909394\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.000868615898805677, Validation loss: 0.0008648613707578562\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007607772013383872, Validation loss: 0.0007487727323190329\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0006741011149005581, Validation loss: 0.0006675346330442327\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0006382819835479695, Validation loss: 0.000631787199725179\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0005846045017638739, Validation loss: 0.0005775956287385618\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005537547601770002, Validation loss: 0.0005408233571599456\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0003931311005416016, Validation loss: 0.0004444717638194561\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 2 completed. Final training loss: 0.0008076295518626769, Validation loss: 0.0008678792998194695\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0006065078391072651, Validation loss: 0.0005867559742182494\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.000507512249580274, Validation loss: 0.0004891613909974694\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00046973251532763245, Validation loss: 0.0004703281804919243\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00045263511737187703, Validation loss: 0.00044740438349545\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00041518517018606266, Validation loss: 0.000409519368596375\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0003967570397381981, Validation loss: 0.00039505604077130557\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011249628270718645, Validation loss: 0.001109520351870897\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.000984134550886691, Validation loss: 0.0009730456442512731\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008722381222438305, Validation loss: 0.0008495418515373417\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008720683724560002, Validation loss: 0.000852104930722333\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007913097030489792, Validation loss: 0.0007788750418323151\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007856042554985124, Validation loss: 0.0008239957188909992\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0007399163317556183, Validation loss: 0.0007097970555595896\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 8 completed. Final training loss: 0.0006699894720371416, Validation loss: 0.0006631651465246018\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.00037182164459178846, Validation loss: 0.0004018053434789181\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006861587198451161, Validation loss: 0.0006509322859346866\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005341377490820984, Validation loss: 0.0005274142436683178\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004734764489966134, Validation loss: 0.0004648539483547211\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00045153471644346913, Validation loss: 0.0004488634496927261\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004380714106063048, Validation loss: 0.00044098284803330897\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.0004341151574937006, Validation loss: 0.0004359154364094138\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00042497169580310584, Validation loss: 0.0004155531693249941\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011548836580774885, Validation loss: 0.0011196599710494913\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009844828991827073, Validation loss: 0.0009736315371032725\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008892571143079417, Validation loss: 0.0008718477956395834\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008706677519775769, Validation loss: 0.0008563940621357649\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008202067764264577, Validation loss: 0.0008174297915018619\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007708230640752412, Validation loss: 0.0007831027196284305\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 7 completed. Final training loss: 0.0007629667227453691, Validation loss: 0.0007574497741904665\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 8 completed. Final training loss: 0.0007487091717990578, Validation loss: 0.0007501882530669583\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0004363947687981029, Validation loss: 0.00043100578393787147\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006804844630261262, Validation loss: 0.000651557469367981\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005433001269586384, Validation loss: 0.0005359754590317607\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.000491252348665148, Validation loss: 0.00047153559178113937\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004554356487157444, Validation loss: 0.0004661198018118739\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004439489159733057, Validation loss: 0.00044128862004727125\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00043398937132830423, Validation loss: 0.0004319794239476323\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0004084352230342726, Validation loss: 0.00040376572478562593\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011543101724237205, Validation loss: 0.0011997049674391746\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009953727862791072, Validation loss: 0.000997168630995649\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0009308416159916009, Validation loss: 0.0009433091263425477\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008857104369828887, Validation loss: 0.0008657421284612823\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007909847992465745, Validation loss: 0.0007878616981287586\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007612385186663967, Validation loss: 0.0007483282960713544\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0007417206908080806, Validation loss: 0.0007435997694413712\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 8 completed. Final training loss: 0.0007365731978194511, Validation loss: 0.0007518861272392121\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.001052157508333524, Validation loss: 0.0010597275875508785\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010521022014319896, Validation loss: 0.0010599061910063028\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 3 completed. Final training loss: 0.0010521244106814266, Validation loss: 0.0010601713206619024\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 4 completed. Final training loss: 0.001052065631126364, Validation loss: 0.001060448046028614\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 5 completed. Final training loss: 0.0010520244739949704, Validation loss: 0.0010600443530827761\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 6 completed. Final training loss: 0.0010520756078884006, Validation loss: 0.0010605028744786978\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 7 completed. Final training loss: 0.0010520479831223686, Validation loss: 0.0010601362206041813\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 8 completed. Final training loss: 0.0010519833424439034, Validation loss: 0.0010601498238742351\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.0013221686233019997, Validation loss: 0.0013232392140049884\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0013220885069050053, Validation loss: 0.0013233006262081736\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.001322059919397142, Validation loss: 0.0013234797322211112\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 4 completed. Final training loss: 0.0013220233561352212, Validation loss: 0.0013237047151840747\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 5 completed. Final training loss: 0.001322007937792768, Validation loss: 0.001323317825952743\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 6 completed. Final training loss: 0.0013220622128285204, Validation loss: 0.001323583573102951\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 7 completed. Final training loss: 0.0013219996214124327, Validation loss: 0.0013235132491334956\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 8 completed. Final training loss: 0.0013220849410624158, Validation loss: 0.0013236274764417335\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 1 completed. Final training loss: 0.0007263300739849607, Validation loss: 0.0008567506939172744\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 2 completed. Final training loss: 0.0008001977487156789, Validation loss: 0.0007894042406231165\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 3 completed. Final training loss: 0.0006761535205567876, Validation loss: 0.0006722327534109353\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 4 completed. Final training loss: 0.0005968081469337145, Validation loss: 0.0005899140240624547\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0005279859988639752, Validation loss: 0.0005197754886001348\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00047528878099595506, Validation loss: 0.00046636757124215365\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.000430425297220548, Validation loss: 0.00042113019060343503\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00039013267749299607, Validation loss: 0.00038181617185473443\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 9 completed. Final training loss: 0.0003561607581252853, Validation loss: 0.00034726879354566334\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0003\n",
      "Epoch: 2/2, Average loss: 0.0003\n",
      "Training of neuron 10 completed. Final training loss: 0.0003283960133480529, Validation loss: 0.0003205616381019354\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011994516666555869, Validation loss: 0.0012020043854383711\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010623301874766958, Validation loss: 0.0010601389602302236\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0009580485698773612, Validation loss: 0.0009535427775947338\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008609627890037307, Validation loss: 0.000854809550330677\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007735677580976634, Validation loss: 0.0007707300429490018\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0006973420668406583, Validation loss: 0.0006940471468136665\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0006307324046163377, Validation loss: 0.0006277899515438587\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005733316520671525, Validation loss: 0.0005718032559974397\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 9 completed. Final training loss: 0.0005263712849626515, Validation loss: 0.0005245632909119446\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 10 completed. Final training loss: 0.00048508380415869203, Validation loss: 0.00048375409324356215\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 1 completed. Final training loss: 0.0005509982977993786, Validation loss: 0.0006703399688005447\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0007125689261903366, Validation loss: 0.000691098428145051\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005861913949872057, Validation loss: 0.0005776254175230861\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0005093985385882357, Validation loss: 0.0005037671182304621\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00048121681809425355, Validation loss: 0.0004656644882634282\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004377330249796311, Validation loss: 0.0004318022524937987\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00039568318836390973, Validation loss: 0.00038734890576452017\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00036047007516026494, Validation loss: 0.00035284289978444576\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0003\n",
      "Training of neuron 9 completed. Final training loss: 0.00033876793446640174, Validation loss: 0.0003298941467888653\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0003\n",
      "Epoch: 2/2, Average loss: 0.0003\n",
      "Training of neuron 10 completed. Final training loss: 0.00031428921865299343, Validation loss: 0.00030798671739175917\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011421473531699772, Validation loss: 0.0011222058728812857\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009901872004798435, Validation loss: 0.0009857784730798386\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008820428000432802, Validation loss: 0.000875906115397811\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007876852187757373, Validation loss: 0.0007782721226202681\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0006953291988977832, Validation loss: 0.0006882651411789529\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0006188268032162748, Validation loss: 0.0006145388553751276\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 7 completed. Final training loss: 0.0005466819449215599, Validation loss: 0.0005376074881233433\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.0004933918562077038, Validation loss: 0.0004907140700503232\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 9 completed. Final training loss: 0.00045028019947468813, Validation loss: 0.00044634132943254835\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 10 completed. Final training loss: 0.00040997964422489314, Validation loss: 0.00040612083810203254\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0003\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0004242426277138293, Validation loss: 0.0005150749446824193\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 2 completed. Final training loss: 0.0008584920485814413, Validation loss: 0.0007737853161990643\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005865814068665107, Validation loss: 0.0005754307564347982\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0005047679358161986, Validation loss: 0.0004990598676726222\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00048233897977819045, Validation loss: 0.00048488928396254777\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00046950730678314964, Validation loss: 0.0004702895749360323\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00044523353977128866, Validation loss: 0.0004362792668864131\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0004224393005793293, Validation loss: 0.0004195761926472187\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 9 completed. Final training loss: 0.00040864216707025967, Validation loss: 0.00040474007669836283\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 10 completed. Final training loss: 0.00039182767073313394, Validation loss: 0.00038676989190280435\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011257877742462123, Validation loss: 0.0011045982014942677\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009682429619860036, Validation loss: 0.000953583979899896\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.000855649808844459, Validation loss: 0.0008517873812308337\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0007646896174600255, Validation loss: 0.0007485706608821737\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0007084639208285627, Validation loss: 0.0006856750536392978\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.000656157218263293, Validation loss: 0.000649207915635185\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0005916793095707524, Validation loss: 0.0005831162647363988\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005631161600199415, Validation loss: 0.000548143944148212\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 9 completed. Final training loss: 0.0005153242935234984, Validation loss: 0.0005104209047722373\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 10 completed. Final training loss: 0.0004890668890538051, Validation loss: 0.00048354858809963186\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.00043926894599571827, Validation loss: 0.000613642068579793\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006890165977800885, Validation loss: 0.0006390169095247984\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005600256358583768, Validation loss: 0.0005522658625617624\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.00048817815237368146, Validation loss: 0.0004815064024180174\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.00046666616974398495, Validation loss: 0.000467620082013309\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00045426311104868847, Validation loss: 0.0004518609382212162\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00042286170401299993, Validation loss: 0.0004139232497662306\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00039984546353419623, Validation loss: 0.0004002853777259588\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 9 completed. Final training loss: 0.00038852634482706587, Validation loss: 0.0003878805911168456\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 10 completed. Final training loss: 0.0003804390113800764, Validation loss: 0.00038047924172133205\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011216461464617375, Validation loss: 0.001105483029671806\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009686632795880238, Validation loss: 0.0009553965767647358\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008633693319197136, Validation loss: 0.0008511674800451766\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0008423225564472642, Validation loss: 0.0008321485044236513\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007712097175696746, Validation loss: 0.0007599328279970808\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0007051988387916316, Validation loss: 0.000679491644804465\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0006688912416616758, Validation loss: 0.0006590935064757124\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0006166461246345747, Validation loss: 0.0006067795203404223\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 9 completed. Final training loss: 0.0005878587110060538, Validation loss: 0.0005937795986996052\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 10 completed. Final training loss: 0.0005558532026285926, Validation loss: 0.0005405500868057951\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 1 completed. Final training loss: 0.0003971289787441492, Validation loss: 0.000477094454318285\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006586107802887758, Validation loss: 0.000651690499484539\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005304884600142637, Validation loss: 0.0005277432044968009\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004740404462752243, Validation loss: 0.00046757102999836204\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004586064485522608, Validation loss: 0.00046420694813132285\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.00045104625389600793, Validation loss: 0.00043617337737232446\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00041584104457870126, Validation loss: 0.00040727267544716594\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00039791000441958505, Validation loss: 0.0003996379861608148\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 9 completed. Final training loss: 0.0003872501950711012, Validation loss: 0.0003899211250245571\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 10 completed. Final training loss: 0.0003828202766987185, Validation loss: 0.00038088156078010797\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011965567171441536, Validation loss: 0.0011021978733070353\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009776654810453138, Validation loss: 0.0009703835422926126\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0008731727566279418, Validation loss: 0.0008894134800326317\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.000850598155289677, Validation loss: 0.0008386558993778965\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.000812642256901038, Validation loss: 0.0007804837052096077\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007615153998770612, Validation loss: 0.000778758875193431\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0007202003381366953, Validation loss: 0.0006933430217682047\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 8 completed. Final training loss: 0.0006738395384902544, Validation loss: 0.0006751508317253691\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 9 completed. Final training loss: 0.000661312158893239, Validation loss: 0.0006550639481382801\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 10 completed. Final training loss: 0.0006498442050267725, Validation loss: 0.0006537029810963159\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 1 completed. Final training loss: 0.0004851126636688908, Validation loss: 0.00048401094060391187\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006537213815376163, Validation loss: 0.0006418347682803869\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 3 completed. Final training loss: 0.0005445825757458806, Validation loss: 0.000530499123968184\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004879956825325886, Validation loss: 0.0004760814880952239\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004506098155242701, Validation loss: 0.00044712900202721357\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.00043775363182649014, Validation loss: 0.0004421669214963913\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.0004319162936260303, Validation loss: 0.000425780295394361\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0004275544059773286, Validation loss: 0.00042449829410761596\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 9 completed. Final training loss: 0.00042124229613691567, Validation loss: 0.0004263233732432127\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 10 completed. Final training loss: 0.0004191935322868327, Validation loss: 0.00042059198189526795\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.001146093855037334, Validation loss: 0.0011321615831966095\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0010193610946641217, Validation loss: 0.0010135096624652121\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0009343140337167056, Validation loss: 0.0009603010296346025\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008828354511634572, Validation loss: 0.0008980021964600112\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 5 completed. Final training loss: 0.0008538607562436386, Validation loss: 0.0008953737689459578\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0008497883107988759, Validation loss: 0.0008355817222531806\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 7 completed. Final training loss: 0.0008096611857453876, Validation loss: 0.0007802651004192042\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 8 completed. Final training loss: 0.0007682913985327943, Validation loss: 0.0007616720713199453\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 9 completed. Final training loss: 0.0007546100136328567, Validation loss: 0.0007486805248450726\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 10 completed. Final training loss: 0.0007512155004136317, Validation loss: 0.0007596329894867984\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "------------MNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010520939690992237, Validation loss: 0.0010601376492530108\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010520290097842614, Validation loss: 0.0010603873375803232\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 3 completed. Final training loss: 0.0010520352715005477, Validation loss: 0.0010596135579049588\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 4 completed. Final training loss: 0.0010519890843580166, Validation loss: 0.0010603762853890657\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 5 completed. Final training loss: 0.0010520129294445118, Validation loss: 0.001060513911768794\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 6 completed. Final training loss: 0.0010520316930487752, Validation loss: 0.0010601636599749326\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 7 completed. Final training loss: 0.0010519550299892824, Validation loss: 0.0010604925941675902\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 8 completed. Final training loss: 0.0010520410101239879, Validation loss: 0.0010601815417408942\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 9 completed. Final training loss: 0.0010520291070143383, Validation loss: 0.0010601903062313796\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 10 completed. Final training loss: 0.0010519430190324783, Validation loss: 0.001060880521312356\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "------------EMNIST------------\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.0013222213612591968, Validation loss: 0.001323653315293028\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0013220444608955308, Validation loss: 0.0013234482344953303\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.0013219877448699153, Validation loss: 0.001323546181492349\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 4 completed. Final training loss: 0.0013219701980260459, Validation loss: 0.0013239624965539639\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 5 completed. Final training loss: 0.0013219704000107574, Validation loss: 0.0013242860633800638\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 6 completed. Final training loss: 0.0013219718500998214, Validation loss: 0.0013231082220977926\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 7 completed. Final training loss: 0.0013219937225077169, Validation loss: 0.0013233943144850275\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 8 completed. Final training loss: 0.0013219731950353012, Validation loss: 0.0013233430295231494\n",
      "\n",
      " ------------ Training of neuron9---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 9 completed. Final training loss: 0.0013219865713374201, Validation loss: 0.001323383000302822\n",
      "\n",
      " ------------ Training of neuron10---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 10 completed. Final training loss: 0.0013219659336915254, Validation loss: 0.0013233301709307\n",
      "------------------------------TRAINING ENDED-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for latent_dim in (6, 8, 10):\n",
    "    print(f\"-----------------------Training models with {latent_dim} latent_dim----------------------\\n\")\n",
    "    for num_hidden_layers in range(1,8):\n",
    "        print(f\"----------------- {num_hidden_layers} num_hidden_layers --------------\")\n",
    "        for dataset in (\"MNIST\", \"EMNIST\"):\n",
    "            print(f\"------------{dataset}------------\")\n",
    "            \n",
    "            input_dim = 784\n",
    "            learning_rate = 1e-3\n",
    "            weight_decay = 1e-5\n",
    "            decrease_rate = 0.7\n",
    "            epochs_for_each_neuron = 3\n",
    "            train_loader = train_loaders[dataset]\n",
    "            val_loader = val_loaders[dataset]\n",
    "            input_dim = 28 * 28\n",
    "\n",
    "            my_model = ProgressiveAE(\n",
    "                input_dim=input_dim,\n",
    "                latent_dim=latent_dim,\n",
    "                decrease_rate=decrease_rate,\n",
    "                device=device,\n",
    "                num_hidden_layers=num_hidden_layers\n",
    "            ).to(device)\n",
    "\n",
    "            model_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/progressive train/2_M_E_MNIST/ld{latent_dim}_dr{decrease_rate}_lr1e3_3epEach_keepTrain_{num_hidden_layers}hl.pth\"\n",
    "            my_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            writer_path = f\"/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/progressive train/2_M_E_MINST/ld{latent_dim}_dr{decrease_rate}_lr1e3_3epEach_keepTrain_{num_hidden_layers}hl.pth\"\n",
    "            writer = SummaryWriter(log_dir = writer_path)\n",
    "\n",
    "            train_ProgressiveAE(\n",
    "                my_model,\n",
    "                epochs_for_each_neuron = 2,\n",
    "                train_loader = train_loader,\n",
    "                val_loader = val_loader,\n",
    "                writer = writer,\n",
    "                freeze_prev_neurons_train = False,\n",
    "                mask_weights = False\n",
    "            )\n",
    "\n",
    "            torch.save(my_model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb3320",
   "metadata": {},
   "source": [
    "# Encoder output: ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1fb7d",
   "metadata": {},
   "source": [
    "## Simultaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eff8033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006506609845906496, Validation loss: 0.000634459638595581\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005139344822304944, Validation loss: 0.0004961186967790127\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005731375296910604, Validation loss: 0.0005569681126624345\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004694999488070607, Validation loss: 0.00045685098413378\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005076957442176839, Validation loss: 0.0004922978181391954\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.000640639099602898, Validation loss: 0.0006233221028000116\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0010\n",
      "Epoch: 1/15, Average loss: 0.0009\n",
      "Epoch: 2/15, Average loss: 0.0009\n",
      "Epoch: 3/15, Average loss: 0.0009\n",
      "Epoch: 4/15, Average loss: 0.0009\n",
      "Epoch: 5/15, Average loss: 0.0009\n",
      "Epoch: 6/15, Average loss: 0.0008\n",
      "Epoch: 7/15, Average loss: 0.0008\n",
      "Epoch: 8/15, Average loss: 0.0008\n",
      "Epoch: 9/15, Average loss: 0.0008\n",
      "Epoch: 10/15, Average loss: 0.0008\n",
      "Epoch: 11/15, Average loss: 0.0008\n",
      "Epoch: 12/15, Average loss: 0.0008\n",
      "Epoch: 13/15, Average loss: 0.0008\n",
      "Epoch: 14/15, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0007529020505646865, Validation loss: 0.0007403505798429251\n",
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004924258892734845, Validation loss: 0.0004947483491152525\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0003\n",
      "Epoch: 4/15, Average loss: 0.0003\n",
      "Epoch: 5/15, Average loss: 0.0003\n",
      "Epoch: 6/15, Average loss: 0.0003\n",
      "Epoch: 7/15, Average loss: 0.0003\n",
      "Epoch: 8/15, Average loss: 0.0003\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003095288710979124, Validation loss: 0.0003141628136858344\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0005\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000435036566046377, Validation loss: 0.00043415966499596833\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0005\n",
      "Epoch: 1/15, Average loss: 0.0004\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0003\n",
      "Epoch: 10/15, Average loss: 0.0003\n",
      "Epoch: 11/15, Average loss: 0.0003\n",
      "Epoch: 12/15, Average loss: 0.0003\n",
      "Epoch: 13/15, Average loss: 0.0003\n",
      "Epoch: 14/15, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003404539073817432, Validation loss: 0.00033824715642258524\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0006\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0004\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003770102501536409, Validation loss: 0.00038174674063920977\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0005\n",
      "Epoch: 2/15, Average loss: 0.0005\n",
      "Epoch: 3/15, Average loss: 0.0004\n",
      "Epoch: 4/15, Average loss: 0.0004\n",
      "Epoch: 5/15, Average loss: 0.0004\n",
      "Epoch: 6/15, Average loss: 0.0004\n",
      "Epoch: 7/15, Average loss: 0.0004\n",
      "Epoch: 8/15, Average loss: 0.0004\n",
      "Epoch: 9/15, Average loss: 0.0004\n",
      "Epoch: 10/15, Average loss: 0.0004\n",
      "Epoch: 11/15, Average loss: 0.0004\n",
      "Epoch: 12/15, Average loss: 0.0004\n",
      "Epoch: 13/15, Average loss: 0.0004\n",
      "Epoch: 14/15, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00038606155142188075, Validation loss: 0.0003861616248264909\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0007\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0005\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005092178120588263, Validation loss: 0.0005166355263441801\n",
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0006\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0005\n",
      "Epoch: 6/15, Average loss: 0.0005\n",
      "Epoch: 7/15, Average loss: 0.0005\n",
      "Epoch: 8/15, Average loss: 0.0005\n",
      "Epoch: 9/15, Average loss: 0.0005\n",
      "Epoch: 10/15, Average loss: 0.0005\n",
      "Epoch: 11/15, Average loss: 0.0005\n",
      "Epoch: 12/15, Average loss: 0.0005\n",
      "Epoch: 13/15, Average loss: 0.0005\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005284687072857686, Validation loss: 0.0005316773050048567\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005414425084327764, Validation loss: 0.0005252597820846007\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005718110853323964, Validation loss: 0.000580530975687694\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0008\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005622734910815109, Validation loss: 0.000565008948299479\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0006\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005586946349102238, Validation loss: 0.0005680875487110399\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0007\n",
      "Epoch: 2/15, Average loss: 0.0007\n",
      "Epoch: 3/15, Average loss: 0.0006\n",
      "Epoch: 4/15, Average loss: 0.0006\n",
      "Epoch: 5/15, Average loss: 0.0006\n",
      "Epoch: 6/15, Average loss: 0.0006\n",
      "Epoch: 7/15, Average loss: 0.0006\n",
      "Epoch: 8/15, Average loss: 0.0006\n",
      "Epoch: 9/15, Average loss: 0.0006\n",
      "Epoch: 10/15, Average loss: 0.0006\n",
      "Epoch: 11/15, Average loss: 0.0006\n",
      "Epoch: 12/15, Average loss: 0.0006\n",
      "Epoch: 13/15, Average loss: 0.0006\n",
      "Epoch: 14/15, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005873127403034968, Validation loss: 0.00059285487900389\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 0/15, Average loss: 0.0009\n",
      "Epoch: 1/15, Average loss: 0.0008\n",
      "Epoch: 2/15, Average loss: 0.0008\n",
      "Epoch: 3/15, Average loss: 0.0007\n",
      "Epoch: 4/15, Average loss: 0.0007\n",
      "Epoch: 5/15, Average loss: 0.0007\n",
      "Epoch: 6/15, Average loss: 0.0007\n",
      "Epoch: 7/15, Average loss: 0.0007\n",
      "Epoch: 8/15, Average loss: 0.0007\n",
      "Epoch: 9/15, Average loss: 0.0007\n",
      "Epoch: 10/15, Average loss: 0.0007\n",
      "Epoch: 11/15, Average loss: 0.0007\n",
      "Epoch: 12/15, Average loss: 0.0007\n",
      "Epoch: 13/15, Average loss: 0.0007\n",
      "Epoch: 14/15, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006756658787039894, Validation loss: 0.0006872615897829862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"2MNIST\", \"MNIST\", \"EMNIST\"):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\\n\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "\n",
    "\n",
    "    for latent_dim in (8,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/simultaneous train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/simultaneous train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/simultaneous train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=15)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/simultaneous train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d490e",
   "metadata": {},
   "source": [
    "## Progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e061429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------1 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0015\n",
      "Epoch: 2/2, Average loss: 0.0014\n",
      "Training of neuron 1 completed. Final training loss: 0.0014054635065297286, Validation loss: 0.0014129798971116542\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0012995138016839822, Validation loss: 0.0013128372371196747\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0010162217194214463, Validation loss: 0.0010010863922536373\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008675738448277115, Validation loss: 0.0008495939362794161\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008424757232889533, Validation loss: 0.0008262554205954075\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007807020554319024, Validation loss: 0.0007516387846320867\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0006808639469866951, Validation loss: 0.0006219425532966853\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0006134583412359158, Validation loss: 0.0005872282430529595\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------2 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.0013254651586214702, Validation loss: 0.0013373559318482875\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.001301198461279273, Validation loss: 0.0013144911028444767\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.001300586296369632, Validation loss: 0.0013142239719629288\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 4 completed. Final training loss: 0.000999759641289711, Validation loss: 0.0009336093127727509\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008344503538683057, Validation loss: 0.0008086042977869511\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0007376404048874975, Validation loss: 0.000710767750069499\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0006577082235366106, Validation loss: 0.0006268012270331382\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0006254551783204078, Validation loss: 0.0005983970940113067\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------3 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0012\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010960347795238097, Validation loss: 0.0010825868748128415\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 2 completed. Final training loss: 0.0009153032694011927, Validation loss: 0.0008990967605262994\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 3 completed. Final training loss: 0.0008106675623605648, Validation loss: 0.000789925530552864\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 4 completed. Final training loss: 0.0007392949166397253, Validation loss: 0.000701026238873601\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0006512567656114697, Validation loss: 0.0006174290589988231\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.000613291965983808, Validation loss: 0.0005895194828510284\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0005992011519769827, Validation loss: 0.0005793898548930883\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005895971776296695, Validation loss: 0.0005676659643650055\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------4 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0012\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010797339379787444, Validation loss: 0.0010941591784358025\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0011\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009826841993878285, Validation loss: 0.000984133767709136\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 3 completed. Final training loss: 0.0008034310099979242, Validation loss: 0.0007792417325079441\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 4 completed. Final training loss: 0.0007125265205899875, Validation loss: 0.0006903777401894331\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0006754105734328429, Validation loss: 0.0006495958935469389\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0005667286107316613, Validation loss: 0.0005400511221960187\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 7 completed. Final training loss: 0.0005330837562369803, Validation loss: 0.0005143487505614758\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.0005025501171126962, Validation loss: 0.00047773597594350577\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------5 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0012\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.00106230162071685, Validation loss: 0.0010462216313928366\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0010\n",
      "Epoch: 2/3, Average loss: 0.0010\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010590941122422615, Validation loss: 0.0010441423878073692\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0010\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.000911299119827648, Validation loss: 0.0008860505357384681\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0008079326661303639, Validation loss: 0.0007876104533672333\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0007776577801133196, Validation loss: 0.0007591385614126921\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007615870014453927, Validation loss: 0.0007436055846512318\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0007131730213140448, Validation loss: 0.0006811936624348164\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 8 completed. Final training loss: 0.0006721397410457333, Validation loss: 0.0006592123467475176\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------6 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0012\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010737828154116869, Validation loss: 0.0010544026780873537\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0010\n",
      "Epoch: 2/3, Average loss: 0.0010\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 2 completed. Final training loss: 0.0009496149195358158, Validation loss: 0.0009410205557942391\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.0009031525555377205, Validation loss: 0.000881711396574974\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008725673414145907, Validation loss: 0.0008574898842722177\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008291779742886623, Validation loss: 0.0007870846066623926\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007794144812350472, Validation loss: 0.0007739770974963903\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 7 completed. Final training loss: 0.0007668916668122014, Validation loss: 0.0007485391765832901\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 8 completed. Final training loss: 0.0007601682771618168, Validation loss: 0.0007377040237188339\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------1 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 1 completed. Final training loss: 0.0011913521077483893, Validation loss: 0.0012065161693841219\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010607319970925649, Validation loss: 0.001064131187275052\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 3 completed. Final training loss: 0.001051773005972306, Validation loss: 0.001060344761610031\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0009189319550370177, Validation loss: 0.0009199261412024497\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 5 completed. Final training loss: 0.000559688621883591, Validation loss: 0.0005361821040511132\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.0005171487776252131, Validation loss: 0.0005161024471744895\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 7 completed. Final training loss: 0.0004695132850048443, Validation loss: 0.0004649993628263474\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.000450718881872793, Validation loss: 0.0004501967104151845\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------2 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 1 completed. Final training loss: 0.0010429952022929987, Validation loss: 0.0009360493332147598\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 2 completed. Final training loss: 0.000845086845010519, Validation loss: 0.0008371649082750082\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 3 completed. Final training loss: 0.0006785297999158501, Validation loss: 0.0006346354279667139\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0005304829498442511, Validation loss: 0.0005211863433942198\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0004922961704743405, Validation loss: 0.0004647860676050186\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.00041108208345249297, Validation loss: 0.0003885468246415257\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.0003700024656020105, Validation loss: 0.000364598642103374\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0003\n",
      "Training of neuron 8 completed. Final training loss: 0.00033214263732855517, Validation loss: 0.00032526606041938065\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------3 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0010546166735390821, Validation loss: 0.0010634771924465894\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0011\n",
      "Training of neuron 2 completed. Final training loss: 0.0010536865832284092, Validation loss: 0.0010619922865182162\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 3 completed. Final training loss: 0.0007760437580446402, Validation loss: 0.0007363296810537576\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 4 completed. Final training loss: 0.0006631941280017296, Validation loss: 0.0006187896758317947\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 5 completed. Final training loss: 0.0005691246967452268, Validation loss: 0.0005432413186877966\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.0004946345503441989, Validation loss: 0.0004716970330104232\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0005\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00044143484588712455, Validation loss: 0.0004301829228177667\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0004\n",
      "Epoch: 2/2, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0004006727476293842, Validation loss: 0.00039338432643562553\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------4 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 1 completed. Final training loss: 0.0008392157736544808, Validation loss: 0.0008505298122763634\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006770141491045554, Validation loss: 0.0006675993677228689\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0005767755017305414, Validation loss: 0.0005679469168186188\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 4 completed. Final training loss: 0.0004979649022221565, Validation loss: 0.0004974285911768675\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0004\n",
      "Training of neuron 5 completed. Final training loss: 0.0004414001578775545, Validation loss: 0.0004358103010803461\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0004\n",
      "Epoch: 2/3, Average loss: 0.0004\n",
      "Epoch: 3/3, Average loss: 0.0004\n",
      "Training of neuron 6 completed. Final training loss: 0.0004136094843658308, Validation loss: 0.0004099372232332826\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0004\n",
      "Epoch: 2/3, Average loss: 0.0004\n",
      "Epoch: 3/3, Average loss: 0.0004\n",
      "Training of neuron 7 completed. Final training loss: 0.00039902418438966076, Validation loss: 0.00039664974361658097\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0004\n",
      "Epoch: 2/3, Average loss: 0.0004\n",
      "Epoch: 3/3, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.0003905419765971601, Validation loss: 0.0003866777652874589\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------5 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 1 completed. Final training loss: 0.0008240558535481492, Validation loss: 0.0007792064879089594\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0007432492617517709, Validation loss: 0.0007418392438441515\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 3 completed. Final training loss: 0.0006554633669555187, Validation loss: 0.0006468974027782679\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 4 completed. Final training loss: 0.0006277594106271863, Validation loss: 0.0006482282217592001\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 5 completed. Final training loss: 0.0005783881353835264, Validation loss: 0.000575456372462213\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.0005109816023459037, Validation loss: 0.0005118290385231376\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 7 completed. Final training loss: 0.00047681269484261673, Validation loss: 0.0004739167783409357\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0004\n",
      "Training of neuron 8 completed. Final training loss: 0.00043312361802284914, Validation loss: 0.0004262416582554579\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------6 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 1 completed. Final training loss: 0.0008087525803595781, Validation loss: 0.0007972249682992697\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 2 completed. Final training loss: 0.0006692045145357649, Validation loss: 0.0007146492529660464\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 3 completed. Final training loss: 0.0006491592533886432, Validation loss: 0.0006412943243980408\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 4 completed. Final training loss: 0.0005829382200414936, Validation loss: 0.0005816363893449306\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 5 completed. Final training loss: 0.0005399548756579558, Validation loss: 0.0005247085088863969\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 6 completed. Final training loss: 0.0004864700316141049, Validation loss: 0.00048325598817318676\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 7 completed. Final training loss: 0.00047678071446716783, Validation loss: 0.00046278071962296965\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0005\n",
      "Epoch: 2/3, Average loss: 0.0005\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.0004585254980251193, Validation loss: 0.0004607604252174497\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 8 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "-----------1 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0015\n",
      "Epoch: 2/2, Average loss: 0.0014\n",
      "Training of neuron 1 completed. Final training loss: 0.0014333585598869315, Validation loss: 0.00142255560158098\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0013264682829538558, Validation loss: 0.0013271399218826852\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0012\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 3 completed. Final training loss: 0.0011751234273142215, Validation loss: 0.0011718371300462713\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0011\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0009476727138253602, Validation loss: 0.0009249097322846981\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008180743809261009, Validation loss: 0.000809171347224966\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007519710213870656, Validation loss: 0.0006695569556285727\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0006372239587313318, Validation loss: 0.0006293781908189363\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0006013313537556017, Validation loss: 0.0005755658097010344\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------2 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0014\n",
      "Epoch: 2/2, Average loss: 0.0014\n",
      "Training of neuron 1 completed. Final training loss: 0.0013851073288510666, Validation loss: 0.0013841615407232273\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0012\n",
      "Training of neuron 2 completed. Final training loss: 0.0011598899885219463, Validation loss: 0.0010698806492492874\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.001010229623288656, Validation loss: 0.0009988009283675792\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0010\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0009227678320760317, Validation loss: 0.0008994086164029989\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 5 completed. Final training loss: 0.0008205124561456924, Validation loss: 0.0007966025252925589\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 6 completed. Final training loss: 0.0007053404677566801, Validation loss: 0.0006928060503002811\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0006479660887931678, Validation loss: 0.0005971731263351567\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0006\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0005529910520029565, Validation loss: 0.0005491560304220369\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------3 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.0013242943999720804, Validation loss: 0.0013253048109881421\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0013230302042149483, Validation loss: 0.0013241755233166065\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/2, Average loss: 0.0013\n",
      "Epoch: 2/2, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.001322622112665616, Validation loss: 0.0013247442182074201\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/2, Average loss: 0.0012\n",
      "Epoch: 2/2, Average loss: 0.0010\n",
      "Training of neuron 4 completed. Final training loss: 0.001028797599876709, Validation loss: 0.0010074966046166548\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/2, Average loss: 0.0009\n",
      "Epoch: 2/2, Average loss: 0.0009\n",
      "Training of neuron 5 completed. Final training loss: 0.0008858790638995298, Validation loss: 0.0008754292604057713\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/2, Average loss: 0.0008\n",
      "Epoch: 2/2, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007793096705597448, Validation loss: 0.0007793171200504962\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0007\n",
      "Training of neuron 7 completed. Final training loss: 0.0006937345577047227, Validation loss: 0.0006872525838936897\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/2, Average loss: 0.0007\n",
      "Epoch: 2/2, Average loss: 0.0006\n",
      "Training of neuron 8 completed. Final training loss: 0.0006368278388383118, Validation loss: 0.0006240882096376191\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------4 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0012\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011058227419655058, Validation loss: 0.001093086587225503\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0010\n",
      "Epoch: 2/3, Average loss: 0.0010\n",
      "Epoch: 3/3, Average loss: 0.0010\n",
      "Training of neuron 2 completed. Final training loss: 0.0009645077581880681, Validation loss: 0.0009546651205721687\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 3 completed. Final training loss: 0.000858201532201934, Validation loss: 0.0008482890591976491\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 4 completed. Final training loss: 0.0008065023972712299, Validation loss: 0.0008058735046614991\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0007\n",
      "Training of neuron 5 completed. Final training loss: 0.0007327199642420978, Validation loss: 0.0007063069032069217\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0007\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 6 completed. Final training loss: 0.0006474368858435158, Validation loss: 0.0006473829319819491\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0006\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0006\n",
      "Training of neuron 7 completed. Final training loss: 0.0005801608294565627, Validation loss: 0.0005802111325666626\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0007\n",
      "Epoch: 2/3, Average loss: 0.0006\n",
      "Epoch: 3/3, Average loss: 0.0005\n",
      "Training of neuron 8 completed. Final training loss: 0.0005308922163065496, Validation loss: 0.0005283264273856866\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------5 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 1 completed. Final training loss: 0.001324169781547806, Validation loss: 0.0013259997666674726\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 2 completed. Final training loss: 0.0013227699340340939, Validation loss: 0.0013238669353279663\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 3 completed. Final training loss: 0.0013223800191272657, Validation loss: 0.0013240187167328722\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 4 completed. Final training loss: 0.001322182517776147, Validation loss: 0.0013233669474720954\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 5 completed. Final training loss: 0.001322227536390225, Validation loss: 0.001323782835710556\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 6 completed. Final training loss: 0.001322181740815335, Validation loss: 0.00132339962659047\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 7 completed. Final training loss: 0.001322171513308236, Validation loss: 0.0013232220241681059\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0013\n",
      "Epoch: 2/3, Average loss: 0.0013\n",
      "Epoch: 3/3, Average loss: 0.0013\n",
      "Training of neuron 8 completed. Final training loss: 0.0013221055168843438, Validation loss: 0.0013234035226892917\n",
      "------------------------------TRAINING ENDED-------------------------------\n",
      "\n",
      "-----------6 hidden layers ----------\n",
      "\n",
      "\n",
      "-----------------------------TRAINING STARTED---------------------------- \n",
      "\n",
      " ------------ Training of neuron1---------------\n",
      "Epoch: 1/3, Average loss: 0.0012\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0011\n",
      "Training of neuron 1 completed. Final training loss: 0.0011230678531730957, Validation loss: 0.0011307137812230181\n",
      "\n",
      " ------------ Training of neuron2---------------\n",
      "Epoch: 1/3, Average loss: 0.0011\n",
      "Epoch: 2/3, Average loss: 0.0011\n",
      "Epoch: 3/3, Average loss: 0.0012\n",
      "Training of neuron 2 completed. Final training loss: 0.0011867441170921563, Validation loss: 0.0011757581209407208\n",
      "\n",
      " ------------ Training of neuron3---------------\n",
      "Epoch: 1/3, Average loss: 0.0011\n",
      "Epoch: 2/3, Average loss: 0.0010\n",
      "Epoch: 3/3, Average loss: 0.0010\n",
      "Training of neuron 3 completed. Final training loss: 0.0010074891932374725, Validation loss: 0.0009983046996862965\n",
      "\n",
      " ------------ Training of neuron4---------------\n",
      "Epoch: 1/3, Average loss: 0.0010\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 4 completed. Final training loss: 0.0008854345228615488, Validation loss: 0.0008861304659079364\n",
      "\n",
      " ------------ Training of neuron5---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0009\n",
      "Epoch: 3/3, Average loss: 0.0009\n",
      "Training of neuron 5 completed. Final training loss: 0.000860587058403919, Validation loss: 0.000849577358904037\n",
      "\n",
      " ------------ Training of neuron6---------------\n",
      "Epoch: 1/3, Average loss: 0.0009\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 6 completed. Final training loss: 0.0007970534585389897, Validation loss: 0.0007825798139055359\n",
      "\n",
      " ------------ Training of neuron7---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 7 completed. Final training loss: 0.0007736263783146621, Validation loss: 0.0007742457177014427\n",
      "\n",
      " ------------ Training of neuron8---------------\n",
      "Epoch: 1/3, Average loss: 0.0008\n",
      "Epoch: 2/3, Average loss: 0.0008\n",
      "Epoch: 3/3, Average loss: 0.0008\n",
      "Training of neuron 8 completed. Final training loss: 0.0007602129565842502, Validation loss: 0.0007648331685823963\n",
      "------------------------------TRAINING ENDED-------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"2MNIST\", \"MNIST\", \"EMNIST\"):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\\n\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    epochs_for_each_neuron = 2\n",
    "\n",
    "    for latent_dim in (8,):\n",
    "        print(f\"\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\")\n",
    "\n",
    "        for num_hidden_layers in range(1, 7):\n",
    "            print(f\"\\n-----------{num_hidden_layers} hidden layers ----------\\n\")\n",
    "\n",
    "\n",
    "            if num_hidden_layers >= 4:\n",
    "                epochs_for_each_neuron = 3\n",
    "            else:\n",
    "                epochs_for_each_neuron = 2\n",
    "                \n",
    "            my_model = ProgressiveAE(\n",
    "                input_dim=input_dim,\n",
    "                latent_dim=latent_dim,\n",
    "                decrease_rate=decrease_rate,\n",
    "                device=device,\n",
    "                num_hidden_layers=num_hidden_layers,\n",
    "                bottleneck_in_fn=nn.ReLU\n",
    "            ).to(device)\n",
    "\n",
    "            log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/progressive train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl'\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "            train_ProgressiveAE(\n",
    "                my_model,\n",
    "                epochs_for_each_neuron = epochs_for_each_neuron,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                writer=writer,\n",
    "                freeze_prev_neurons_train=False\n",
    "            )\n",
    "\n",
    "            model_path = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/progressive train/{dataset}/ld{latent_dim}_lr1e-3_dr{decrease_rate}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(my_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2c799",
   "metadata": {},
   "source": [
    "# He init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4ce79",
   "metadata": {},
   "source": [
    "## 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460a1ed",
   "metadata": {},
   "source": [
    "### bias 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a0ba4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0011\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003908462950028479, Validation loss: 0.0003919352373108268\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003482725552593668, Validation loss: 0.00034963339259848\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000399674172171702, Validation loss: 0.00039927499629557135\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00046114753084257245, Validation loss: 0.00045940075647085904\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00039422094952315095, Validation loss: 0.00039187796097248795\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005161155790711443, Validation loss: 0.0005179469123482704\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.1\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.1).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.1).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50101c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003949329761167367, Validation loss: 0.0003993710337206721\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005052182322678467, Validation loss: 0.0005033171217888594\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.1\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        for num_hidden_layers in (3,4):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            model_path = f\"../models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers-1}hl.pth\"\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers-1, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.1).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5aa5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006025245074341272, Validation loss: 0.0006041387462631819\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005006521963514388, Validation loss: 0.0005050780663781978\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.000604122948439805, Validation loss: 0.0006081234814321741\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005082214517182017, Validation loss: 0.0005152373224932779\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004985566306462948, Validation loss: 0.0005031082160929417\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005828462235286092, Validation loss: 0.0005862817772604684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.5\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.1).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.1).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93088911",
   "metadata": {},
   "source": [
    "### bias 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5978e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044647559182097514, Validation loss: 0.0004490873884409666\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044735428498437005, Validation loss: 0.00044784525632858275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "    train_num = 1\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        for num_hidden_layers in (3,5):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "\n",
    "            ex_model_path = f'../models/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers-1}hl_{train_num}.pth'\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers-1, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(torch.load(ex_model_path, map_location=device))\n",
    "\n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=0.2).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e490863",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d88f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00043625265415757895, Validation loss: 0.00043595574721693993\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035695796962827446, Validation loss: 0.0003576659318059683\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005330029132775962, Validation loss: 0.0005292562110349536\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003810009585383038, Validation loss: 0.0003805349249392748\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004721532320603728, Validation loss: 0.00047137406188994645\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044153560396904747, Validation loss: 0.00044134135078638794\n"
     ]
    }
   ],
   "source": [
    "# 0\n",
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004372881212582191, Validation loss: 0.0004384378997609019\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00034985337949668366, Validation loss: 0.0003508427707478404\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005415838089150687, Validation loss: 0.0005370109677314759\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003882970908656716, Validation loss: 0.00039264157488942144\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005218153613929948, Validation loss: 0.000520173141732812\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044115780976911387, Validation loss: 0.0004440493570640683\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd7b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00043984282560025653, Validation loss: 0.0004394606212154031\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00039281919949377575, Validation loss: 0.0003958023808896542\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004504675171027581, Validation loss: 0.00044607597198337317\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00039192123003304, Validation loss: 0.0003926589833572507\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00045098769078031183, Validation loss: 0.0004543608784675598\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003942378002529343, Validation loss: 0.00039569168593734505\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e8c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003987126320290069, Validation loss: 0.0003965224679559469\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003513746539130807, Validation loss: 0.0003524451209232211\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044593286719173196, Validation loss: 0.0004496995061635971\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003633115565714737, Validation loss: 0.00036479394137859343\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005269984242816766, Validation loss: 0.0005285517487674952\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004403526441194117, Validation loss: 0.0004502970721572638\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a440c0",
   "metadata": {},
   "source": [
    "#### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69690d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006797049079645187, Validation loss: 0.000682065949795094\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005086508186217001, Validation loss: 0.0005153612755833471\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0012\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.000622811927988544, Validation loss: 0.0006234950930910541\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005205424260579261, Validation loss: 0.0005277143944887087\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0012\n",
      "Epoch: 2/10, Average loss: 0.0010\n",
      "Epoch: 3/10, Average loss: 0.0010\n",
      "Epoch: 4/10, Average loss: 0.0009\n",
      "Epoch: 5/10, Average loss: 0.0009\n",
      "Epoch: 6/10, Average loss: 0.0009\n",
      "Epoch: 7/10, Average loss: 0.0009\n",
      "Epoch: 8/10, Average loss: 0.0009\n",
      "Epoch: 9/10, Average loss: 0.0009\n",
      "Epoch: 10/10, Average loss: 0.0009\n",
      "Training completed. Final training loss: 0.0008969428494955737, Validation loss: 0.0008867451891382324\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0009\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0008\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006564533486896267, Validation loss: 0.0006571254260996555\n"
     ]
    }
   ],
   "source": [
    "# 0\n",
    "\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac4df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006015755537672774, Validation loss: 0.0006041570079136403\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005034506639485506, Validation loss: 0.0005091433916637238\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005723413216537987, Validation loss: 0.0005764900040911867\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0010\n",
      "Epoch: 3/10, Average loss: 0.0009\n",
      "Epoch: 4/10, Average loss: 0.0009\n",
      "Epoch: 5/10, Average loss: 0.0009\n",
      "Epoch: 6/10, Average loss: 0.0009\n",
      "Epoch: 7/10, Average loss: 0.0009\n",
      "Epoch: 8/10, Average loss: 0.0009\n",
      "Epoch: 9/10, Average loss: 0.0009\n",
      "Epoch: 10/10, Average loss: 0.0009\n",
      "Training completed. Final training loss: 0.0008785183856541172, Validation loss: 0.0008837989911912603\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005814238859617964, Validation loss: 0.0005915970452367625\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.000571865306453819, Validation loss: 0.0005785352322570187\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500b16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006665498469064527, Validation loss: 0.0006681541658620885\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005199170923884298, Validation loss: 0.0005261549462584105\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0013\n",
      "Epoch: 3/10, Average loss: 0.0013\n",
      "Epoch: 4/10, Average loss: 0.0013\n",
      "Epoch: 5/10, Average loss: 0.0013\n",
      "Epoch: 6/10, Average loss: 0.0013\n",
      "Epoch: 7/10, Average loss: 0.0013\n",
      "Epoch: 8/10, Average loss: 0.0013\n",
      "Epoch: 9/10, Average loss: 0.0013\n",
      "Epoch: 10/10, Average loss: 0.0013\n",
      "Training completed. Final training loss: 0.0013223016998525842, Validation loss: 0.0013234847443217928\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0013\n",
      "Epoch: 3/10, Average loss: 0.0013\n",
      "Epoch: 4/10, Average loss: 0.0013\n",
      "Epoch: 5/10, Average loss: 0.0013\n",
      "Epoch: 6/10, Average loss: 0.0013\n",
      "Epoch: 7/10, Average loss: 0.0013\n",
      "Epoch: 8/10, Average loss: 0.0013\n",
      "Epoch: 9/10, Average loss: 0.0013\n",
      "Epoch: 10/10, Average loss: 0.0013\n",
      "Training completed. Final training loss: 0.0013220746300004898, Validation loss: 0.0013234742909194307\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0013\n",
      "Epoch: 3/10, Average loss: 0.0013\n",
      "Epoch: 4/10, Average loss: 0.0013\n",
      "Epoch: 5/10, Average loss: 0.0013\n",
      "Epoch: 6/10, Average loss: 0.0013\n",
      "Epoch: 7/10, Average loss: 0.0013\n",
      "Epoch: 8/10, Average loss: 0.0013\n",
      "Epoch: 9/10, Average loss: 0.0013\n",
      "Epoch: 10/10, Average loss: 0.0013\n",
      "Training completed. Final training loss: 0.001321921097048631, Validation loss: 0.001323052713290808\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0013\n",
      "Epoch: 3/10, Average loss: 0.0013\n",
      "Epoch: 4/10, Average loss: 0.0013\n",
      "Epoch: 5/10, Average loss: 0.0013\n",
      "Epoch: 6/10, Average loss: 0.0013\n",
      "Epoch: 7/10, Average loss: 0.0013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m writer = SummaryWriter(log_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m../runs/relu_output/he init/simultaneous train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mfeatures/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecrease_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_bias\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mhl_2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m save_dir = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m../models/relu_output/he init/simultaneous train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mfeatures/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecrease_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_bias\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mhl_2.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     43\u001b[39m torch.save(new_model.state_dict(), save_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Tesi/Autoencoders/AE/train.py:33\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, epochs, train_loader, val_loader, optimizer, writer, scheduler, save_tensorboard_parameters, starting_epoch)\u001b[39m\n\u001b[32m     31\u001b[39m loss = nn.MSELoss()(output, data)\n\u001b[32m     32\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m train_loss += loss.item()\n\u001b[32m     35\u001b[39m global_batch_idx += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:456\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    454\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    459\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4caedf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0008\n",
      "Epoch: 6/10, Average loss: 0.0008\n",
      "Epoch: 7/10, Average loss: 0.0008\n",
      "Epoch: 8/10, Average loss: 0.0008\n",
      "Epoch: 9/10, Average loss: 0.0008\n",
      "Epoch: 10/10, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0007501866016536951, Validation loss: 0.0007459733561870265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        for num_hidden_layers in (6,):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            ex_model_path = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers-1}hl_0.pth'\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers-1, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(torch.load(ex_model_path, map_location=device))\n",
    "\n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_0')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_0.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cf71ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0009\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006792914878629835, Validation loss: 0.0006809322345764079\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005200641807201748, Validation loss: 0.0005240565560203284\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0012\n",
      "Epoch: 2/10, Average loss: 0.0011\n",
      "Epoch: 3/10, Average loss: 0.0010\n",
      "Epoch: 4/10, Average loss: 0.0009\n",
      "Epoch: 5/10, Average loss: 0.0009\n",
      "Epoch: 6/10, Average loss: 0.0008\n",
      "Epoch: 7/10, Average loss: 0.0008\n",
      "Epoch: 8/10, Average loss: 0.0008\n",
      "Epoch: 9/10, Average loss: 0.0008\n",
      "Epoch: 10/10, Average loss: 0.0008\n",
      "Training completed. Final training loss: 0.0008048326876807086, Validation loss: 0.0007980695382711735\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005768543072639311, Validation loss: 0.0005800029274789577\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005666717049725195, Validation loss: 0.0005790471295489275\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006569256202535743, Validation loss: 0.0006659852608324999\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}features/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfd58e",
   "metadata": {},
   "source": [
    "#### 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006617199646309018, Validation loss: 0.0006449983097612858\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005332874351491531, Validation loss: 0.0005157286465168\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0012\n",
      "Epoch: 2/10, Average loss: 0.0011\n",
      "Epoch: 3/10, Average loss: 0.0010\n",
      "Epoch: 4/10, Average loss: 0.0009\n",
      "Epoch: 5/10, Average loss: 0.0009\n",
      "Epoch: 6/10, Average loss: 0.0009\n",
      "Epoch: 7/10, Average loss: 0.0009\n",
      "Epoch: 8/10, Average loss: 0.0009\n",
      "Epoch: 9/10, Average loss: 0.0009\n",
      "Epoch: 10/10, Average loss: 0.0009\n",
      "Training completed. Final training loss: 0.0008581424310803413, Validation loss: 0.0008453733295202255\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005768879153455297, Validation loss: 0.0005616678904742002\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005786595499143004, Validation loss: 0.0005584585074335336\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006319176348547141, Validation loss: 0.0006222522921860218\n"
     ]
    }
   ],
   "source": [
    "# 0\n",
    "\n",
    "for dataset in (\"2MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f118b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006663647266725699, Validation loss: 0.0006521976307034493\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005278326058760285, Validation loss: 0.0005112233458086849\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0011\n",
      "Epoch: 2/10, Average loss: 0.0009\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006788465453932683, Validation loss: 0.0006611422546207905\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0008\n",
      "Epoch: 6/10, Average loss: 0.0008\n",
      "Epoch: 7/10, Average loss: 0.0008\n",
      "Epoch: 8/10, Average loss: 0.0008\n",
      "Epoch: 9/10, Average loss: 0.0008\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007483476554974914, Validation loss: 0.0007419919651001692\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006418238944684466, Validation loss: 0.0006301453419029712\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0008\n",
      "Epoch: 6/10, Average loss: 0.0008\n",
      "Epoch: 7/10, Average loss: 0.0008\n",
      "Epoch: 8/10, Average loss: 0.0008\n",
      "Epoch: 9/10, Average loss: 0.0008\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0007462759566803773, Validation loss: 0.0007295187454670667\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "\n",
    "for dataset in (\"2MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_1.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0009\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006664422866577903, Validation loss: 0.0006476511012762784\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005999757376189033, Validation loss: 0.0005870631821453571\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0012\n",
      "Epoch: 2/10, Average loss: 0.0010\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006781511886666218, Validation loss: 0.0006578325644135475\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005749873006095489, Validation loss: 0.0005632860891520977\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006465074014539519, Validation loss: 0.0006319373916834593\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0010\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0008\n",
      "Epoch: 4/10, Average loss: 0.0008\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0007\n",
      "Epoch: 8/10, Average loss: 0.0007\n",
      "Epoch: 9/10, Average loss: 0.0007\n",
      "Epoch: 10/10, Average loss: 0.0007\n",
      "Training completed. Final training loss: 0.0006757062292968234, Validation loss: 0.0006646419916301966\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "\n",
    "for dataset in (\"2MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,7):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/{dataset}/ld{latent_dim}_lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_2.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d070ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 6 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0005940342855950197, Validation loss: 0.0005799976237118244\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005187341637909413, Validation loss: 0.0005032878885045648\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0013\n",
      "Epoch: 2/10, Average loss: 0.0013\n",
      "Epoch: 3/10, Average loss: 0.0013\n",
      "Epoch: 4/10, Average loss: 0.0013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m writer = SummaryWriter(log_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/runs/relu_output/he init/simultaneous train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mld/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecrease_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_bias\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mhl_3\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m save_dir = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders/models/relu_output/he init/simultaneous train/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mld/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecrease_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_bias\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_hidden_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mhl_3.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     43\u001b[39m torch.save(new_model.state_dict(), save_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Tesi/Autoencoders/AE/train.py:33\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, epochs, train_loader, val_loader, optimizer, writer, scheduler, save_tensorboard_parameters, starting_epoch)\u001b[39m\n\u001b[32m     31\u001b[39m loss = nn.MSELoss()(output, data)\n\u001b[32m     32\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m train_loss += loss.item()\n\u001b[32m     35\u001b[39m global_batch_idx += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/adam.py:392\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    389\u001b[39m step_t = state_steps[i]\n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_compiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m capturable:\n\u001b[32m    393\u001b[39m     capturable_supported_devices = _get_capturable_supported_devices()\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    395\u001b[39m         param.device.type == step_t.device.type\n\u001b[32m    396\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m param.device.type \u001b[38;5;129;01min\u001b[39;00m capturable_supported_devices\n\u001b[32m    397\u001b[39m     ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf capturable=True, params and state_steps must be on supported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapturable_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/compiler/__init__.py:380\u001b[39m, in \u001b[36mis_compiling\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    376\u001b[39m _is_compiling_flag: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    377\u001b[39m _is_exporting_flag: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_compiling\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    381\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[33;03m    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\u001b[39;00m\n\u001b[32m    383\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    394\u001b[39m \u001b[33;03m        >>>     # ...rest of the function...\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_scripting():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3\n",
    "\n",
    "for dataset in (\"2MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    input_dim = 28 * 28\n",
    "    bias = 0.0\n",
    "\n",
    "\n",
    "    for latent_dim in (6,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,4):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU, he_init=True, set_bias=bias).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/relu_output/he init/simultaneous train/{latent_dim}ld/{dataset}/lr{learning_rate}_dr{decrease_rate}_bias{bias}_{num_hidden_layers}hl_3.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.ReLU).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac4091",
   "metadata": {},
   "source": [
    "# Output func: Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527c278",
   "metadata": {},
   "source": [
    "## Simultaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95739b8",
   "metadata": {},
   "source": [
    "### 10 ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d48f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNISTonly------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003943694289617388, Validation loss: 0.0003999051504385795\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00033399221443887846, Validation loss: 0.0003451591415345207\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00033046855063206717, Validation loss: 0.0003438382030486367\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003489949782674406, Validation loss: 0.00035927042928199436\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004066387966735405, Validation loss: 0.0004457056345490291\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004547866972331514, Validation loss: 0.0004696470627546772\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005295715945359843, Validation loss: 0.0005670112499952778\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"2MNISTonly\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e-3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 0\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4041a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNISTonly------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003881844297706701, Validation loss: 0.0003936980388943077\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003322214557194958, Validation loss: 0.0003438872382651235\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003247737910157084, Validation loss: 0.0003391336667705183\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0003334887003130303, Validation loss: 0.000354274914260636\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00042020592503387203, Validation loss: 0.000444727258525731\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004771761033470177, Validation loss: 0.0004977352390166863\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005267268895837796, Validation loss: 0.0005462602134649606\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"2MNISTonly\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e-3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 1\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b3e9e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------2MNISTonly------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003909928165813867, Validation loss: 0.00039278013205747733\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00033789374792123647, Validation loss: 0.00035101584115520467\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035709374249431663, Validation loss: 0.00037196919127085873\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003888939231253983, Validation loss: 0.00039837044995826807\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004534560514250711, Validation loss: 0.0004695599479961765\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.000527014355942702, Validation loss: 0.0005493464161345894\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0007\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0006\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005312446309750654, Validation loss: 0.0005495605679849783\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"2MNISTonly\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e-3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 2\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05045bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003558810314474007, Validation loss: 0.00034992641527205705\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0004\n",
      "Epoch: 2/10, Average loss: 0.0003\n",
      "Epoch: 3/10, Average loss: 0.0003\n",
      "Epoch: 4/10, Average loss: 0.0003\n",
      "Epoch: 5/10, Average loss: 0.0003\n",
      "Epoch: 6/10, Average loss: 0.0003\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002947599304839969, Validation loss: 0.0002895511319860816\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0003\n",
      "Epoch: 3/10, Average loss: 0.0003\n",
      "Epoch: 4/10, Average loss: 0.0003\n",
      "Epoch: 5/10, Average loss: 0.0003\n",
      "Epoch: 6/10, Average loss: 0.0003\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002716461719945073, Validation loss: 0.00026819137949496506\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0003\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.000330169482746472, Validation loss: 0.00032709288923069837\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003800122088442246, Validation loss: 0.00037783624827861787\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00041175637397294245, Validation loss: 0.00041370589174330235\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.00045684826569631693, Validation loss: 0.00046157151218503714\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 1\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91cd5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------MNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00035711239244168004, Validation loss: 0.00035173319336026906\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0004\n",
      "Epoch: 2/10, Average loss: 0.0003\n",
      "Epoch: 3/10, Average loss: 0.0003\n",
      "Epoch: 4/10, Average loss: 0.0003\n",
      "Epoch: 5/10, Average loss: 0.0003\n",
      "Epoch: 6/10, Average loss: 0.0003\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002949633580632508, Validation loss: 0.0002916248071938753\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0005\n",
      "Epoch: 2/10, Average loss: 0.0003\n",
      "Epoch: 3/10, Average loss: 0.0003\n",
      "Epoch: 4/10, Average loss: 0.0003\n",
      "Epoch: 5/10, Average loss: 0.0003\n",
      "Epoch: 6/10, Average loss: 0.0003\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.0002701762002427131, Validation loss: 0.0002664738828316331\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0003\n",
      "Epoch: 8/10, Average loss: 0.0003\n",
      "Epoch: 9/10, Average loss: 0.0003\n",
      "Epoch: 10/10, Average loss: 0.0003\n",
      "Training completed. Final training loss: 0.00033602826120331884, Validation loss: 0.000331153542548418\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00036203384520486, Validation loss: 0.0003603022951632738\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044815208623185754, Validation loss: 0.00044799796082079413\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0006\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0005126929372859498, Validation loss: 0.0005280155995860695\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"MNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 2\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc6e89c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004918764829173262, Validation loss: 0.0004920146085242642\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00040732288752940105, Validation loss: 0.0004082170813443496\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00038587410278710806, Validation loss: 0.0003866402386747142\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0003955914384639221, Validation loss: 0.00039664496865836866\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.00044424853109298866, Validation loss: 0.00044956070598848956\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004636259589027217, Validation loss: 0.000475729542209747\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0008\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0006\n",
      "Epoch: 6/10, Average loss: 0.0006\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006096508200709701, Validation loss: 0.0006138138330363213\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 1\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f7e96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "------------EMNIST------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------Training models with 10 latent_dim----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------- 1 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004909833188413094, Validation loss: 0.0004930691677641044\n",
      "\n",
      "\n",
      "----------------- 2 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0005\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004058108074220677, Validation loss: 0.00040635341788305247\n",
      "\n",
      "\n",
      "----------------- 3 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000376784175069646, Validation loss: 0.00037905542140311384\n",
      "\n",
      "\n",
      "----------------- 4 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0006\n",
      "Epoch: 2/10, Average loss: 0.0004\n",
      "Epoch: 3/10, Average loss: 0.0004\n",
      "Epoch: 4/10, Average loss: 0.0004\n",
      "Epoch: 5/10, Average loss: 0.0004\n",
      "Epoch: 6/10, Average loss: 0.0004\n",
      "Epoch: 7/10, Average loss: 0.0004\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.000371418876048336, Validation loss: 0.00037238244868894205\n",
      "\n",
      "\n",
      "----------------- 5 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0004\n",
      "Epoch: 9/10, Average loss: 0.0004\n",
      "Epoch: 10/10, Average loss: 0.0004\n",
      "Training completed. Final training loss: 0.0004376999101211839, Validation loss: 0.0004415132151916623\n",
      "\n",
      "\n",
      "----------------- 6 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0007\n",
      "Epoch: 2/10, Average loss: 0.0006\n",
      "Epoch: 3/10, Average loss: 0.0005\n",
      "Epoch: 4/10, Average loss: 0.0005\n",
      "Epoch: 5/10, Average loss: 0.0005\n",
      "Epoch: 6/10, Average loss: 0.0005\n",
      "Epoch: 7/10, Average loss: 0.0005\n",
      "Epoch: 8/10, Average loss: 0.0005\n",
      "Epoch: 9/10, Average loss: 0.0005\n",
      "Epoch: 10/10, Average loss: 0.0005\n",
      "Training completed. Final training loss: 0.0004617328943473333, Validation loss: 0.00046651252515693295\n",
      "\n",
      "\n",
      "----------------- 7 num_hidden_layers --------------\n",
      "\n",
      "\n",
      "Epoch: 1/10, Average loss: 0.0009\n",
      "Epoch: 2/10, Average loss: 0.0008\n",
      "Epoch: 3/10, Average loss: 0.0007\n",
      "Epoch: 4/10, Average loss: 0.0007\n",
      "Epoch: 5/10, Average loss: 0.0007\n",
      "Epoch: 6/10, Average loss: 0.0007\n",
      "Epoch: 7/10, Average loss: 0.0006\n",
      "Epoch: 8/10, Average loss: 0.0006\n",
      "Epoch: 9/10, Average loss: 0.0006\n",
      "Epoch: 10/10, Average loss: 0.0006\n",
      "Training completed. Final training loss: 0.0006179493570911651, Validation loss: 0.0006307462678152196\n"
     ]
    }
   ],
   "source": [
    "for dataset in (\"EMNIST\",):\n",
    "    print(f\"\\n\\n\\n------------{dataset}------------\\n\")\n",
    "\n",
    "    input_dim = 784\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_str = \"1e3\"\n",
    "    decrease_rate_str = \"06\"\n",
    "    weight_decay = 1e-5\n",
    "    decrease_rate = 0.6\n",
    "    train_loader = train_loaders[dataset]\n",
    "    val_loader = val_loaders[dataset]\n",
    "    train_num = 2\n",
    "\n",
    "\n",
    "    for latent_dim in (10,):\n",
    "        print(f\"\\n\\n\\n-----------------------Training models with {latent_dim} latent_dim----------------------\\n\\n\\n\")\n",
    "\n",
    "        num_hidden_layers = 1\n",
    "\n",
    "        print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "        \n",
    "        new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "        writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "        optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "        save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "        torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "        ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "        ex_model.load_state_dict(new_model.state_dict())\n",
    "\n",
    "        for num_hidden_layers in range(2,8):\n",
    "            print(f\"\\n\\n----------------- {num_hidden_layers} num_hidden_layers --------------\\n\\n\")\n",
    "            \n",
    "            new_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid, he_init=False).to(device)\n",
    "            layer_wise_pretrain_load_dict(ex_model, new_model)\n",
    "\n",
    "            writer = SummaryWriter(log_dir=f'../runs/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}')\n",
    "            optimizer = optim.Adam(new_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            train(new_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, epochs=10)\n",
    "            save_dir = f'../models/sigmoid_output/simultaneous train/{latent_dim}ld/{dataset}/dr{decrease_rate_str}_lr{learning_rate_str}_lwpretrain_{num_hidden_layers}hl_{train_num}.pth'\n",
    "            torch.save(new_model.state_dict(), save_dir)\n",
    "\n",
    "            ex_model = AE_0(input_dim=input_dim, latent_dim=latent_dim, decrease_rate=decrease_rate, device=device, hidden_layers = num_hidden_layers, output_activation_encoder=nn.Sigmoid).to(device)\n",
    "            ex_model.load_state_dict(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feeb5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
