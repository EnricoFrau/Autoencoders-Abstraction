{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5779f1",
   "metadata": {},
   "source": [
    "# Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b896b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/enricofrausin/Programmazione/PythonProjects/Tesi/Autoencoders')\n",
    "\n",
    "from AE.models import AE_0, ProgressiveAE\n",
    "from AE.datasets import MNISTDigit2Dataset, MNISTDigit2OnlyDataset, FEMNISTDataset\n",
    "\n",
    "from AE.depth_utils import calc_hfm_kld_with_optimal_g, compute_bottleneck_neurons_activ_freq, compute_emp_states_dict_gauged, compute_bottleneck_neurons_activ_freq_gauged, compute_dataset_klds_gs_dict_with_optimal_threshold_, compute_dataset_klds_gs_dict_from_sampled_binarized_vectors_\n",
    "from AE.plotter_functions import visualize_bottleneck_neurons, plot_KLs_vs_hidden_layers, datasets_dicts_comparison\n",
    "from AE.plotter_functions import datasets_dicts_comparison_colored\n",
    "\n",
    "#––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device(\"cpu\")  # Fallback to CPU if no GPU is available\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba11832",
   "metadata": {},
   "source": [
    "\n",
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405072f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "## MNIST\n",
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "## ExtendedMNIST\n",
    "\n",
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "## 2MNIST\n",
    "\n",
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_2MNISTonly_train = MNISTDigit2OnlyDataset(train=True, download=True)\n",
    "train_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_2MNISTonly_val = MNISTDigit2OnlyDataset(train=False, download=True)\n",
    "val_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "datasets = [\"MNIST\", \"EMNIST\", \"2MNIST\", \"2MNISTonly\"]\n",
    "train_loaders = {\n",
    "    \"MNIST\": train_loader_MNIST,\n",
    "    \"EMNIST\": train_loader_EMNIST,\n",
    "    \"2MNIST\": train_loader_2MNIST,\n",
    "    \"2MNISTonly\": train_loader_2MNISTonly\n",
    "}\n",
    "val_loaders = {\n",
    "    \"MNIST\": val_loader_MNIST,\n",
    "    \"EMNIST\": val_loader_EMNIST,\n",
    "    \"2MNIST\": val_loader_2MNIST,\n",
    "    \"2MNISTonly\": val_loader_2MNISTonly\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_2MNISTonly_train = MNISTDigit2OnlyDataset(train=True, download=True)\n",
    "train_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_2MNISTonly_val = MNISTDigit2OnlyDataset(train=False, download=True)\n",
    "val_loader_2MNISTonly = DataLoader(dataset_2MNISTonly_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"MNIST\", \"EMNIST\", \"2MNISTonly\"]\n",
    "train_loaders = {\n",
    "    \"MNIST\": train_loader_MNIST,\n",
    "    \"EMNIST\": train_loader_EMNIST,\n",
    "    \"2MNISTonly\": train_loader_2MNISTonly\n",
    "}\n",
    "val_loaders = {\n",
    "    \"MNIST\": val_loader_MNIST,\n",
    "    \"EMNIST\": val_loader_EMNIST,\n",
    "    \"2MNISTonly\": val_loader_2MNISTonly\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_FEMNIST_train = FEMNISTDataset(train=True, download=True)\n",
    "train_loader_FEMNIST = DataLoader(dataset_FEMNIST_train, batch_size=batch_size, shuffle=True)\n",
    "dataset_FEMNIST_val = FEMNISTDataset(train=False, download=True)\n",
    "val_loader_FEMNIST = DataLoader(dataset_FEMNIST_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_loaders[\"FEMNIST\"] = train_loader_FEMNIST\n",
    "val_loaders[\"FEMNIST\"] = val_loader_FEMNIST\n",
    "print(len(train_loader_FEMNIST.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29917ce",
   "metadata": {},
   "source": [
    "\n",
    "## FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a19df",
   "metadata": {},
   "source": [
    "\n",
    "## OTHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ffb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from AE.datasets import Dataset_HFM, Dataset_pureHFM\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "## train over pureHFM\n",
    "\n",
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size= batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size= batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "## train over expandedHFM 32-1024\n",
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size= batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4b00d",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AE.overlaps import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_MSE_loss(model, dataset_loader, l1_lambda=0.0, device=None):\n",
    "    \"\"\"\n",
    "    Return the loss computed exactly as in the train(...) function:\n",
    "    sum over batches of (MSE(output, data) + l1_lambda * sum(abs(params))) \n",
    "    divided by len(dataset_loader.dataset).\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        try:\n",
    "            device = model.device\n",
    "        except AttributeError:\n",
    "            # fallback to first parameter device\n",
    "            device = next(model.parameters()).device\n",
    "\n",
    "    total_loss = 0.0\n",
    "    mse = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in dataset_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            batch_loss = mse(output, data)\n",
    "            # L1 term added exactly as in train()\n",
    "            l1_term = l1_lambda * sum(p.abs().sum() for p in model.parameters())\n",
    "            total_loss += (batch_loss + l1_term).item()\n",
    "\n",
    "    # Use same denominator as train(): len(dataset_loader.dataset) if available\n",
    "    try:\n",
    "        denom = len(dataset_loader.dataset)\n",
    "    except Exception:\n",
    "        # fallback: total number of samples iterated (approx)\n",
    "        denom = sum(d.size(0) for d, _ in dataset_loader)\n",
    "\n",
    "    return total_loss / denom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_mse(model, data_loader, device=None):\n",
    "    \"\"\"\n",
    "    Compute MSE over a whole dataset using the same per-batch mean loss as `train`.\n",
    "    This matches train(...) which uses nn.MSELoss() (default reduction='mean'),\n",
    "    accumulates batch losses and then divides by len(dataset).\n",
    "    Args:\n",
    "        model: torch.nn.Module (AE_0) - forward(x) should return reconstruction or (recon, ...)\n",
    "        data_loader: torch.utils.data.DataLoader\n",
    "        device: torch.device or None (if None, prefer model.device then model params)\n",
    "    Returns:\n",
    "        float: loss value computed the same way as in train()\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import math\n",
    "\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = getattr(model, \"device\", None)\n",
    "        if device is None:\n",
    "            try:\n",
    "                device = next(model.parameters()).device\n",
    "            except StopIteration:\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction='mean')  # default reduction='mean' to match train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            recon = out[0] if isinstance(out, (list, tuple)) else out\n",
    "            # ensure shapes compatible\n",
    "            if recon.shape != x.shape and recon.dim() == 2 and x.dim() > 2 and recon.size(0) == x.size(0):\n",
    "                recon = recon.view_as(x)\n",
    "            total_loss += loss_fn(recon, x).item()\n",
    "\n",
    "    dataset_size = len(getattr(data_loader, \"dataset\", []))\n",
    "    return total_loss / dataset_size if dataset_size > 0 else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 10\n",
    "\n",
    "for i, dataset in enumerate((\"2MNISTonly\", \"2MNIST\", \"MNIST\", \"EMNIST\", \"FEMNIST\")):\n",
    "    model_kwargs = {\n",
    "        'input_dim': 28*28,\n",
    "        'latent_dim': latent_dim,\n",
    "        'decrease_rate': 0.6,\n",
    "        'device': device,\n",
    "        'output_activation_encoder': nn.Sigmoid\n",
    "    }\n",
    "    model_path_kwargs = {\n",
    "        'output_activation_encoder': 'sigmoid output',\n",
    "        'train_type': 'simultaneous train',\n",
    "        'latent_dim': f\"{model_kwargs['latent_dim']}ld\",\n",
    "        'dataset': dataset,\n",
    "        'decrease_rate': '06',\n",
    "        'train_num': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        rep_dataset_train_loss_dict = {}\n",
    "        rep_dataset_val_loss_dict = {}\n",
    "\n",
    "    num_hidden_layers_range = range(1,8)\n",
    "    for train_num in range(6):\n",
    "        for num_hidden_layers in num_hidden_layers_range:\n",
    "            model_path_kwargs['num_hidden_layers'] = num_hidden_layers\n",
    "            model_kwargs['hidden_layers'] = num_hidden_layers\n",
    "            model_path_kwargs['train_num'] = train_num\n",
    "\n",
    "            if train_num not in rep_dataset_train_loss_dict:\n",
    "                rep_dataset_train_loss_dict[train_num] = {'2MNISTonly': [], \"2MNIST\": [], 'MNIST': [], 'EMNIST': [], 'FEMNIST': []}\n",
    "            if train_num not in rep_dataset_val_loss_dict:\n",
    "                rep_dataset_val_loss_dict[train_num] = {'2MNISTonly': [], \"2MNIST\": [], 'MNIST': [], 'EMNIST': [], 'FEMNIST': []}\n",
    "\n",
    "            model = load_model(model_path_kwargs, model_kwargs)\n",
    "\n",
    "            rep_dataset_train_loss_dict[train_num][dataset].append(calc_MSE_loss(model, train_loaders[dataset], device=device))\n",
    "            # rep_dataset_val_loss_dict[train_num][dataset].append(calc_MSE_loss(model, val_loaders[dataset], device=device))\n",
    "import pickle\n",
    "\n",
    "with open(f'../savings/losses/sigmoid decoder output/{latent_dim}ld/rep_dataset_train_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(rep_dataset_train_loss_dict, f)\n",
    "\n",
    "\n",
    "with open(f'../savings/losses/sigmoid decoder output/{latent_dim}ld/rep_dataset_val_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(rep_dataset_val_loss_dict, f)\n",
    "# ...existing code...\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mean_loss_over_depth(rep_dataset_train_loss_dict,\n",
    "                              datasets=None,\n",
    "                              num_hidden_layers_range=range(1,8),\n",
    "                              show_std=True,\n",
    "                              figsize=(8,5),\n",
    "                              title=\"Mean loss vs # hidden layers\",\n",
    "                              xlabel=\"Number of hidden layers\",\n",
    "                              ylabel=\"MSE\",\n",
    "                              save_path=None,\n",
    "                              ax=None):\n",
    "    \"\"\"\n",
    "    Compute mean (and std) across train_num realizations and plot loss vs depth.\n",
    "    Args:\n",
    "        rep_dataset_train_loss_dict: dict keyed by train_num -> dict(dataset -> list(losses per depth))\n",
    "        datasets: list of dataset names to plot (default: keys from first inner dict)\n",
    "        num_hidden_layers_range: iterable of depth values (default 1..7)\n",
    "        show_std: if True fill ±1 std region\n",
    "        figsize, title, xlabel, ylabel: plot params\n",
    "        save_path: if given, save figure to path\n",
    "        ax: optional matplotlib Axes to draw into\n",
    "    Returns:\n",
    "        mean_dict, std_dict: dicts dataset -> list(mean), dataset -> list(std)\n",
    "    \"\"\"\n",
    "    # collect train_nums\n",
    "    train_nums = sorted(rep_dataset_train_loss_dict.keys())\n",
    "    if len(train_nums) == 0:\n",
    "        raise ValueError(\"rep_dataset_train_loss_dict is empty\")\n",
    "\n",
    "    # infer datasets\n",
    "    first_inner = rep_dataset_train_loss_dict[train_nums[0]]\n",
    "    if datasets is None:\n",
    "        datasets = list(first_inner.keys())\n",
    "\n",
    "    # prepare output\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "\n",
    "    depths = list(num_hidden_layers_range)\n",
    "\n",
    "    for ds in datasets:\n",
    "        # gather lists from each train_num if available\n",
    "        rows = []\n",
    "        for tn in train_nums:\n",
    "            inner = rep_dataset_train_loss_dict.get(tn, {})\n",
    "            vals = inner.get(ds)\n",
    "            if vals is None:\n",
    "                continue\n",
    "            # ensure it's a numpy 1d array\n",
    "            rows.append(np.asarray(vals))\n",
    "\n",
    "        if len(rows) == 0:\n",
    "            # no data for this dataset\n",
    "            mean_dict[ds] = []\n",
    "            std_dict[ds] = []\n",
    "            continue\n",
    "\n",
    "        # align lengths: use minimum available length across realizations\n",
    "        min_len = min(r.shape[0] for r in rows)\n",
    "        stacked = np.vstack([r[:min_len] for r in rows])\n",
    "        mean = stacked.mean(axis=0)\n",
    "        std = stacked.std(axis=0)\n",
    "\n",
    "        mean_dict[ds] = mean.tolist()\n",
    "        std_dict[ds] = std.tolist()\n",
    "\n",
    "    # plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "    for ds, mean in mean_dict.items():\n",
    "        if len(mean) == 0:\n",
    "            continue\n",
    "        x_plot = depths[:len(mean)]\n",
    "        ax.plot(x_plot, mean, marker='o', label=ds)\n",
    "        if show_std and len(std_dict.get(ds, [])) == len(mean):\n",
    "            std = np.asarray(std_dict[ds])\n",
    "            ax.fill_between(x_plot, np.asarray(mean)-std, np.asarray(mean)+std, alpha=0.25)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(depths)\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax.legend()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "    return mean_dict, std_dict\n",
    "# ...existing code...\n",
    "# compute and plot mean train loss over depths\n",
    "mean_losses, std_losses = plot_mean_loss_over_depth(rep_dataset_train_loss_dict,\n",
    "                                                    datasets=['2MNISTonly','2MNIST','MNIST','EMNIST','FEMNIST'],\n",
    "                                                    num_hidden_layers_range=range(1,8),\n",
    "                                                    show_std=False,\n",
    "                                                    title=\"Mean training MSE vs depth\",\n",
    "                                                    save_path=None)\n",
    "plt.show()\n",
    "# compute and plot mean train loss over depths\n",
    "mean_losses, std_losses = plot_mean_loss_over_depth(rep_dataset_train_loss_dict,\n",
    "                                                    datasets=['2MNISTonly','2MNIST','MNIST','EMNIST','FEMNIST'],\n",
    "                                                    num_hidden_layers_range=range(1,8),\n",
    "                                                    show_std=False,\n",
    "                                                    title=\"Mean training MSE vs depth\",\n",
    "                                                    save_path=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e293bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
